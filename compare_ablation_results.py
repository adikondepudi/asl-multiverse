# FILE: compare_ablation_results.py
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from pathlib import Path
import json
import sys
import numpy as np

def main():
    root_dir = Path("hpc_ablation_jobs")
    results = []
    
    print("Crawling results...")
    
    # 1. Crawl all metrics.json files
    for metrics_file in root_dir.rglob("metrics.json"):
        # Load Metrics (Generated by validate.py)
        with open(metrics_file, 'r') as f:
            m = json.load(f)
            
        # Load Config (To know what hyperparameters produced this)
        config_file = metrics_file.parent.parent / "config.yaml"
        if not config_file.exists():
            # Try one level up if validation_results is a subdirectory
            config_file = metrics_file.parent.parent.parent / "config.yaml"
        
        if not config_file.exists():
            print(f"Warning: Config not found for {metrics_file}, skipping.")
            continue
            
        import yaml
        with open(config_file, 'r') as f:
            cfg = yaml.safe_load(f)
            
        # Flatten Data (Handle potential missing keys gracefully)
        try:
            scenarios = m.get('scenarios', {})
            # Try to find a scenario with the expected structure
            scenario_key = 'C_VarBoth_SNR10'
            if scenario_key not in scenarios:
                # Use first available scenario
                scenario_key = list(scenarios.keys())[0] if scenarios else None
            
            if scenario_key:
                scenario_data = scenarios[scenario_key]
                cbf_data = scenario_data.get('CBF', {})
                
                row = {
                    "ID": metrics_file.parent.parent.name,
                    "Noise_Robustness": "Robust" if "physio" in cfg.get('data_noise_components', []) else "Standard",
                    "Num_Features": len(cfg.get('active_features', [])),
                    "Features": str(cfg.get('active_features', [])),
                    "NN_MAE_CBF": cbf_data.get('Neural_Net', {}).get('MAE', np.nan),
                    "LS_MAE_CBF": cbf_data.get('Least_Squares', {}).get('MAE', np.nan),
                    "NN_Win_Rate": cbf_data.get('NN_vs_LS_Win_Rate', np.nan)
                }
                results.append(row)
        except Exception as e:
            print(f"Warning: Could not parse metrics from {metrics_file}: {e}")
            continue
        
    if not results:
        print("No results found.")
        sys.exit(0)
        
    df = pd.DataFrame(results)
    df.to_csv("final_ablation_summary.csv", index=False)
    print(f"Saved summary to final_ablation_summary.csv ({len(df)} experiments)")
    
    # 2. PLOT 1: Feature Importance (Bar Chart)
    plt.figure(figsize=(10, 6))
    sns.barplot(data=df, x="Features", y="NN_MAE_CBF", hue="Noise_Robustness")
    plt.xticks(rotation=45, ha='right')
    plt.title("Impact of Input Features on CBF Error")
    plt.ylabel("Neural Network MAE (CBF)")
    plt.xlabel("Feature Configuration")
    plt.tight_layout()
    plt.savefig("plot_feature_impact.png", dpi=150)
    print("Saved plot_feature_impact.png")
    
    # 3. PLOT 2: The "Grand Tournament" (Scatter)
    plt.figure(figsize=(8, 8))
    sns.scatterplot(data=df, x="LS_MAE_CBF", y="NN_MAE_CBF", hue="Noise_Robustness", style="Features", s=100)
    
    # Draw Identity Line (Below line = NN wins)
    all_vals = list(df["LS_MAE_CBF"].dropna()) + list(df["NN_MAE_CBF"].dropna())
    if all_vals:
        lims = [min(all_vals) * 0.9, max(all_vals) * 1.1]
        plt.plot(lims, lims, 'k--', alpha=0.75, zorder=0, label='NN = LS')
    
    plt.xlabel("Least Squares Error (MAE)")
    plt.ylabel("Neural Network Error (MAE)")
    plt.title("NN vs LS: All Experiments\n(Points below line = NN wins)")
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.savefig("plot_tournament.png", dpi=150)
    print("Saved plot_tournament.png")
    
    print("\nAnalysis Complete!")
    
    # Print summary statistics
    print("\n=== Summary Statistics ===")
    print(f"Total experiments: {len(df)}")
    if not df["NN_MAE_CBF"].isna().all():
        print(f"Best NN MAE: {df['NN_MAE_CBF'].min():.3f} (Experiment: {df.loc[df['NN_MAE_CBF'].idxmin(), 'ID']})")
        win_rate = (df["NN_MAE_CBF"] < df["LS_MAE_CBF"]).mean() * 100
        print(f"NN vs LS Win Rate: {win_rate:.1f}%")

if __name__ == "__main__":
    main()
