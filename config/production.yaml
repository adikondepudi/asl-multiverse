training:
  batch_size: 2048
  learning_rate: 0.0028
  weight_decay: 0.00001
  hidden_sizes: [256, 128, 64]
  n_ensembles: 5
  dropout_rate: 0.1
  norm_type: "batch"
  n_subjects_stage1: 40000
  n_epochs_stage1: 200
  n_subjects_stage2: 10000
  n_epochs_stage2: 80
  loss_pinn_weight_stage1: 2.0
  loss_pinn_weight_stage2: 0.1
  learning_rate_stage2: 0.0002
  use_transformer_temporal_model: true
  m0_input_feature_model: false
  use_focused_transformer_model: true
  transformer_d_model_focused: 32
  transformer_nhead_model: 4
  transformer_nlayers_model: 2
  log_var_cbf_min: -6.0
  log_var_cbf_max: 7.0
  log_var_att_min: -2.0
  log_var_att_max: 14.0
  loss_weight_cbf: 1.0
  loss_weight_att: 2.0
  loss_log_var_reg_lambda: 0.0

data:
  val_split: 0.2
  pld_values: [500, 1000, 1500, 2000, 2500, 3000]
  att_ranges_config:
    - [500, 1500, "Short ATT"]
    - [1500, 2500, "Medium ATT"]
    - [2500, 4000, "Long ATT"]
  include_m0_in_training_data: false

simulation:
  T1_artery: 1850.0
  T2_factor: 1.0
  alpha_BS1: 1.0
  alpha_PCASL: 0.85
  alpha_VSASL: 0.56
  T_tau: 1800.0
  reference_CBF: 60.0

optuna:
  optuna_n_trials: 100
  optuna_timeout_hours: 4.0
  optuna_n_subjects: 2000
  optuna_n_epochs: 40
  optuna_study_name: "asl_multiverse_hpo_production"

logging:
  level: "INFO"

wandb:
  wandb_project: "asl-multiverse-production"
  wandb_entity: null