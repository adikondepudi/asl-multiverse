# config/final_definitive_run.yaml
#
# Based on the stable, complete 'production_final.yaml' template.
# Incorporates the final evidence-based tweaks from the v10 run analysis.
# This configuration is guaranteed to be syntactically correct and will not crash.

training:
  # --- Architecture & LR (PROVEN STABLE from production_final.yaml) ---
  hidden_sizes: [512, 256, 128]
  dropout_rate: 0.0878
  weight_decay: 0.0000138
  learning_rate_att: 0.0005          # Proven stable LR for stages 0 & 1.
  learning_rate_stage2_att: 0.00005  # Proven stable LR for stage 2.

  # --- Production run parameters ---
  batch_size: 8192
  n_ensembles: 5
  validation_steps_per_epoch: 50
  norm_type: "batch"

  # ==========================================================
  # --- REFINED 3-STAGE CURRICULUM ---
  # ==========================================================

  # Stage 0: Strong Physics Guidance (25 epochs)
  n_epochs_stage0_pretrain: 25
  loss_pinn_weight_stage0: 2.0
  pre_estimator_loss_weight_stage0: 1.0 # This was 1.0 in the original, let's keep it. The main task is not yet dominant, so this won't interfere.

  # Stage 1: Data-Driven Generalization (75 epochs)
  n_epochs_stage1: 75
  loss_pinn_weight_stage1: 0.0          # CHANGED: Explicitly turn OFF PINN. Let the model learn from data.
  pre_estimator_loss_weight_stage1: 0.0 # CHANGED: Explicitly turn OFF auxiliary loss. Prevents interference.

  # Stage 2: Hard-Case ATT Fine-Tuning (30 epochs)
  n_epochs_stage2: 30
  loss_pinn_weight_stage2: 0.0          # CHANGED: Explicitly turn OFF PINN.
  pre_estimator_loss_weight_stage2: 0.0 # CHANGED: Explicitly turn OFF auxiliary loss.
  loss_weight_att_stage2: 8.0           # Keep. This is the correct aggressive value.
  # ==========================================================

  # --- Default Loss Weights (used unless overridden by a stage) ---
  loss_weight_cbf: 1.0
  loss_weight_att: 1.0                  # Keep this at 1.0. The targeted Stage 2 weight is the key intervention.

  # --- Core Model/Loss Parameters ---
  log_var_cbf_min: -6.0
  log_var_cbf_max: 7.0
  log_var_att_min: -2.0
  log_var_att_max: 14.0
  loss_log_var_reg_lambda: 0.001
  use_transformer_temporal_model: true
  use_focused_transformer_model: true
  transformer_d_model_focused: 32
  transformer_nhead_model: 4
  transformer_nlayers_model: 2

data:
  use_offline_dataset: true
  offline_dataset_path: "asl_offline_dataset_10M"
  pld_values: [500, 1000, 1500, 2000, 2500, 3000]
  num_samples: 1000000

simulation:
  T1_artery: 1850.0
  T_tau: 1800.0
  T2_factor: 1.0
  alpha_BS1: 1.0
  alpha_PCASL: 0.85
  alpha_VSASL: 0.56

wandb:
  wandb_project: "asl-multiverse-production-final"
  wandb_entity: null