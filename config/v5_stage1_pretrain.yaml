# FILE: config/v5_stage1_pretrain.yaml
# Configuration for the v5 self-supervised pre-training stage (Stage 1).
# Trains a generalist encoder via denoising.

training:
  model_class_name: "DisentangledASLNet"
  encoder_type: "physics_processor"
  hidden_sizes: [256, 128, 128]

  # Regularization for the denoising autoencoder
  dropout_rate: 0.1
  weight_decay: 0.0001
  learning_rate: 0.0003
  
  # Production Scale Parameters
  batch_size: 8192
  n_ensembles: 1 # Only need one expert encoder
  validation_steps_per_epoch: 25
  norm_type: "batch"
  n_epochs: 15

  # Denoising autoencoder does not use these
  log_var_cbf_min: 0.0
  log_var_cbf_max: 0.0
  log_var_att_min: 0.0
  log_var_att_max: 0.0
  
  # Architecture details for the encoder
  transformer_d_model_focused: 32
  transformer_nhead_model: 4
  transformer_nlayers_model: 2

data:
  use_offline_dataset: true
  offline_dataset_path: "asl_offline_dataset_10M_v3"
  pld_values: [500, 1000, 1500, 2000, 2500, 3000]
  num_samples: 1000000

  # Noise and normalization settings (see base_template.yaml for full docs)
  noise_type: "gaussian"           # Options: 'gaussian' (default), 'rician'
  normalization_mode: "per_curve"  # Options: 'per_curve' (default), 'global_scale'
  global_scale_factor: 10.0        # Used only when normalization_mode='global_scale' 

simulation:
  T1_artery: 1650.0  # 3T consensus (Alsop 2015)
  T_tau: 1800.0
  T2_factor: 1.0
  alpha_BS1: 1.0
  alpha_PCASL: 0.85
  alpha_VSASL: 0.56

wandb:
  wandb_project: "asl-multiverse-prod-v5"
  wandb_entity: "adikondepudi"