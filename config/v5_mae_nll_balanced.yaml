# Configuration for balanced MAE + NLL loss training
# MAE forces accurate predictions, NLL provides uncertainty estimation
# This is a good balance for production use

training:
  # === Loss Mode Configuration ===
  loss_mode: "mae_nll"   # MAE primary + NLL for uncertainty
  mae_weight: 1.0        # Primary accuracy term
  nll_weight: 0.1        # Secondary uncertainty term (10% weight)

  # Loss weights for CBF vs ATT
  loss_weight_cbf: 1.0
  loss_weight_att: 1.0

  # Light regularization on log_var to prevent uncertainty collapse
  loss_log_var_reg_lambda: 0.001

  # Training parameters
  learning_rate: 0.001
  hidden_sizes: [128, 64, 32]
  dropout_rate: 0.1
  n_ensembles: 3
  training_epochs: 50
  batch_size: 512
  early_stopping_patience: 15

  # Architecture
  encoder_type: "physics_processor"
  transformer_d_model_focused: 32
  transformer_nhead_model: 4

data:
  dataset_path: "asl_clean_library_10M"
  pld_values: [500, 1000, 1500, 2000, 2500, 3000]
  active_features: ["mean", "std", "peak", "t1_artery"]
  data_noise_components: ["thermal"]

  # Normalization
  noise_type: "gaussian"
  normalization_mode: "per_curve"

simulation:
  T1_artery: 1650.0  # 3T consensus (Alsop 2015)
  T_tau: 1800.0
  alpha_PCASL: 0.85
  alpha_VSASL: 0.56

noise_config:
  snr_range: [5, 30]
