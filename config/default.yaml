training:
  batch_size: 256
  learning_rate: 0.001
  weight_decay: 0.00001 # Added weight_decay
  hidden_sizes: [256, 128, 64] 
  training_n_epochs: 200
  n_ensembles: 5
  dropout_rate: 0.1
  norm_type: "batch"
  
  # Model-specific params for EnhancedASLNet
  use_transformer_temporal_model: true
  m0_input_feature_model: false 

  # Transformer settings
  use_focused_transformer_model: true # FIX: Enable focused transformer for separate PCASL/VSASL processing
  transformer_d_model: 64             # d_model for shared transformer (not used if focused is true)
  transformer_d_model_focused: 32     # d_model per branch if focused
  transformer_nhead_model: 4
  transformer_nlayers_model: 2
  
  # Uncertainty head settings for EnhancedASLNet
  log_var_cbf_min: -6.0
  log_var_cbf_max: 7.0
  log_var_att_min: -2.0
  log_var_att_max: 14.0

  # CustomLoss settings
  loss_weight_cbf: 1.0
  loss_weight_att: 1.0
  loss_log_var_reg_lambda: 0.0 # 0.0 means off, try 1e-4 or 1e-3 if needed

data:
  n_training_subjects: 10000 # Renamed from n_samples for clarity in main.py
  val_split: 0.2
  pld_values: [500, 1000, 1500, 2000, 2500, 3000]
  att_ranges_config: # Renamed from att_ranges for clarity
    - [500, 1500, "Short ATT"]
    - [1500, 2500, "Medium ATT"]
    - [2500, 4000, "Long ATT"]
  include_m0_in_training_data: false

simulation:
  T1_artery: 1850.0
  T2_factor: 1.0
  alpha_BS1: 1.0
  alpha_PCASL: 0.85
  alpha_VSASL: 0.56
  T_tau: 1800.0
  reference_CBF: 60.0

optuna:
  optuna_n_trials: 20 # Renamed for clarity
  optuna_timeout_hours: 0.5 # Renamed
  optuna_n_subjects: 500 # Renamed
  optuna_n_epochs: 20    # Renamed
  optuna_study_name: "asl_multiverse_hpo"
  # HPO can also optimize weight_decay if added to HyperparameterOptimizer.objective

logging:
  level: "INFO"

wandb: 
  wandb_project: "asl-multiverse-project"
  wandb_entity: null 
