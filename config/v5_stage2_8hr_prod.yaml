# FILE: config/v5_stage2_8hr_prod.yaml
training:
  model_class_name: "DisentangledASLNet"
  encoder_type: "physics_processor"
  
  hidden_sizes: [512, 256, 128]
  dropout_rate: 0.15          # Regularization for the head.
  weight_decay: 0.0001        # Regularization for the head.
  log_var_reg_lambda: 0.05

  learning_rate: 0.00005      # LR for the HEAD only.
  batch_size: 8192
  n_ensembles: 5

  # STRATEGY: Fine-tune on a large dataset with early stopping.
  n_epochs: 10
  steps_per_epoch: 488 # 488 steps * 8192 batch_size = ~4M samples per epoch

  validation_steps_per_epoch: 25
  early_stopping_patience: 5
  early_stopping_min_delta: 0.0005

  norm_type: "batch"
  log_var_cbf_min: -5.0
  log_var_cbf_max: 7.0
  log_var_att_min: -5.0
  log_var_att_max: 14.0
  transformer_d_model_focused: 32
  transformer_nhead_model: 4
  transformer_nlayers_model: 2

# NEW SECTION: Controls discriminative fine-tuning behavior
fine_tuning:
  enabled: true
  encoder_lr_factor: 20.0 # Encoder LR will be learning_rate / this factor

moe:
  num_experts: 4
  gating_dropout_rate: 0.1

# NEW SECTION: Principled Online Hard Example Mining (OHEM)
ohem:
  ohem_start_epoch: 3           # Warm-up for 3 epochs before activating OHEM.
  ohem_fraction: 0.5            # Focus on the top 50% hardest examples per batch.
  loss_clip_percentile: 0.99    # Mitigate outliers by ignoring the top 1% of losses during selection.

data:
  use_offline_dataset: true
  offline_dataset_path: "asl_offline_dataset_10M_v3"
  num_samples_to_load: 4000000 
  pld_values: [500, 1000, 1500, 2000, 2500, 3000]

simulation:
  T1_artery: 1850.0
  T_tau: 1800.0
  T2_factor: 1.0
  alpha_BS1: 1.0
  alpha_PCASL: 0.85
  alpha_VSASL: 0.56

wandb:
  wandb_project: "asl-multiverse-prod-v5"
  wandb_entity: "adikondepudi"