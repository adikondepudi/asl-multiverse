import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import numpy as np
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass
from collections import defaultdict
from torch.optim.lr_scheduler import OneCycleLR
import math
import multiprocessing as mp
from pathlib import Path
import wandb 
from sklearn.metrics import mean_absolute_error, mean_squared_error 


num_workers = mp.cpu_count()

from enhanced_asl_network import EnhancedASLNet, CustomLoss
# from enhanced_simulation import RealisticASLSimulator # For type hinting

import logging
logger = logging.getLogger(__name__)
if not logger.hasHandlers():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')


class ASLNet(nn.Module):
    """Neural network for ASL parameter estimation"""

    def __init__(self, input_size: int, hidden_sizes: List[int] = [64, 32, 16]):
        super().__init__()

        # Build network layers
        layers = []
        prev_size = input_size

        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_size),
                nn.Dropout(0.1)
            ])
            prev_size = hidden_size

        # Output layer for CBF and ATT
        layers.append(nn.Linear(prev_size, 2))

        self.network = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x)

class ASLDataset(Dataset):
    """Dataset for ASL signals and parameters"""

    def __init__(self, signals: np.ndarray, params: np.ndarray):
        self.signals = torch.FloatTensor(signals)
        self.params = torch.FloatTensor(params)

    def __len__(self) -> int:
        return len(self.signals)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        return self.signals[idx], self.params[idx]

class ASLTrainer:
    """Training manager for ASL parameter estimation network"""

    def __init__(self,
                 input_size: int,
                 hidden_sizes: List[int] = [64, 32, 16],
                 learning_rate: float = 1e-3,
                 batch_size: int = 32,
                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):

        self.device = device
        self.batch_size = batch_size

        # Initialize network
        self.model = ASLNet(input_size, hidden_sizes).to(device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.criterion = nn.MSELoss()

        # Track training progress
        self.train_losses = []
        self.val_losses = []

    def prepare_data(self,
                    simulator, # ASLSimulator instance
                    n_samples: int = 10000,
                    val_split: float = 0.2,
                    plds_definition: Optional[np.ndarray] = None) -> Tuple[DataLoader, DataLoader]:
        """Prepare training and validation data loaders"""
        if plds_definition is None:
            plds = np.arange(500, 3001, 500)
        else:
            plds = plds_definition

        att_values = np.arange(0, 4001, 100)
        signals_dict = simulator.generate_synthetic_data(
            plds, att_values, n_noise=n_samples, tsnr=5.0,
            cbf_val=simulator.params.CBF
        )

        num_total_instances = n_samples * len(att_values)
        X = np.zeros((num_total_instances, len(plds) * 2))

        pcasl_all = signals_dict['PCASL'].reshape(num_total_instances, len(plds))
        vsasl_all = signals_dict['VSASL'].reshape(num_total_instances, len(plds))

        X[:, :len(plds)] = pcasl_all
        X[:, len(plds):] = vsasl_all

        cbf_true_val = simulator.params.CBF
        cbf_params = np.full(num_total_instances, cbf_true_val)
        att_params = np.tile(np.repeat(att_values, 1), n_samples)
        y = np.column_stack((cbf_params, att_params))

        if num_total_instances == 0:
            raise ValueError("No data generated by simulator.prepare_data.")
        n_val = int(num_total_instances * val_split)
        if n_val == 0 and num_total_instances > 1: n_val = 1
        if n_val >= num_total_instances : n_val = num_total_instances -1 if num_total_instances > 0 else 0

        X_train, X_val = X[:-n_val], X[-n_val:]
        y_train, y_val = y[:-n_val], y[-n_val:]

        if X_train.shape[0] == 0:
            raise ValueError("Training set is empty after split.")

        train_dataset = ASLDataset(X_train, y_train)
        val_dataset = ASLDataset(X_val, y_val)

        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=self.batch_size if X_val.shape[0] > 0 else 1)
        return train_loader, val_loader

    def train_epoch(self, train_loader: DataLoader) -> float:
        self.model.train()
        total_loss = 0.0
        for signals, params in train_loader:
            signals, params = signals.to(self.device), params.to(self.device)
            self.optimizer.zero_grad()
            outputs = self.model(signals)
            loss = self.criterion(outputs, params)
            loss.backward()
            self.optimizer.step()
            total_loss += loss.item()
        return total_loss / len(train_loader) if len(train_loader) > 0 else 0.0

    def validate(self, val_loader: DataLoader) -> float:
        if val_loader is None or len(val_loader) == 0:
            logger.warning("Validation loader is empty or None, skipping validation.")
            return float('inf')
        self.model.eval()
        total_loss = 0.0
        with torch.no_grad():
            for signals, params in val_loader:
                signals, params = signals.to(self.device), params.to(self.device)
                outputs = self.model(signals)
                loss = self.criterion(outputs, params)
                total_loss += loss.item()
        return total_loss / len(val_loader) if len(val_loader) > 0 else float('inf')

    def train(self,
              train_loader: DataLoader,
              val_loader: DataLoader,
              n_epochs: int = 100,
              early_stopping_patience: int = 10) -> Dict[str, List[float]]:
        best_val_loss = float('inf')
        patience_counter = 0
        for epoch in range(n_epochs):
            train_loss = self.train_epoch(train_loader)
            val_loss = self.validate(val_loader)
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                torch.save(self.model.state_dict(), 'best_model_ASLTrainer.pt')
            else:
                patience_counter += 1
            if patience_counter >= early_stopping_patience:
                logger.info(f'Early stopping triggered at epoch {epoch + 1}')
                break
            if (epoch + 1) % 10 == 0:
                logger.info(f'Epoch {epoch + 1}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}')
        model_path = Path('best_model_ASLTrainer.pt')
        if model_path.exists():
            self.model.load_state_dict(torch.load(str(model_path)))
            logger.info(f"Loaded best model from {model_path}")
        else:
            logger.warning(f"Best model file {model_path} not found. Current model state will be used.")
        return {'train_losses': self.train_losses, 'val_losses': self.val_losses}

    def predict(self, signals: np.ndarray) -> np.ndarray:
        self.model.eval()
        with torch.no_grad():
            signals_tensor = torch.FloatTensor(signals).to(self.device)
            if signals_tensor.ndim == 1:
                signals_tensor = signals_tensor.unsqueeze(0)
            predictions = self.model(signals_tensor)
            return predictions.cpu().numpy()

    def evaluate_performance(self,
                           test_signals: np.ndarray,
                           true_params: np.ndarray) -> Dict[str, float]:
        predictions = self.predict(test_signals)
        mae_cbf = np.mean(np.abs(predictions[:,0] - true_params[:,0]))
        mae_att = np.mean(np.abs(predictions[:,1] - true_params[:,1]))
        rmse_cbf = np.sqrt(np.mean((predictions[:,0] - true_params[:,0])**2))
        rmse_att = np.sqrt(np.mean((predictions[:,1] - true_params[:,1])**2))
        rel_error_cbf = np.mean(np.abs(predictions[:,0] - true_params[:,0]) / np.clip(true_params[:,0], 1e-6, None))
        rel_error_att = np.mean(np.abs(predictions[:,1] - true_params[:,1]) / np.clip(true_params[:,1], 1e-6, None))
        return {'MAE_CBF': mae_cbf, 'MAE_ATT': mae_att, 'RMSE_CBF': rmse_cbf, 'RMSE_ATT': rmse_att,
                'RelError_CBF': rel_error_cbf, 'RelError_ATT': rel_error_att}


class EnhancedASLDataset(Dataset):
    def __init__(self,
                 signals: np.ndarray, # Assumed to be already normalized if per-modality norm is done upstream
                 params: np.ndarray, # Assumed to be already normalized if target normalization is done upstream
                 # Augmentation parameters
                 noise_config: Dict = {'type': 'additive_gaussian', 'std_fraction': 0.05, 'abs_std': 0.0001}, # std_fraction of signal max, or abs_std
                 dropout_range: Tuple[float, float] = (0.05, 0.15), # Dropout of PLD points
                 global_scale_range: Tuple[float, float] = (0.95, 1.05), # Simulates efficiency variations
                 baseline_shift_std_factor: float = 0.01, # Fraction of signal mean_abs for baseline shift
                 reference_signal_max_for_noise: float = 3.0, # Approx max expected NORMALIZED ASL signal for noise scaling
                 spike_config: Optional[Dict] = None, # e.g., {'prob': 0.05, 'magnitude_factor': 3.0}
                 drift_config: Optional[Dict] = None, # e.g., {'prob': 0.3, 'magnitude_factor': 0.05}
                ):
        self.signals = torch.FloatTensor(signals)
        self.params = torch.FloatTensor(params)
        
        self.noise_config = noise_config
        self.dropout_range = dropout_range
        self.global_scale_range = global_scale_range
        self.baseline_shift_std_factor = baseline_shift_std_factor
        self.reference_signal_max_for_noise = reference_signal_max_for_noise
        self.spike_config = spike_config if spike_config is not None else {}
        self.drift_config = drift_config if drift_config is not None else {}


    def __len__(self) -> int:
        return len(self.signals)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        signal = self.signals[idx].clone()
        param = self.params[idx].clone() # Params are already normalized if done in prepare_curriculum_data

        # 1. Parameter-Aware Noise (Phase 2, Item 2.2)
        if self.noise_config and self.noise_config.get('type') == 'additive_gaussian' and np.random.rand() < 0.5:
            # Noise std can be a fraction of a reference max signal, or an absolute std
            noise_std = self.noise_config.get('std_fraction', 0.05) * self.reference_signal_max_for_noise \
                        if 'std_fraction' in self.noise_config else self.noise_config.get('abs_std', 0.0001)
            
            if noise_std > 1e-9: # Ensure noise_std is sensible
                noise = torch.randn_like(signal) * noise_std
                signal += noise

        # 2. Dropout of PLD points (original)
        if self.dropout_range and np.random.rand() < 0.5:
            dropout_prob = np.random.uniform(*self.dropout_range)
            mask = torch.rand_like(signal) > dropout_prob
            signal *= mask

        # 3. Global Scaling (simulates efficiency perturbations - Phase 2, Item 2.2)
        if self.global_scale_range and np.random.rand() < 0.5:
            scale_factor = np.random.uniform(*self.global_scale_range)
            signal *= scale_factor
            
        # 4. Baseline Shift (original)
        if self.baseline_shift_std_factor > 0 and np.random.rand() < 0.5:
            signal_mean_abs = torch.mean(torch.abs(signal))
            if signal_mean_abs > 1e-6: # Avoid issues with all-zero signals
                shift_std = self.baseline_shift_std_factor * signal_mean_abs
                shift = torch.randn(1) * shift_std 
                signal += shift.item() # Add scalar shift to all elements
        
        # 5. Randomly scale the PCASL and VSASL components of the signal independently
        if np.random.rand() < 0.5:
            # Number of PLDs is half the raw signal length
            num_plds = signal.shape[0] // 2
            # Apply independent random scaling to PCASL and VSASL parts
            pcasl_scale = np.random.uniform(0.8, 1.2)
            vsasl_scale = np.random.uniform(0.8, 1.2)
            signal[:num_plds] *= pcasl_scale
            signal[num_plds:] *= vsasl_scale

        # 6. Spike Artifact (Tier 3)
        if self.spike_config and np.random.rand() < self.spike_config.get('prob', 0.0):
            noise_std = self.noise_config.get('std_fraction', 0.05) * self.reference_signal_max_for_noise
            if noise_std > 1e-9:
                spike_magnitude = self.spike_config.get('magnitude_factor', 3.0) * noise_std
                spike_idx = np.random.randint(0, len(signal))
                signal[spike_idx] += spike_magnitude * np.random.choice([-1, 1])
        
        # 7. Slow Baseline Drift (Tier 3)
        if self.drift_config and np.random.rand() < self.drift_config.get('prob', 0.0):
            signal_max = torch.max(torch.abs(signal)) if torch.any(torch.abs(signal) > 0) else 1.0
            drift_amplitude = self.drift_config.get('magnitude_factor', 0.05) * signal_max
            if drift_amplitude > 1e-9:
                num_points = len(signal)
                phase = np.random.uniform(0, 2 * np.pi)
                frequency = np.random.uniform(0.1, 0.5)
                t_axis = torch.linspace(0, 2 * np.pi * frequency, num_points)
                drift = drift_amplitude * torch.sin(t_axis + phase)
                signal += drift

        return signal, param

class EnhancedASLTrainer:
    def __init__(self,
                 model_config: Dict, # Pass necessary params for model instantiation and loss
                 model_class: callable, # e.g., EnhancedASLNet
                 input_size: int, # Actual input size for the model
                 learning_rate: float = 0.001,
                 weight_decay: float = 1e-5, # Added weight decay
                 batch_size: int = 256,
                 n_ensembles: int = 5,
                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu',
                 n_plds_for_model: Optional[int] = None, # Used for splitting PCASL/VSASL if needed
                 m0_input_feature_model: bool = False
                ):
        self.device = device
        self.batch_size = batch_size
        self.n_ensembles = n_ensembles
        self.n_plds_for_model = n_plds_for_model # Number of PLDs per modality
        self.m0_input_feature_model = m0_input_feature_model
        self.input_size_model = input_size
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay # Store weight decay
        self.model_config = model_config # Store the full config for model and loss

        self.models = [model_class(**model_config).to(device) for _ in range(n_ensembles)]
        self.best_states = [None] * self.n_ensembles
        self.optimizers = [torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=self.weight_decay) for model in self.models] # Added weight_decay
        self.schedulers = [] # To be initialized in prepare_curriculum_data
        
        # Pass relevant params from model_config to CustomLoss
        loss_params = {
            'w_cbf': model_config.get('loss_weight_cbf', 1.0),
            'w_att': model_config.get('loss_weight_att', 1.0),
            'log_var_reg_lambda': model_config.get('loss_log_var_reg_lambda', 0.0),
            'focal_gamma': model_config.get('focal_gamma', 1.5),
            'pinn_weight': model_config.get('loss_pinn_weight_stage1', model_config.get('loss_pinn_weight', 0.0)), # Use stage1 as default
            'model_params': model_config, # NEW: Pass all model params for physics constants
            'pre_estimator_loss_weight': model_config.get('pre_estimator_loss_weight', 0.5) # Add new param
        }
        self.custom_loss_fn = CustomLoss(**loss_params)

        self.train_losses = defaultdict(list)
        self.val_metrics = defaultdict(list)
        self.global_step = 0
        self.norm_stats = None # Will be populated by prepare_curriculum_data

    def prepare_curriculum_data(self,
                                simulator, # RealisticASLSimulator instance
                                plds: np.ndarray,
                                n_training_subjects: int = 10000,
                                val_split: float = 0.2,
                                curriculum_att_ranges_config: Optional[List[Tuple[float, float, str]]] = None,
                                training_conditions_config: Optional[List[str]] = None,
                                training_noise_levels_config: Optional[List[float]] = None,
                                n_epochs_for_scheduler: int = 200,
                                include_m0_in_data: bool = False,
                                dataset_aug_config: Optional[Dict] = None,
                                precomputed_dataset: Optional[Dict] = None
                                ) -> Tuple[List[DataLoader], List[Optional[DataLoader]], Optional[Dict]]:
        
        if precomputed_dataset:
            logger.info("Using pre-computed dataset for training data preparation.")
            raw_dataset = precomputed_dataset
        else:
            logger.info("No pre-computed dataset provided. Generating diverse dataset now...")
            if plds is None: plds = np.arange(500, 3001, 500)
            conditions = training_conditions_config if training_conditions_config is not None else ['healthy', 'stroke', 'tumor', 'elderly']
            noise_levels = training_noise_levels_config if training_noise_levels_config is not None else [3.0, 5.0, 10.0, 15.0]
            logger.info(f"Generating diverse training data: {n_training_subjects} base subjects, cond: {conditions}, SNRs: {noise_levels}")
            raw_dataset = simulator.generate_diverse_dataset(
                plds=plds, n_subjects=n_training_subjects, conditions=conditions, noise_levels=noise_levels
            )
        
        if plds is None: plds = np.arange(500, 3001, 500)
        num_raw_signal_features = len(plds) * 2

        X_all_raw, y_all_raw = raw_dataset['signals'], raw_dataset['parameters']

        n_total_samples_raw = X_all_raw.shape[0]
        if n_total_samples_raw == 0: raise ValueError("No data generated.")
        
        indices_raw = np.random.permutation(n_total_samples_raw)
        n_val_raw = int(n_total_samples_raw * val_split)
        if n_val_raw == 0 and n_total_samples_raw > 1: n_val_raw = 1
        if n_val_raw >= n_total_samples_raw: n_val_raw = n_total_samples_raw -1 if n_total_samples_raw > 0 else 0

        train_idx_raw, val_idx_raw = indices_raw[:-n_val_raw], indices_raw[-n_val_raw:]
        X_train_raw, X_val_raw = X_all_raw[train_idx_raw], X_all_raw[val_idx_raw]
        y_train_raw, y_val_raw = y_all_raw[train_idx_raw], y_all_raw[val_idx_raw]
        
        X_train_raw_signal_part = X_train_raw[:, :num_raw_signal_features]

        norm_stats = { 
            'pcasl_mean': np.zeros(len(plds)), 'pcasl_std': np.ones(len(plds)),
            'vsasl_mean': np.zeros(len(plds)), 'vsasl_std': np.ones(len(plds)),
            'y_mean_cbf': 0.0, 'y_std_cbf': 1.0, 'y_mean_att': 0.0, 'y_std_att': 1.0,
            'amplitude_mean': 0.0, 'amplitude_std': 1.0
        }

        if X_train_raw_signal_part.shape[0] > 0 :
            pcasl_train_signals = X_train_raw_signal_part[:, :len(plds)]
            norm_stats['pcasl_mean'] = np.mean(pcasl_train_signals, axis=0)
            norm_stats['pcasl_std'] = np.std(pcasl_train_signals, axis=0)
            norm_stats['pcasl_std'][norm_stats['pcasl_std'] < 1e-6] = 1.0

            vsasl_train_signals = X_train_raw_signal_part[:, len(plds):]
            norm_stats['vsasl_mean'] = np.mean(vsasl_train_signals, axis=0)
            norm_stats['vsasl_std'] = np.std(vsasl_train_signals, axis=0)
            norm_stats['vsasl_std'][norm_stats['vsasl_std'] < 1e-6] = 1.0
            
            amplitudes = np.linalg.norm(X_train_raw_signal_part, axis=1)
            norm_stats['amplitude_mean'] = np.mean(amplitudes)
            norm_stats['amplitude_std'] = np.std(amplitudes)
            if norm_stats['amplitude_std'] < 1e-6: norm_stats['amplitude_std'] = 1.0

            norm_stats['y_mean_cbf'] = np.mean(y_train_raw[:, 0])
            norm_stats['y_std_cbf'] = np.std(y_train_raw[:, 0])
            if norm_stats['y_std_cbf'] < 1e-6: norm_stats['y_std_cbf'] = 1.0
            
            norm_stats['y_mean_att'] = np.mean(y_train_raw[:, 1])
            norm_stats['y_std_att'] = np.std(y_train_raw[:, 1])
            if norm_stats['y_std_att'] < 1e-6: norm_stats['y_std_att'] = 1.0
        else:
            logger.warning("Raw training set for normalization stats is empty.")

        X_all_processed = X_all_raw.copy()
        X_all_processed[:, :len(plds)] = (X_all_raw[:, :len(plds)] - norm_stats['pcasl_mean']) / (norm_stats['pcasl_std'] + 1e-6)
        X_all_processed[:, len(plds):num_raw_signal_features] = (X_all_raw[:, len(plds):num_raw_signal_features] - norm_stats['vsasl_mean']) / (norm_stats['vsasl_std'] + 1e-6)

        y_all_normalized = np.zeros_like(y_all_raw)
        y_all_normalized[:, 0] = (y_all_raw[:, 0] - norm_stats['y_mean_cbf']) / norm_stats['y_std_cbf']
        y_all_normalized[:, 1] = (y_all_raw[:, 1] - norm_stats['y_mean_att']) / norm_stats['y_std_att']

        self.norm_stats = norm_stats
        self.custom_loss_fn.norm_stats = norm_stats

        for model in self.models:
            model.set_norm_stats(self.norm_stats)
        logger.info("Normalization stats buffers set on all ensemble models.")

        logger.info(f"Total processed samples for training/validation: {X_all_processed.shape[0]}")
        
        X_train, y_train_norm = X_all_processed[train_idx_raw], y_all_normalized[train_idx_raw]
        y_train_raw_split = y_all_raw[train_idx_raw]
        X_val, y_val_norm = X_all_processed[val_idx_raw], y_all_normalized[val_idx_raw]
        y_val_raw_split = y_all_raw[val_idx_raw]


        logger.info(f"Training samples: {X_train.shape[0]}, Validation samples: {X_val.shape[0]}")
        if X_train.shape[0] == 0: raise ValueError("Training set is empty after split and normalization.")

        if curriculum_att_ranges_config is None:
            min_att, max_att = simulator.physio_var.att_range
            curriculum_stages_def = [(min_att, 1500.0), (1500.0, 2500.0), (2500.0, max_att)]
        else:
            curriculum_stages_def = [(r[0], r[1]) for r in curriculum_att_ranges_config]

        train_loaders, val_loaders = [], []
        default_aug_config = {
            'noise_config': {'type': 'additive_gaussian', 'std_fraction': 0.05},
            'dropout_range': (0.05, 0.15),
            'global_scale_range': (0.95, 1.05),
            'baseline_shift_std_factor': 0.01,
            'reference_signal_max_for_noise': 3.0,
            'spike_config': {'prob': 0.1, 'magnitude_factor': 4.0},
            'drift_config': {'prob': 0.3, 'magnitude_factor': 0.05}
        }
        if dataset_aug_config: default_aug_config.update(dataset_aug_config)

        val_aug_config_minimal = {**default_aug_config, 'noise_config': {}, 'dropout_range': None, 'spike_config': None, 'drift_config': None}

        for i, (att_min, att_max) in enumerate(curriculum_stages_def):
            train_mask = (y_train_raw_split[:, 1] >= att_min) & (y_train_raw_split[:, 1] < att_max)
            stage_X_train, stage_y_train_norm = X_train[train_mask], y_train_norm[train_mask]
            
            if len(stage_X_train) == 0:
                logger.warning(f"Curriculum train stage {i+1} has no samples. Skipping.")
                train_loaders.append(DataLoader(EnhancedASLDataset(np.array([]), np.array([])), batch_size=self.batch_size))
            else:
                att_weights = np.atleast_1d(np.exp(-np.clip(y_train_raw_split[train_mask, 1], 100.0, None) / 2000.0))
                sampler = WeightedRandomSampler(att_weights, len(att_weights), replacement=True) if np.sum(att_weights) > 1e-9 else None
                dataset = EnhancedASLDataset(stage_X_train, stage_y_train_norm, **default_aug_config)
                loader = DataLoader(dataset, batch_size=self.batch_size, sampler=sampler, num_workers=max(0,num_workers-1), pin_memory=True, drop_last=(len(stage_X_train) > self.batch_size))
                train_loaders.append(loader)

            if X_val.shape[0] > 0:
                val_mask = (y_val_raw_split[:, 1] >= att_min) & (y_val_raw_split[:, 1] < att_max)
                stage_X_val, stage_y_val_norm = X_val[val_mask], y_val_norm[val_mask]
                val_dataset = EnhancedASLDataset(stage_X_val, stage_y_val_norm, **val_aug_config_minimal)
                val_loaders.append(DataLoader(val_dataset, batch_size=self.batch_size, num_workers=max(0,num_workers-1), pin_memory=True))
            else:
                val_loaders.append(DataLoader(EnhancedASLDataset(np.array([]), np.array([])), batch_size=self.batch_size))
        
        self.schedulers = []
        if train_loaders:
            total_steps_overall = sum(len(loader) for loader in train_loaders) * n_epochs_for_scheduler
            if total_steps_overall > 0:
                for opt in self.optimizers:
                    self.schedulers.append(OneCycleLR(opt, max_lr=opt.param_groups[0]['lr'], total_steps=total_steps_overall))
        
        return train_loaders, val_loaders, norm_stats

    def train_ensemble(self,
                   train_loaders: List[DataLoader],
                   val_loaders: List[Optional[DataLoader]],
                   epoch_schedule: List[int],
                   early_stopping_patience: int = 20) -> Dict[str, Any]:
        
        histories = defaultdict(lambda: defaultdict(list))
        self.global_step = 0
        global_epoch_counter = 0 

        if not train_loaders:
            logger.error("train_loaders is empty. Aborting training.")
            return {'final_mean_train_loss': float('nan'), 'final_mean_val_loss': float('nan'), 'all_histories': histories}

        for stage_idx, train_loader in enumerate(train_loaders):
            # --- STAGE-SPECIFIC PARAMETER ADJUSTMENTS ---
            if stage_idx == 0:
                self.custom_loss_fn.pinn_weight = self.model_config.get('loss_pinn_weight_stage1', 1.0)
                logger.info(f"--- STAGE {stage_idx+1}: Foundational Pre-training ---")
                logger.info(f"  Setting PINN weight to {self.custom_loss_fn.pinn_weight}")
            # elif stage_idx == 1:
            #     self.custom_loss_fn.pinn_weight = self.model_config.get('loss_pinn_weight_stage2', 0.1)
            #     new_lr = self.model_config.get('learning_rate_stage2', self.learning_rate / 5.0)
            #     logger.info(f"--- STAGE {stage_idx+1}: Full-Spectrum Fine-tuning ---")
            #     logger.info(f"  Setting PINN weight to {self.custom_loss_fn.pinn_weight}")
            #     logger.info(f"  Setting learning rate to {new_lr}")
            #     for opt in self.optimizers:
            #         for param_group in opt.param_groups:
            #             param_group['lr'] = new_lr
            elif stage_idx == 1:
                self.custom_loss_fn.pinn_weight = self.model_config.get('loss_pinn_weight_stage2', 0.1)
                logger.info(f"--- STAGE {stage_idx+1}: Full-Spectrum Fine-tuning ---")
                logger.info(f"  Setting PINN weight to {self.custom_loss_fn.pinn_weight}")
                # The learning rate is now managed automatically by the scheduler across both stages.
            # --- END of STAGE-SPECIFIC ADJUSTMENTS ---

            current_val_loader = val_loaders[stage_idx] if stage_idx < len(val_loaders) else None
            
            if current_val_loader: logger.info(f"  Using validation loader with {len(current_val_loader)} batches.")
            else: logger.info("  No validation loader for this stage.")
            
            logger.info(f"  Training for {epoch_schedule[stage_idx]} epochs with {len(train_loader)} train batches.")
            if len(train_loader) == 0:
                logger.warning(f"Skipping empty curriculum training stage {stage_idx + 1}.")
                continue
            
            logger.info(f"Resetting early stopping state for Stage {stage_idx+1}.")
            best_val_losses_stage = [float('inf')] * self.n_ensembles
            patience_counters_stage = [0] * self.n_ensembles

            if not hasattr(self, 'overall_best_val_losses'):
                self.overall_best_val_losses = [float('inf')] * self.n_ensembles

            n_epochs_stage = epoch_schedule[stage_idx]
            for epoch in range(n_epochs_stage):
                epoch_train_losses_all_models, epoch_val_metrics_all_models = [], []
                
                for model_idx in range(self.n_ensembles):
                    train_loss_epoch = self._train_epoch(self.models[model_idx], train_loader, 
                                                       self.optimizers[model_idx], 
                                                       self.schedulers[model_idx] if self.schedulers else None, 
                                                       global_epoch_counter)
                    epoch_train_losses_all_models.append(train_loss_epoch)
                    histories[model_idx][f'train_losses_stage_{stage_idx}'].append(train_loss_epoch)
                    
                    if current_val_loader:
                        val_metrics_dict = self._validate(self.models[model_idx], current_val_loader, global_epoch_counter)
                        epoch_val_metrics_all_models.append(val_metrics_dict)
                        histories[model_idx][f'val_metrics_stage_{stage_idx}'].append(val_metrics_dict)
                        
                        val_loss_for_es = val_metrics_dict.get('val_loss', float('inf'))
                        
                        if val_loss_for_es < best_val_losses_stage[model_idx]:
                            best_val_losses_stage[model_idx] = val_loss_for_es
                            patience_counters_stage[model_idx] = 0
                            
                            if val_loss_for_es < self.overall_best_val_losses[model_idx]:
                                self.overall_best_val_losses[model_idx] = val_loss_for_es
                                self.best_states[model_idx] = self.models[model_idx].state_dict()
                                logger.debug(f"Model {model_idx} new overall best state saved (Val loss: {val_loss_for_es:.4f})")

                        else:
                            patience_counters_stage[model_idx] += 1
                    else:
                        histories[model_idx]['val_metrics_stage_'+str(stage_idx)].append({'val_loss': float('inf')}) 

                if wandb.run:
                    mean_epoch_train_loss = np.nanmean(epoch_train_losses_all_models) if epoch_train_losses_all_models else float('nan')
                    wandb.log({f'Epoch_Stage{stage_idx}/Mean_Train_Loss': mean_epoch_train_loss, 'epoch_global': global_epoch_counter, 'epoch_stage': epoch})
                    if current_val_loader and len(current_val_loader) > 0 and epoch_val_metrics_all_models:
                        epoch_val_metrics_agg = defaultdict(list)
                        for model_metrics in epoch_val_metrics_all_models:
                            for metric_name, value in model_metrics.items():
                                epoch_val_metrics_agg[metric_name].append(value)
                        
                        for metric_name, values_list in epoch_val_metrics_agg.items():
                            mean_val_metric = np.nanmean(values_list) if values_list else float('nan')
                            wandb.log({f'Epoch_Stage{stage_idx}/Mean_Val_{metric_name.capitalize()}': mean_val_metric, 'epoch_global': global_epoch_counter, 'epoch_stage': epoch})

                if sum(1 for p_count in patience_counters_stage if p_count < early_stopping_patience) == 0 and epoch > 0 : 
                    logger.info(f"All active models early stopped within stage {stage_idx+1}, epoch {epoch+1}. Moving to next stage.")
                    break 

                if (epoch + 1) % 10 == 0:
                    active_train_losses = [histories[i][f'train_losses_stage_{stage_idx}'][-1] for i in range(self.n_ensembles) if patience_counters_stage[i] < early_stopping_patience]
                    active_val_losses_stage = [histories[i][f'val_metrics_stage_{stage_idx}'][-1]['val_loss'] for i in range(self.n_ensembles) if histories[i].get(f'val_metrics_stage_{stage_idx}') and patience_counters_stage[i] < early_stopping_patience]
                    
                    mean_train_loss_console = np.nanmean(active_train_losses) if active_train_losses else float('nan')
                    mean_val_loss_console_stage = np.nanmean(active_val_losses_stage) if active_val_losses_stage else float('nan')
                    logger.info(f"Stage {stage_idx+1}, Epoch {epoch + 1}/{n_epochs_stage}: Mean Active Train Loss = {mean_train_loss_console:.6f}, Mean Active Val Loss (Stage) = {mean_val_loss_console_stage:.6f}")
                
                global_epoch_counter += 1

        if not hasattr(self, 'overall_best_val_losses'):
            self.overall_best_val_losses = [float('inf')] * self.n_ensembles

        for model_idx, state in enumerate(self.best_states):
            if state is not None:
                self.models[model_idx].load_state_dict(state)
                logger.info(f"Loaded best overall state for model {model_idx} (Overall Val Loss: {self.overall_best_val_losses[model_idx]:.4f})")
            else: 
                logger.warning(f"No best overall state found for model {model_idx}. Using final state from last trained stage.")

        final_train_losses_list_overall = []
        for i in range(self.n_ensembles):
            model_train_losses = []
            for s_idx in range(len(train_loaders)):
                key = f'train_losses_stage_{s_idx}'
                if key in histories[i] and histories[i][key]:
                    model_train_losses.extend(histories[i][key])
            if model_train_losses:
                final_train_losses_list_overall.append(np.nanmean(model_train_losses))

        final_val_losses_list_overall = [self.overall_best_val_losses[i] for i in range(self.n_ensembles) if self.overall_best_val_losses[i] != float('inf')]
        
        final_mean_train_loss_overall = np.nanmean(final_train_losses_list_overall) if final_train_losses_list_overall else float('nan')
        final_mean_val_loss_overall = np.nanmean(final_val_losses_list_overall) if final_val_losses_list_overall else float('nan')

        if wandb.run:
            wandb.summary['final_mean_train_loss_overall'] = final_mean_train_loss_overall
            wandb.summary['final_mean_val_loss_overall'] = final_mean_val_loss_overall

        return {'final_mean_train_loss': final_mean_train_loss_overall, 'final_mean_val_loss': final_mean_val_loss_overall, 'all_histories': histories}

    def _train_epoch(self, model, train_loader, optimizer, scheduler, current_global_epoch: int) -> float:
        model.train(); total_loss = 0.0
        for signals, params_norm in train_loader:
            signals, params_norm = signals.to(self.device), params_norm.to(self.device)
            optimizer.zero_grad()
            
            # Unpack all model outputs, including the rough estimates
            outputs = model(signals)
            cbf_mean_norm, att_mean_norm, cbf_log_var, att_log_var, cbf_rough, att_rough = outputs

            loss = self.custom_loss_fn(signals, cbf_mean_norm, att_mean_norm, params_norm[:, 0:1], params_norm[:, 1:2], 
                                       cbf_log_var, att_log_var, cbf_rough, att_rough, current_global_epoch)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            if scheduler: scheduler.step() 
            total_loss += loss.item()
            self.global_step += 1
        return total_loss / len(train_loader) if len(train_loader) > 0 else 0.0

    def _validate(self, model, val_loader, current_global_epoch: int) -> Dict[str, float]:
        model.eval(); total_loss_val = 0.0
        all_cbf_preds_norm, all_att_preds_norm = [], []
        all_cbf_trues_norm, all_att_trues_norm = [], []
        all_cbf_log_vars, all_att_log_vars = [], []

        with torch.no_grad():
            for signals, params_norm in val_loader:
                signals, params_norm = signals.to(self.device), params_norm.to(self.device)
                outputs = model(signals)
                cbf_mean_norm, att_mean_norm, cbf_log_var, att_log_var, cbf_rough, att_rough = outputs

                loss = self.custom_loss_fn(signals, cbf_mean_norm, att_mean_norm, params_norm[:, 0:1], params_norm[:, 1:2], 
                                           cbf_log_var, att_log_var, cbf_rough, att_rough, current_global_epoch)
                total_loss_val += loss.item()
                all_cbf_preds_norm.append(cbf_mean_norm.cpu()); all_att_preds_norm.append(att_mean_norm.cpu())
                all_cbf_trues_norm.append(params_norm[:, 0:1].cpu()); all_att_trues_norm.append(params_norm[:, 1:2].cpu())
                all_cbf_log_vars.append(cbf_log_var.cpu()); all_att_log_vars.append(att_log_var.cpu())
        
        avg_loss_val = total_loss_val / len(val_loader) if len(val_loader) > 0 else float('inf')
        metrics_dict = {'val_loss': avg_loss_val}

        if all_cbf_preds_norm and self.norm_stats:
            y_mean_cbf = self.norm_stats.get('y_mean_cbf', 0.0)
            y_std_cbf = self.norm_stats.get('y_std_cbf', 1.0)
            y_mean_att = self.norm_stats.get('y_mean_att', 0.0)
            y_std_att = self.norm_stats.get('y_std_att', 1.0)

            cbf_preds_norm_cat = torch.cat(all_cbf_preds_norm).numpy().squeeze()
            att_preds_norm_cat = torch.cat(all_att_preds_norm).numpy().squeeze()
            cbf_trues_norm_cat = torch.cat(all_cbf_trues_norm).numpy().squeeze()
            att_trues_norm_cat = torch.cat(all_att_trues_norm).numpy().squeeze()

            cbf_preds_denorm = cbf_preds_norm_cat * y_std_cbf + y_mean_cbf
            att_preds_denorm = att_preds_norm_cat * y_std_att + y_mean_att
            cbf_trues_denorm = cbf_trues_norm_cat * y_std_cbf + y_mean_cbf
            att_trues_denorm = att_trues_norm_cat * y_std_att + y_mean_att
            
            if len(cbf_preds_denorm) > 0 : 
                metrics_dict['cbf_mae'] = mean_absolute_error(cbf_trues_denorm, cbf_preds_denorm)
                metrics_dict['cbf_rmse'] = np.sqrt(mean_squared_error(cbf_trues_denorm, cbf_preds_denorm))
                metrics_dict['att_mae'] = mean_absolute_error(att_trues_denorm, att_preds_denorm)
                metrics_dict['att_rmse'] = np.sqrt(mean_squared_error(att_trues_denorm, att_preds_denorm))

                cbf_log_vars_cat = torch.cat(all_cbf_log_vars).numpy().squeeze()
                att_log_vars_cat = torch.cat(all_att_log_vars).numpy().squeeze()
                metrics_dict['mean_cbf_log_var'] = np.mean(cbf_log_vars_cat)
                metrics_dict['mean_att_log_var'] = np.mean(att_log_vars_cat)
        return metrics_dict

    def predict(self, signals: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        signals_tensor = torch.FloatTensor(signals).to(self.device)
        if signals_tensor.ndim == 1: signals_tensor = signals_tensor.unsqueeze(0)
        
        all_cbf_means_norm_list, all_att_means_norm_list = [], []
        all_cbf_aleatoric_vars_list, all_att_aleatoric_vars_list = [], []

        for model in self.models:
            model.eval()
            with torch.no_grad():
                # We only need the final predictions here, so we slice the model output
                cbf_mean_norm, att_mean_norm, cbf_log_var, att_log_var, _, _ = model(signals_tensor)
                all_cbf_means_norm_list.append(cbf_mean_norm.cpu().numpy())
                all_att_means_norm_list.append(att_mean_norm.cpu().numpy())
                all_cbf_aleatoric_vars_list.append(torch.exp(cbf_log_var).cpu().numpy())
                all_att_aleatoric_vars_list.append(torch.exp(att_log_var).cpu().numpy())
        
        if signals_tensor.shape[0] == 1:
            all_cbf_means_norm_np = np.array(all_cbf_means_norm_list).squeeze() 
            all_att_means_norm_np = np.array(all_att_means_norm_list).squeeze() 
            all_cbf_aleatoric_vars_np = np.array(all_cbf_aleatoric_vars_list).squeeze() 
            all_att_aleatoric_vars_np = np.array(all_att_aleatoric_vars_list).squeeze() 
            
            # --- Tier 1: Uncertainty-Weighted Averaging ---
            cbf_weights = 1.0 / (all_cbf_aleatoric_vars_np + 1e-9)
            ensemble_cbf_mean_norm = np.average(all_cbf_means_norm_np, weights=cbf_weights)
            
            att_weights = 1.0 / (all_att_aleatoric_vars_np + 1e-9)
            ensemble_att_mean_norm = np.average(all_att_means_norm_np, weights=att_weights)
            
            mean_aleatoric_cbf_var_norm = np.mean(all_cbf_aleatoric_vars_np)
            mean_aleatoric_att_var_norm = np.mean(all_att_aleatoric_vars_np)
            epistemic_cbf_var_norm = np.var(all_cbf_means_norm_np) if self.n_ensembles > 1 else 0.0
            epistemic_att_var_norm = np.var(all_att_means_norm_np) if self.n_ensembles > 1 else 0.0

        else:
            all_cbf_means_norm_np = np.concatenate(all_cbf_means_norm_list, axis=1) 
            all_att_means_norm_np = np.concatenate(all_att_means_norm_list, axis=1) 
            all_cbf_aleatoric_vars_np = np.concatenate(all_cbf_aleatoric_vars_list, axis=1) 
            all_att_aleatoric_vars_np = np.concatenate(all_att_aleatoric_vars_list, axis=1) 

            # --- Tier 1: Uncertainty-Weighted Averaging ---
            cbf_weights = 1.0 / (all_cbf_aleatoric_vars_np + 1e-9)
            ensemble_cbf_mean_norm = np.average(all_cbf_means_norm_np, axis=1, weights=cbf_weights)

            att_weights = 1.0 / (all_att_aleatoric_vars_np + 1e-9)
            ensemble_att_mean_norm = np.average(all_att_means_norm_np, axis=1, weights=att_weights)

            mean_aleatoric_cbf_var_norm = np.mean(all_cbf_aleatoric_vars_np, axis=1)
            mean_aleatoric_att_var_norm = np.mean(all_att_aleatoric_vars_np, axis=1)
            epistemic_cbf_var_norm = np.var(all_cbf_means_norm_np, axis=1) if self.n_ensembles > 1 else np.zeros_like(ensemble_cbf_mean_norm)
            epistemic_att_var_norm = np.var(all_att_means_norm_np, axis=1) if self.n_ensembles > 1 else np.zeros_like(ensemble_att_mean_norm)

        y_mean_cbf, y_std_cbf, y_mean_att, y_std_att = 0.0, 1.0, 0.0, 1.0
        if self.norm_stats:
            y_mean_cbf = self.norm_stats.get('y_mean_cbf', 0.0)
            y_std_cbf = self.norm_stats.get('y_std_cbf', 1.0)
            y_mean_att = self.norm_stats.get('y_mean_att', 0.0)
            y_std_att = self.norm_stats.get('y_std_att', 1.0)

        ensemble_cbf_mean_denorm = ensemble_cbf_mean_norm * y_std_cbf + y_mean_cbf
        ensemble_att_mean_denorm = ensemble_att_mean_norm * y_std_att + y_mean_att
        
        total_cbf_var_norm = mean_aleatoric_cbf_var_norm + epistemic_cbf_var_norm
        total_att_var_norm = mean_aleatoric_att_var_norm + epistemic_att_var_norm
        
        total_cbf_var_denorm = total_cbf_var_norm * (y_std_cbf**2)
        total_att_var_denorm = total_att_var_norm * (y_std_att**2)
        
        total_cbf_std_denorm = np.sqrt(np.maximum(total_cbf_var_denorm, 0)) 
        total_att_std_denorm = np.sqrt(np.maximum(total_att_var_denorm, 0))
        
        return ensemble_cbf_mean_denorm, ensemble_att_mean_denorm, total_cbf_std_denorm, total_att_std_denorm