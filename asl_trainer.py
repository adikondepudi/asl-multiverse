// FILE: asl_trainer.py
# FILE: asl_trainer.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, IterableDataset
from torch.amp.autocast_mode import autocast
import numpy as np
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass
from collections import defaultdict
from torch.optim.lr_scheduler import OneCycleLR
import math
import multiprocessing as mp
from pathlib import Path
import wandb 
from sklearn.metrics import mean_absolute_error, mean_squared_error 
from torch.cuda.amp import GradScaler, autocast
import time
from itertools import islice
import traceback
import optuna

num_workers = mp.cpu_count()

from enhanced_asl_network import EnhancedASLNet, CustomLoss, DisentangledASLNet
from enhanced_simulation import RealisticASLSimulator, _generate_single_balanced_subject
from utils import engineer_signal_features

import logging
logger = logging.getLogger(__name__)
if not logger.hasHandlers():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

class ASLNet(nn.Module):
    """Neural network for ASL parameter estimation"""

    def __init__(self, input_size: int, hidden_sizes: List[int] = [64, 32, 16]):
        super().__init__()
        layers = []
        prev_size = input_size
        for hidden_size in hidden_sizes:
            layers.extend([nn.Linear(prev_size, hidden_size), nn.ReLU(), nn.BatchNorm1d(hidden_size), nn.Dropout(0.1)])
            prev_size = hidden_size
        layers.append(nn.Linear(prev_size, 2))
        self.network = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x)

class ASLDataset(Dataset):
    """Dataset for ASL signals and parameters"""
    def __init__(self, signals: np.ndarray, params: np.ndarray):
        self.signals = torch.FloatTensor(signals)
        self.params = torch.FloatTensor(params)

    def __len__(self) -> int:
        return len(self.signals)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        return self.signals[idx], self.params[idx]

class ASLTrainer:
    """Training manager for ASL parameter estimation network"""
    def __init__(self, input_size: int, hidden_sizes: List[int] = [64, 32, 16], learning_rate: float = 1e-3, batch_size: int = 32, device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):
        self.device = device
        self.batch_size = batch_size
        self.model = ASLNet(input_size, hidden_sizes).to(device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.criterion = nn.MSELoss()
        self.train_losses = []
        self.val_losses = []

    def prepare_data(self, simulator, n_samples: int = 10000, val_split: float = 0.2, plds_definition: Optional[np.ndarray] = None) -> Tuple[DataLoader, DataLoader]:
        if plds_definition is None: plds = np.arange(500, 3001, 500)
        else: plds = plds_definition
        att_values = np.arange(0, 4001, 100)
        signals_dict = simulator.generate_synthetic_data(plds, att_values, n_noise=n_samples, tsnr=5.0, cbf_val=simulator.params.CBF)
        num_total_instances = n_samples * len(att_values)
        X = np.zeros((num_total_instances, len(plds) * 2))
        pcasl_all = signals_dict['PCASL'].reshape(num_total_instances, len(plds))
        vsasl_all = signals_dict['VSASL'].reshape(num_total_instances, len(plds))
        X[:, :len(plds)] = pcasl_all
        X[:, len(plds):] = vsasl_all
        cbf_true_val = simulator.params.CBF
        cbf_params = np.full(num_total_instances, cbf_true_val)
        att_params = np.tile(np.repeat(att_values, 1), n_samples)
        y = np.column_stack((cbf_params, att_params))
        if num_total_instances == 0: raise ValueError("No data generated by simulator.prepare_data.")
        n_val = int(num_total_instances * val_split)
        if n_val == 0 and num_total_instances > 1: n_val = 1
        if n_val >= num_total_instances : n_val = num_total_instances -1 if num_total_instances > 0 else 0
        X_train, X_val = X[:-n_val], X[-n_val:]
        y_train, y_val = y[:-n_val], y[-n_val:]
        if X_train.shape[0] == 0: raise ValueError("Training set is empty after split.")
        train_dataset = ASLDataset(X_train, y_train)
        val_dataset = ASLDataset(X_val, y_val)
        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=self.batch_size if X_val.shape[0] > 0 else 1)
        return train_loader, val_loader

    def train_epoch(self, train_loader: DataLoader) -> float:
        self.model.train()
        total_loss = 0.0
        for signals, params in train_loader:
            signals, params = signals.to(self.device), params.to(self.device)
            self.optimizer.zero_grad()
            outputs = self.model(signals)
            loss = self.criterion(outputs, params)
            loss.backward()
            self.optimizer.step()
            total_loss += loss.item()
        return total_loss / len(train_loader) if len(train_loader) > 0 else 0.0

    def validate(self, val_loader: DataLoader) -> float:
        if val_loader is None or len(val_loader) == 0:
            logger.warning("Validation loader is empty or None, skipping validation.")
            return float('inf')
        self.model.eval()
        total_loss = 0.0
        with torch.no_grad():
            for signals, params in val_loader:
                signals, params = signals.to(self.device), params.to(self.device)
                outputs = self.model(signals)
                loss = self.criterion(outputs, params)
                total_loss += loss.item()
        return total_loss / len(val_loader) if len(val_loader) > 0 else float('inf')

    def train(self, train_loader: DataLoader, val_loader: DataLoader, n_epochs: int = 100, early_stopping_patience: int = 10) -> Dict[str, List[float]]:
        best_val_loss = float('inf')
        patience_counter = 0
        for epoch in range(n_epochs):
            train_loss = self.train_epoch(train_loader)
            val_loss = self.validate(val_loader)
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                torch.save(self.model.state_dict(), 'best_model_ASLTrainer.pt')
            else:
                patience_counter += 1
            if patience_counter >= early_stopping_patience:
                logger.info(f'Early stopping triggered at epoch {epoch + 1}')
                break
            if (epoch + 1) % 10 == 0:
                logger.info(f'Epoch {epoch + 1}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}')
        model_path = Path('best_model_ASLTrainer.pt')
        if model_path.exists():
            self.model.load_state_dict(torch.load(str(model_path)))
            logger.info(f"Loaded best model from {model_path}")
        else:
            logger.warning(f"Best model file {model_path} not found. Current model state will be used.")
        return {'train_losses': self.train_losses, 'val_losses': self.val_losses}

    def predict(self, signals: np.ndarray) -> np.ndarray:
        self.model.eval()
        with torch.no_grad():
            signals_tensor = torch.FloatTensor(signals).to(self.device)
            if signals_tensor.ndim == 1: signals_tensor = signals_tensor.unsqueeze(0)
            predictions = self.model(signals_tensor)
            return predictions.cpu().numpy()

    def evaluate_performance(self, test_signals: np.ndarray, true_params: np.ndarray) -> Dict[str, float]:
        predictions = self.predict(test_signals)
        mae_cbf = np.mean(np.abs(predictions[:,0] - true_params[:,0]))
        mae_att = np.mean(np.abs(predictions[:,1] - true_params[:,1]))
        rmse_cbf = np.sqrt(np.mean((predictions[:,0] - true_params[:,0])**2))
        rmse_att = np.sqrt(np.mean((predictions[:,1] - true_params[:,1])**2))
        rel_error_cbf = np.mean(np.abs(predictions[:,0] - true_params[:,0]) / np.clip(true_params[:,0], 1e-6, None))
        rel_error_att = np.mean(np.abs(predictions[:,1] - true_params[:,1]) / np.clip(true_params[:,1], 1e-6, None))
        return {'MAE_CBF': mae_cbf, 'MAE_ATT': mae_att, 'RMSE_CBF': rmse_cbf, 'RMSE_ATT': rmse_att, 'RelError_CBF': rel_error_cbf, 'RelError_ATT': rel_error_att}

class EnhancedASLDataset(Dataset):
    def __init__(self, signals: np.ndarray, params: np.ndarray, **kwargs):
        self.signals = torch.FloatTensor(signals)
        self.params = torch.FloatTensor(params)
    def __len__(self) -> int: return len(self.signals)
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]: return self.signals[idx], self.params[idx]

class ASLIterableDataset(IterableDataset):
    """
    An iterable dataset for on-the-fly data generation.
    MODIFIED to support both original and new disentangled data formats.
    """
    def __init__(self, simulator: RealisticASLSimulator, plds: np.ndarray, 
                 noise_levels: List[float], norm_stats: Dict, num_att_bins: int = 14,
                 disentangled_mode: bool = False):
        super().__init__()
        self.simulator = simulator
        self.plds = plds
        self.num_plds = len(plds)
        self.noise_levels = noise_levels
        self.num_att_bins = num_att_bins
        self.base_params = simulator.params
        self.physio_var = simulator.physio_var
        self.att_range = self.physio_var.att_range
        self.cbf_range = self.physio_var.cbf_range
        self.t1_range = self.physio_var.t1_artery_range
        self.norm_stats = norm_stats
        self.disentangled_mode = disentangled_mode

    def __iter__(self):
        worker_info = torch.utils.data.get_worker_info()
        if worker_info is not None: np.random.seed(worker_info.id + int(time.time() * 1000) % (2**32))
        else: np.random.seed(int(time.time() * 1000) % (2**32))

        consecutive_failures = 0
        max_consecutive_failures = 100 

        while True:
            try:
                # 1. Stratified ATT sampling
                att_bins = np.linspace(self.att_range[0], self.att_range[1], self.num_att_bins + 1)
                att_bin_idx = np.random.randint(0, self.num_att_bins)
                true_att = np.random.uniform(att_bins[att_bin_idx], att_bins[att_bin_idx+1])
                true_cbf = np.random.uniform(*self.cbf_range)
                true_t1_artery = np.random.uniform(*self.t1_range)
                current_snr = np.random.choice(self.noise_levels)
                
                # 2. Apply sequence parameter perturbations for robustness
                perturbed_t_tau = self.base_params.T_tau * (1 + np.random.uniform(*self.physio_var.t_tau_perturb_range))
                perturbed_alpha_pcasl = np.clip(self.base_params.alpha_PCASL * (1 + np.random.uniform(*self.physio_var.alpha_perturb_range)), 0.1, 1.1)
                perturbed_alpha_vsasl = np.clip(self.base_params.alpha_VSASL * (1 + np.random.uniform(*self.physio_var.alpha_perturb_range)), 0.1, 1.0)

                # 3. Generate the noisy signal
                data_dict = self.simulator.generate_synthetic_data(
                    self.plds, att_values=np.array([true_att]), n_noise=1, tsnr=current_snr,
                    cbf_val=true_cbf, t1_artery_val=true_t1_artery, t_tau_val=perturbed_t_tau,
                    alpha_pcasl_val=perturbed_alpha_pcasl, alpha_vsasl_val=perturbed_alpha_vsasl)
                
                pcasl_noisy = data_dict['MULTIVERSE'][0, 0, :, 0]
                vsasl_noisy = data_dict['MULTIVERSE'][0, 0, :, 1]
                raw_signal = np.concatenate([pcasl_noisy, vsasl_noisy])
                
                # 4. Engineer features from raw signal
                eng_features = engineer_signal_features(raw_signal, self.num_plds)
                
                if self.disentangled_mode:
                    # 5a. Disentangled Input Generation
                    amplitude = np.linalg.norm(raw_signal) + 1e-6
                    shape_vector = raw_signal / amplitude
                    
                    amplitude_norm = (amplitude - self.norm_stats['amplitude_mean']) / (self.norm_stats['amplitude_std'] + 1e-6)

                    # Concatenate shape, features, and amplitude for the model
                    final_input = np.concatenate([shape_vector, eng_features.flatten(), np.array([amplitude_norm])])
                else:
                    # 5b. Original Input Generation
                    pcasl_norm = (raw_signal[:self.num_plds] - self.norm_stats['pcasl_mean']) / (np.array(self.norm_stats['pcasl_std']) + 1e-6)
                    vsasl_norm = (raw_signal[self.num_plds:] - self.norm_stats['vsasl_mean']) / (np.array(self.norm_stats['vsasl_std']) + 1e-6)
                    final_input = np.concatenate([pcasl_norm, vsasl_norm, eng_features.flatten()])

                # 6. Normalize the target parameters
                params = np.array([true_cbf, true_att])
                params_norm = np.array([
                    (params[0] - self.norm_stats['y_mean_cbf']) / self.norm_stats['y_std_cbf'],
                    (params[1] - self.norm_stats['y_mean_att']) / self.norm_stats['y_std_att']
                ])
                
                yield torch.from_numpy(final_input.astype(np.float32)), torch.from_numpy(params_norm.astype(np.float32))

                consecutive_failures = 0

            except Exception as e:
                worker_id = worker_info.id if worker_info else -1
                logger.error(f"DataLoader worker {worker_id} failed to generate a sample: {e}")
                traceback.print_exc()

                consecutive_failures += 1
                if consecutive_failures >= max_consecutive_failures:
                    raise RuntimeError(
                        f"DataLoader worker {worker_id} has failed {max_consecutive_failures} consecutive times. "
                        "This indicates a persistent bug in data generation. Aborting training."
                    ) from e
                
                continue 

class EnhancedASLTrainer:
    def __init__(self,
                 model_config: Dict, model_class: callable, input_size: int,
                 weight_decay: float = 1e-5, batch_size: int = 256, n_ensembles: int = 5,
                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu',
                 n_plds_for_model: Optional[int] = None, m0_input_feature_model: bool = False):
        self.device = torch.device(device)
        self.batch_size = batch_size
        self.model_config = model_config # Store the full config

        self.lr_base = model_config.get('learning_rate', 0.001)
        self.weight_decay = weight_decay
        self.n_ensembles = n_ensembles
        self.validation_steps_per_epoch = model_config.get('validation_steps_per_epoch', 50)
        self.scalers = [torch.cuda.amp.GradScaler() for _ in range(self.n_ensembles)]

        logger.info("Initializing models and casting to bfloat16 for mixed-precision training compatibility.")
        self.models = [model_class(**model_config).to(self.device, dtype=torch.bfloat16) for _ in range(n_ensembles)]
        
        print("INFO: Compiling models with torch.compile() for optimized performance...")
        self.models = [torch.compile(m, mode="max-autotune") for m in self.models]
        print("INFO: Model compilation complete.")

        if wandb.run:
            for i, model in enumerate(self.models):
                wandb.watch(model, log='all', log_freq=200, idx=i)

        self.best_states = [None] * self.n_ensembles
        
        self.optimizers = []
        self.schedulers = [] 
        
        loss_params = {
            'w_cbf': model_config.get('loss_weight_cbf', 1.0), 
            'w_att': model_config.get('loss_weight_att', 1.0),
            'log_var_reg_lambda': model_config.get('loss_log_var_reg_lambda', 0.0),
            'pinn_weight': model_config.get('loss_pinn_weight', 0.0),
            'model_params': model_config,
        }
        self.custom_loss_fn = CustomLoss(**loss_params)

        self.train_losses = defaultdict(list)
        self.val_metrics = defaultdict(list)
        self.global_step = 0
        self.norm_stats = None

    def train_ensemble(self,
                       train_loader: DataLoader, val_loader: Optional[DataLoader],
                       n_epochs: int, steps_per_epoch: Optional[int],
                       early_stopping_patience: int = 20, optuna_trial: Optional[Any] = None) -> Dict[str, Any]:
        
        histories = defaultdict(lambda: defaultdict(list))
        self.global_step = 0
        
        if not train_loader:
            logger.error("train_loader is empty. Aborting training.")
            return {'final_mean_train_loss': float('nan'), 'final_mean_val_loss': float('nan'), 'all_histories': histories}
        
        self.overall_best_val_losses = [float('inf')] * self.n_ensembles
        
        if steps_per_epoch is None:
            steps_per_epoch = len(train_loader)
            logger.info(f"Using map-style dataset. One epoch will be {steps_per_epoch} steps (full dataset).")
        
        logger.info(f"--- Starting Unified Training for {n_epochs} epochs ---")
        logger.info(f"  PINN Weight: {self.custom_loss_fn.pinn_weight}, Max LR: {self.lr_base:.6f}")

        self.optimizers = []
        for model in self.models:
            self.optimizers.append(torch.optim.AdamW(model.parameters(), lr=self.lr_base, weight_decay=self.weight_decay, betas=(0.9, 0.98)))
        
        total_steps = steps_per_epoch * n_epochs
        self.schedulers = [torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=self.lr_base, total_steps=total_steps) for opt in self.optimizers]
        best_val_losses_stage = [float('inf')] * self.n_ensembles
        patience_counters = [0] * self.n_ensembles
        
        for epoch in range(n_epochs):
            epoch_train_losses_all_models = defaultdict(list)
            epoch_val_metrics_all_models = []
            
            train_loader_iter = iter(train_loader)
            for model_idx in range(self.n_ensembles):
                train_loss_epoch, train_loss_components = self._train_epoch(self.models[model_idx], train_loader_iter, self.optimizers[model_idx], self.scalers[model_idx], self.schedulers[model_idx], epoch, steps_per_epoch)
                epoch_train_losses_all_models['total_loss'].append(train_loss_epoch)
                for key, val in train_loss_components.items(): epoch_train_losses_all_models[key].append(val)
                histories[model_idx]['train_losses'].append(train_loss_epoch)
                if model_idx < self.n_ensembles - 1: train_loader_iter = iter(train_loader)

            if val_loader is not None and len(val_loader.dataset) > 0:
                for model_idx in range(self.n_ensembles):
                    val_metrics_dict = self._validate(self.models[model_idx], val_loader, epoch)
                    epoch_val_metrics_all_models.append(val_metrics_dict)
                    histories[model_idx]['val_metrics'].append(val_metrics_dict)
                    val_loss_for_es = val_metrics_dict.get('val_loss', float('inf'))
                    
                    if val_loss_for_es < best_val_losses_stage[model_idx]:
                        best_val_losses_stage[model_idx] = val_loss_for_es
                        patience_counters[model_idx] = 0
                        if val_loss_for_es < self.overall_best_val_losses[model_idx]:
                            self.overall_best_val_losses[model_idx] = val_loss_for_es
                            unwrapped_model = getattr(self.models[model_idx], '_orig_mod', self.models[model_idx])
                            self.best_states[model_idx] = unwrapped_model.state_dict()
                            logger.debug(f"Model {model_idx} new overall best state saved (Val loss: {val_loss_for_es:.4f})")
                    else:
                        patience_counters[model_idx] += 1
            
            if wandb.run:
                log_dict = {'epoch': epoch}
                for key, values in epoch_train_losses_all_models.items():
                    log_dict[f'train/mean_{key}'] = np.nanmean(values)
                if epoch_val_metrics_all_models:
                    aggregated_val_metrics = defaultdict(list)
                    for m_dict in epoch_val_metrics_all_models:
                        for key, value in m_dict.items():
                            if value is not None and np.isfinite(value): aggregated_val_metrics[key].append(value)
                    for key, values in aggregated_val_metrics.items():
                        log_dict[f'val/mean_{key}'] = np.nanmean(values)
                wandb.log(log_dict, step=self.global_step)

            if optuna_trial:
                mean_val_loss_for_pruning = np.nanmean([m.get('val_loss', np.nan) for m in epoch_val_metrics_all_models]) if epoch_val_metrics_all_models else float('inf')
                optuna_trial.report(mean_val_loss_for_pruning, epoch)
                if optuna_trial.should_prune(): raise optuna.exceptions.TrialPruned()

            if all(p_count >= early_stopping_patience for p_count in patience_counters): 
                logger.info(f"All models early stopped at epoch {epoch+1}. Ending training.")
                break 
            if (epoch + 1) % 10 == 0:
                mean_train_loss_console = np.nanmean(epoch_train_losses_all_models.get('total_loss', [np.nan]))
                mean_val_loss_console = np.nanmean([m.get('val_loss', np.nan) for m in epoch_val_metrics_all_models]) if epoch_val_metrics_all_models else np.nan
                logger.info(f"Epoch {epoch + 1}/{n_epochs}: Mean Train Loss = {mean_train_loss_console:.6f}, Mean Val Loss = {mean_val_loss_console:.6f}")

        for model_idx, state in enumerate(self.best_states):
            if state is not None:
                unwrapped_model = getattr(self.models[model_idx], '_orig_mod', self.models[model_idx])
                unwrapped_model.load_state_dict(state)
                logger.info(f"Loaded best overall state for model {model_idx} (Val Loss: {self.overall_best_val_losses[model_idx]:.4f})")
            else:
                logger.warning(f"No best state found for model {model_idx}. Using final state.")

        final_val_losses_list_overall = [loss for loss in self.overall_best_val_losses if loss != float('inf')]
        final_mean_val_loss_overall = np.nanmean(final_val_losses_list_overall) if final_val_losses_list_overall else float('nan')
        
        if wandb.run: wandb.summary['final_mean_val_loss_overall'] = final_mean_val_loss_overall
        return {'final_mean_val_loss': final_mean_val_loss_overall, 'all_histories': histories}

    def _train_epoch(self, model, train_loader_iter, optimizer, scaler, scheduler, current_global_epoch: int, steps_per_epoch: int) -> Tuple[float, Dict[str, float]]:
        model.train()
        total_loss = 0.0
        total_components = defaultdict(float)
        
        log_interval = max(1, steps_per_epoch // 10)

        for i, (signals, params_norm) in enumerate(islice(train_loader_iter, steps_per_epoch)):
            signals, params_norm = signals.to(self.device), params_norm.to(self.device)
            optimizer.zero_grad(set_to_none=True)

            with torch.amp.autocast(device_type=self.device.type, dtype=torch.bfloat16):
                outputs = model(signals)
                cbf_mean_norm, att_mean_norm, cbf_log_var, att_log_var, cbf_rough, att_rough = outputs
                loss, loss_components = self.custom_loss_fn(signals, cbf_mean_norm, att_mean_norm, params_norm[:, 0:1], params_norm[:, 1:2], cbf_log_var, att_log_var, cbf_rough, att_rough, current_global_epoch)

            model_is_bfloat16 = next(model.parameters()).dtype == torch.bfloat16

            if model_is_bfloat16:
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
            elif scaler is not None:
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer) 
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()
            else:
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()

            if scheduler: scheduler.step()
            
            total_loss += loss.item()
            for key, value in loss_components.items(): total_components[key] += value.item()
            
            if wandb.run and scheduler: wandb.log({'lr': scheduler.get_last_lr()[0]}, step=self.global_step)
            
            if (i + 1) % log_interval == 0 or (i + 1) == steps_per_epoch:
                logger.info(f"  Epoch {current_global_epoch+1} Training | Step [{i+1}/{steps_per_epoch}] | Current Loss: {loss.item():.4f}")

            self.global_step += 1
        
        num_steps = i + 1
        mean_total_loss = total_loss / num_steps if num_steps > 0 else 0.0
        mean_components = {key: val / num_steps for key, val in total_components.items()} if num_steps > 0 else {}
        
        return mean_total_loss, mean_components

    def _validate(self, model, val_loader, current_global_epoch: int) -> Dict[str, float]:
        model.eval(); total_loss_val = 0.0
        all_cbf_preds_norm, all_att_preds_norm, all_cbf_trues_norm, all_att_trues_norm = [], [], [], []
        all_cbf_log_vars, all_att_log_vars = [], []

        is_iterable = isinstance(val_loader.dataset, IterableDataset)
        val_iterator = islice(val_loader, self.validation_steps_per_epoch) if is_iterable else val_loader
        total_steps = self.validation_steps_per_epoch if is_iterable else len(val_loader)
        log_interval = max(1, total_steps // 4)

        with torch.no_grad():
            for i, (signals, params_norm) in enumerate(val_iterator):
                signals, params_norm = signals.to(self.device), params_norm.to(self.device)
                with torch.amp.autocast(device_type=self.device.type, dtype=torch.bfloat16):
                    outputs = model(signals)
                    cbf_mean_norm, att_mean_norm, cbf_log_var, att_log_var, cbf_rough, att_rough = outputs
                    loss, _ = self.custom_loss_fn(signals, cbf_mean_norm, att_mean_norm, params_norm[:, 0:1], params_norm[:, 1:2], cbf_log_var, att_log_var, cbf_rough, att_rough, current_global_epoch)
                total_loss_val += loss.item()
                all_cbf_preds_norm.append(cbf_mean_norm.cpu()); all_att_preds_norm.append(att_mean_norm.cpu())
                all_cbf_trues_norm.append(params_norm[:, 0:1].cpu()); all_att_trues_norm.append(params_norm[:, 1:2].cpu())
                all_cbf_log_vars.append(cbf_log_var.cpu()); all_att_log_vars.append(att_log_var.cpu())
                if (i + 1) % log_interval == 0 or (i + 1) == total_steps:
                    logger.info(f"  Epoch {current_global_epoch+1} Validation | Step [{i+1}/{total_steps}]")
        
        avg_loss_val = total_loss_val / total_steps if total_steps > 0 else float('inf')
        metrics_dict = {'val_loss': avg_loss_val}

        if all_cbf_preds_norm and self.norm_stats:
            y_mean_cbf, y_std_cbf = self.norm_stats.get('y_mean_cbf', 0.0), self.norm_stats.get('y_std_cbf', 1.0)
            y_mean_att, y_std_att = self.norm_stats.get('y_mean_att', 0.0), self.norm_stats.get('y_std_att', 1.0)
            cbf_preds_norm_cat = torch.cat(all_cbf_preds_norm).float().numpy().squeeze(); att_preds_norm_cat = torch.cat(all_att_preds_norm).float().numpy().squeeze()
            cbf_trues_norm_cat = torch.cat(all_cbf_trues_norm).float().numpy().squeeze(); att_trues_norm_cat = torch.cat(all_att_trues_norm).float().numpy().squeeze()
            cbf_preds_denorm = cbf_preds_norm_cat * y_std_cbf + y_mean_cbf; att_preds_denorm = att_preds_norm_cat * y_std_att + y_mean_att
            cbf_trues_denorm = cbf_trues_norm_cat * y_std_cbf + y_mean_cbf; att_trues_denorm = att_trues_norm_cat * y_std_att + y_mean_att
            if len(cbf_preds_denorm) > 0 : 
                metrics_dict['cbf_mae'] = mean_absolute_error(cbf_trues_denorm, cbf_preds_denorm)
                metrics_dict['cbf_rmse'] = np.sqrt(mean_squared_error(cbf_trues_denorm, cbf_preds_denorm))
                metrics_dict['att_mae'] = mean_absolute_error(att_trues_denorm, att_preds_denorm)
                metrics_dict['att_rmse'] = np.sqrt(mean_squared_error(att_trues_denorm, att_preds_denorm))
                metrics_dict['cbf_mae_norm'] = mean_absolute_error(cbf_trues_norm_cat, cbf_preds_norm_cat)
                metrics_dict['att_mae_norm'] = mean_absolute_error(att_trues_norm_cat, att_preds_norm_cat)
                cbf_log_vars_cat = torch.cat(all_cbf_log_vars).float().numpy().squeeze(); att_log_vars_cat = torch.cat(all_att_log_vars).float().numpy().squeeze()
                metrics_dict['mean_cbf_log_var'] = np.mean(cbf_log_vars_cat); metrics_dict['mean_att_log_var'] = np.mean(att_log_vars_cat)
        return metrics_dict

    def predict(self, signals: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        signals_tensor = torch.FloatTensor(signals).to(self.device, dtype=torch.bfloat16)
        if signals_tensor.ndim == 1: signals_tensor = signals_tensor.unsqueeze(0)
        all_cbf_means_norm_list, all_att_means_norm_list = [], []; all_cbf_aleatoric_vars_list, all_att_aleatoric_vars_list = [], []
        for model in self.models:
            model.eval()
            with torch.no_grad():
                cbf_mean_norm, att_mean_norm, cbf_log_var, att_log_var, _, _ = model(signals_tensor)
                all_cbf_means_norm_list.append(cbf_mean_norm.cpu().numpy()); all_att_means_norm_list.append(att_mean_norm.cpu().numpy())
                all_cbf_aleatoric_vars_list.append(torch.exp(cbf_log_var).cpu().numpy()); all_att_aleatoric_vars_list.append(torch.exp(att_log_var).cpu().numpy())
        if signals_tensor.shape[0] == 1:
            all_cbf_means_norm_np = np.array(all_cbf_means_norm_list).squeeze(); all_att_means_norm_np = np.array(all_att_means_norm_list).squeeze() 
            all_cbf_aleatoric_vars_np = np.array(all_cbf_aleatoric_vars_list).squeeze(); all_att_aleatoric_vars_np = np.array(all_att_aleatoric_vars_list).squeeze() 
            cbf_weights = 1.0 / (all_cbf_aleatoric_vars_np + 1e-9); ensemble_cbf_mean_norm = np.average(all_cbf_means_norm_np, weights=cbf_weights)
            att_weights = 1.0 / (all_att_aleatoric_vars_np + 1e-9); ensemble_att_mean_norm = np.average(all_att_means_norm_np, weights=att_weights)
            mean_aleatoric_cbf_var_norm = np.mean(all_cbf_aleatoric_vars_np); mean_aleatoric_att_var_norm = np.mean(all_att_aleatoric_vars_np)
            epistemic_cbf_var_norm = np.var(all_cbf_means_norm_np) if self.n_ensembles > 1 else 0.0
            epistemic_att_var_norm = np.var(all_att_means_norm_np) if self.n_ensembles > 1 else 0.0
        else:
            all_cbf_means_norm_np = np.concatenate(all_cbf_means_norm_list, axis=1); all_att_means_norm_np = np.concatenate(all_att_means_norm_list, axis=1) 
            all_cbf_aleatoric_vars_np = np.concatenate(all_cbf_aleatoric_vars_list, axis=1); all_att_aleatoric_vars_np = np.concatenate(all_att_aleatoric_vars_list, axis=1) 
            cbf_weights = 1.0 / (all_cbf_aleatoric_vars_np + 1e-9); ensemble_cbf_mean_norm = np.average(all_cbf_means_norm_np, axis=1, weights=cbf_weights)
            att_weights = 1.0 / (all_att_aleatoric_vars_np + 1e-9); ensemble_att_mean_norm = np.average(all_att_means_norm_np, axis=1, weights=att_weights)
            mean_aleatoric_cbf_var_norm = np.mean(all_cbf_aleatoric_vars_np, axis=1); mean_aleatoric_att_var_norm = np.mean(all_att_aleatoric_vars_np, axis=1)
            epistemic_cbf_var_norm = np.var(all_cbf_means_norm_np, axis=1) if self.n_ensembles > 1 else np.zeros_like(ensemble_cbf_mean_norm)
            epistemic_att_var_norm = np.var(all_att_means_norm_np, axis=1) if self.n_ensembles > 1 else np.zeros_like(ensemble_att_mean_norm)
        y_mean_cbf, y_std_cbf, y_mean_att, y_std_att = 0.0, 1.0, 0.0, 1.0
        if self.norm_stats:
            y_mean_cbf, y_std_cbf = self.norm_stats.get('y_mean_cbf', 0.0), self.norm_stats.get('y_std_cbf', 1.0)
            y_mean_att, y_std_att = self.norm_stats.get('y_mean_att', 0.0), self.norm_stats.get('y_std_att', 1.0)
        ensemble_cbf_mean_denorm = ensemble_cbf_mean_norm * y_std_cbf + y_mean_cbf; ensemble_att_mean_denorm = ensemble_att_mean_norm * y_std_att + y_mean_att
        total_cbf_var_norm = mean_aleatoric_cbf_var_norm + epistemic_cbf_var_norm; total_att_var_norm = mean_aleatoric_att_var_norm + epistemic_att_var_norm
        total_cbf_var_denorm = total_cbf_var_norm * (y_std_cbf**2); total_att_var_denorm = total_att_var_norm * (y_std_att**2)
        total_cbf_std_denorm = np.sqrt(np.maximum(total_cbf_var_denorm, 0)); total_att_std_denorm = np.sqrt(np.maximum(total_att_var_denorm, 0))
        return ensemble_cbf_mean_denorm, ensemble_att_mean_denorm, total_cbf_std_denorm, total_att_std_denorm

class ASLInMemoryDataset(torch.utils.data.Dataset):
    """
    A high-performance PyTorch Dataset that pre-loads and pre-processes the
    entire offline dataset into RAM.
    """
    def __init__(self, data_dir: Optional[str], norm_stats: dict, disentangled_mode: bool = False):
        self.data_dir = Path(data_dir) if data_dir else None
        self.norm_stats = norm_stats
        self.num_plds = len(norm_stats.get('pcasl_mean', []))
        self.disentangled_mode = disentangled_mode

        if self.data_dir:
            logger.info(f"Loading all dataset chunks from {self.data_dir} into RAM...")
            file_paths = sorted(list(self.data_dir.glob('dataset_chunk_*.npz')))
            if not file_paths:
                raise FileNotFoundError(f"No dataset chunks found in {data_dir}. Run generate_offline_dataset.py first.")
            
            all_signals_with_features, all_params = [], []
            for f in file_paths: # Removed tqdm wrapper
                with np.load(f) as data:
                    all_signals_with_features.append(data['signals'])
                    all_params.append(data['params'])
            
            self.signals_unnormalized = np.concatenate(all_signals_with_features, axis=0)
            self.params_unnormalized = np.concatenate(all_params, axis=0)
            logger.info(f"Loaded {len(self.signals_unnormalized)} samples into memory.")

            logger.info("Performing one-shot vectorized processing on the entire dataset...")
            self.signals_processed = self._process_signals(self.signals_unnormalized)
            self.params_normalized = self._normalize_params(self.params_unnormalized)
            logger.info("Processing complete. Dataset is ready.")

            self.signals_tensor = torch.from_numpy(self.signals_processed.astype(np.float32))
            self.params_tensor = torch.from_numpy(self.params_normalized.astype(np.float32))
        else:
            logger.info("ASLInMemoryDataset initialized in manual mode. Populate tensors directly.")
            self.signals_unnormalized = np.array([]); self.params_unnormalized = np.array([])
            self.signals_processed = np.array([]); self.params_normalized = np.array([])
            self.signals_tensor = torch.empty(0); self.params_tensor = torch.empty(0)

    def _process_signals(self, signals_unnorm: np.ndarray) -> np.ndarray:
        raw_signal_part = signals_unnorm[:, :self.num_plds*2]
        eng_features_part = signals_unnorm[:, self.num_plds*2:]

        if self.disentangled_mode:
            amplitude = np.linalg.norm(raw_signal_part, axis=1, keepdims=True) + 1e-6
            shape_vector = raw_signal_part / amplitude
            amp_mean, amp_std = self.norm_stats['amplitude_mean'], self.norm_stats['amplitude_std'] + 1e-6
            amplitude_norm = (amplitude - amp_mean) / amp_std
            return np.concatenate([shape_vector, eng_features_part, amplitude_norm], axis=1)
        else:
            pcasl_raw, vsasl_raw = raw_signal_part[:, :self.num_plds], raw_signal_part[:, self.num_plds:]
            pcasl_mean, pcasl_std = np.array(self.norm_stats['pcasl_mean']), np.array(self.norm_stats['pcasl_std']) + 1e-6
            vsasl_mean, vsasl_std = np.array(self.norm_stats['vsasl_mean']), np.array(self.norm_stats['vsasl_std']) + 1e-6
            pcasl_norm = (pcasl_raw - pcasl_mean) / pcasl_std
            vsasl_norm = (vsasl_raw - vsasl_mean) / vsasl_std
            return np.concatenate([pcasl_norm, vsasl_norm, eng_features_part], axis=1)

    def _normalize_params(self, params_unnorm: np.ndarray) -> np.ndarray:
        cbf_unnorm, att_unnorm = params_unnorm[:, 0], params_unnorm[:, 1]
        cbf_norm = (cbf_unnorm - self.norm_stats['y_mean_cbf']) / self.norm_stats['y_std_cbf']
        att_norm = (att_unnorm - self.norm_stats['y_mean_att']) / self.norm_stats['y_std_att']
        return np.stack([cbf_norm, att_norm], axis=1)

    def __len__(self):
        return self.signals_tensor.shape[0]

    def __getitem__(self, idx):
        return self.signals_tensor[idx], self.params_tensor[idx]