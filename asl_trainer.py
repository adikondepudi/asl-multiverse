# asl_trainer.py

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, IterableDataset
import numpy as np
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass
from collections import defaultdict
from torch.optim.lr_scheduler import OneCycleLR
import math
import multiprocessing as mp
from pathlib import Path
import wandb 
from sklearn.metrics import mean_absolute_error, mean_squared_error 
from torch.cuda.amp import GradScaler, autocast
from tqdm import trange # Import tqdm for progress bars
import time

num_workers = mp.cpu_count()

from enhanced_asl_network import EnhancedASLNet, CustomLoss
from enhanced_simulation import RealisticASLSimulator, _generate_single_balanced_subject

import logging
logger = logging.getLogger(__name__)
if not logger.hasHandlers():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

class ASLNet(nn.Module):
    """Neural network for ASL parameter estimation"""

    def __init__(self, input_size: int, hidden_sizes: List[int] = [64, 32, 16]):
        super().__init__()

        # Build network layers
        layers = []
        prev_size = input_size

        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_size),
                nn.Dropout(0.1)
            ])
            prev_size = hidden_size

        # Output layer for CBF and ATT
        layers.append(nn.Linear(prev_size, 2))

        self.network = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x)

class ASLDataset(Dataset):
    """Dataset for ASL signals and parameters"""

    def __init__(self, signals: np.ndarray, params: np.ndarray):
        self.signals = torch.FloatTensor(signals)
        self.params = torch.FloatTensor(params)

    def __len__(self) -> int:
        return len(self.signals)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        return self.signals[idx], self.params[idx]

class ASLTrainer:
    """Training manager for ASL parameter estimation network"""

    def __init__(self,
                 input_size: int,
                 hidden_sizes: List[int] = [64, 32, 16],
                 learning_rate: float = 1e-3,
                 batch_size: int = 32,
                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):

        self.device = device
        self.batch_size = batch_size

        # Initialize network
        self.model = ASLNet(input_size, hidden_sizes).to(device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.criterion = nn.MSELoss()

        # Track training progress
        self.train_losses = []
        self.val_losses = []

    def prepare_data(self,
                    simulator, # ASLSimulator instance
                    n_samples: int = 10000,
                    val_split: float = 0.2,
                    plds_definition: Optional[np.ndarray] = None) -> Tuple[DataLoader, DataLoader]:
        """Prepare training and validation data loaders"""
        if plds_definition is None:
            plds = np.arange(500, 3001, 500)
        else:
            plds = plds_definition

        att_values = np.arange(0, 4001, 100)
        signals_dict = simulator.generate_synthetic_data(
            plds, att_values, n_noise=n_samples, tsnr=5.0,
            cbf_val=simulator.params.CBF
        )

        num_total_instances = n_samples * len(att_values)
        X = np.zeros((num_total_instances, len(plds) * 2))

        pcasl_all = signals_dict['PCASL'].reshape(num_total_instances, len(plds))
        vsasl_all = signals_dict['VSASL'].reshape(num_total_instances, len(plds))

        X[:, :len(plds)] = pcasl_all
        X[:, len(plds):] = vsasl_all

        cbf_true_val = simulator.params.CBF
        cbf_params = np.full(num_total_instances, cbf_true_val)
        att_params = np.tile(np.repeat(att_values, 1), n_samples)
        y = np.column_stack((cbf_params, att_params))

        if num_total_instances == 0:
            raise ValueError("No data generated by simulator.prepare_data.")
        n_val = int(num_total_instances * val_split)
        if n_val == 0 and num_total_instances > 1: n_val = 1
        if n_val >= num_total_instances : n_val = num_total_instances -1 if num_total_instances > 0 else 0

        X_train, X_val = X[:-n_val], X[-n_val:]
        y_train, y_val = y[:-n_val], y[-n_val:]

        if X_train.shape[0] == 0:
            raise ValueError("Training set is empty after split.")

        train_dataset = ASLDataset(X_train, y_train)
        val_dataset = ASLDataset(X_val, y_val)

        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=self.batch_size if X_val.shape[0] > 0 else 1)
        return train_loader, val_loader

    def train_epoch(self, train_loader: DataLoader) -> float:
        self.model.train()
        total_loss = 0.0
        for signals, params in train_loader:
            signals, params = signals.to(self.device), params.to(self.device)
            self.optimizer.zero_grad()
            outputs = self.model(signals)
            loss = self.criterion(outputs, params)
            loss.backward()
            self.optimizer.step()
            total_loss += loss.item()
        return total_loss / len(train_loader) if len(train_loader) > 0 else 0.0

    def validate(self, val_loader: DataLoader) -> float:
        if val_loader is None or len(val_loader) == 0:
            logger.warning("Validation loader is empty or None, skipping validation.")
            return float('inf')
        self.model.eval()
        total_loss = 0.0
        with torch.no_grad():
            for signals, params in val_loader:
                signals, params = signals.to(self.device), params.to(self.device)
                outputs = self.model(signals)
                loss = self.criterion(outputs, params)
                total_loss += loss.item()
        return total_loss / len(val_loader) if len(val_loader) > 0 else float('inf')

    def train(self,
              train_loader: DataLoader,
              val_loader: DataLoader,
              n_epochs: int = 100,
              early_stopping_patience: int = 10) -> Dict[str, List[float]]:
        best_val_loss = float('inf')
        patience_counter = 0
        for epoch in range(n_epochs):
            train_loss = self.train_epoch(train_loader)
            val_loss = self.validate(val_loader)
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                torch.save(self.model.state_dict(), 'best_model_ASLTrainer.pt')
            else:
                patience_counter += 1
            if patience_counter >= early_stopping_patience:
                logger.info(f'Early stopping triggered at epoch {epoch + 1}')
                break
            if (epoch + 1) % 10 == 0:
                logger.info(f'Epoch {epoch + 1}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}')
        model_path = Path('best_model_ASLTrainer.pt')
        if model_path.exists():
            self.model.load_state_dict(torch.load(str(model_path)))
            logger.info(f"Loaded best model from {model_path}")
        else:
            logger.warning(f"Best model file {model_path} not found. Current model state will be used.")
        return {'train_losses': self.train_losses, 'val_losses': self.val_losses}

    def predict(self, signals: np.ndarray) -> np.ndarray:
        self.model.eval()
        with torch.no_grad():
            signals_tensor = torch.FloatTensor(signals).to(self.device)
            if signals_tensor.ndim == 1:
                signals_tensor = signals_tensor.unsqueeze(0)
            predictions = self.model(signals_tensor)
            return predictions.cpu().numpy()

    def evaluate_performance(self,
                           test_signals: np.ndarray,
                           true_params: np.ndarray) -> Dict[str, float]:
        predictions = self.predict(test_signals)
        mae_cbf = np.mean(np.abs(predictions[:,0] - true_params[:,0]))
        mae_att = np.mean(np.abs(predictions[:,1] - true_params[:,1]))
        rmse_cbf = np.sqrt(np.mean((predictions[:,0] - true_params[:,0])**2))
        rmse_att = np.sqrt(np.mean((predictions[:,1] - true_params[:,1])**2))
        rel_error_cbf = np.mean(np.abs(predictions[:,0] - true_params[:,0]) / np.clip(true_params[:,0], 1e-6, None))
        rel_error_att = np.mean(np.abs(predictions[:,1] - true_params[:,1]) / np.clip(true_params[:,1], 1e-6, None))
        return {'MAE_CBF': mae_cbf, 'MAE_ATT': mae_att, 'RMSE_CBF': rmse_cbf, 'RMSE_ATT': rmse_att,
                'RelError_CBF': rel_error_cbf, 'RelError_ATT': rel_error_att}


class EnhancedASLDataset(Dataset):
    def __init__(self,
                 signals: np.ndarray, # Assumed to be already normalized if per-modality norm is done upstream
                 params: np.ndarray, # Assumed to be already normalized if target normalization is done upstream
                 # Augmentation parameters
                 noise_config: Dict = {'type': 'additive_gaussian', 'std_fraction': 0.05, 'abs_std': 0.0001}, # std_fraction of signal max, or abs_std
                 dropout_range: Tuple[float, float] = (0.05, 0.15), # Dropout of PLD points
                 global_scale_range: Tuple[float, float] = (0.95, 1.05), # Simulates efficiency variations
                 baseline_shift_std_factor: float = 0.01, # Fraction of signal mean_abs for baseline shift
                 reference_signal_max_for_noise: float = 3.0, # Approx max expected NORMALIZED ASL signal for noise scaling
                 spike_config: Optional[Dict] = None, # e.g., {'prob': 0.05, 'magnitude_factor': 3.0}
                 drift_config: Optional[Dict] = None, # e.g., {'prob': 0.3, 'magnitude_factor': 0.05}
                ):
        self.signals = torch.FloatTensor(signals)
        self.params = torch.FloatTensor(params)
        
        self.noise_config = noise_config
        self.dropout_range = dropout_range
        self.global_scale_range = global_scale_range
        self.baseline_shift_std_factor = baseline_shift_std_factor
        self.reference_signal_max_for_noise = reference_signal_max_for_noise
        self.spike_config = spike_config if spike_config is not None else {}
        self.drift_config = drift_config if drift_config is not None else {}


    def __len__(self) -> int:
        return len(self.signals)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        signal = self.signals[idx].clone()
        param = self.params[idx].clone() # Params are already normalized if done in prepare_curriculum_data

        # 1. Parameter-Aware Noise (Phase 2, Item 2.2)
        if self.noise_config and self.noise_config.get('type') == 'additive_gaussian' and np.random.rand() < 0.5:
            # Noise std can be a fraction of a reference max signal, or an absolute std
            noise_std = self.noise_config.get('std_fraction', 0.05) * self.reference_signal_max_for_noise \
                        if 'std_fraction' in self.noise_config else self.noise_config.get('abs_std', 0.0001)
            
            if noise_std > 1e-9: # Ensure noise_std is sensible
                noise = torch.randn_like(signal) * noise_std
                signal += noise

        # 2. Dropout of PLD points (original)
        if self.dropout_range and np.random.rand() < 0.5:
            dropout_prob = np.random.uniform(*self.dropout_range)
            mask = torch.rand_like(signal) > dropout_prob
            signal *= mask

        # 3. Global Scaling (simulates efficiency perturbations - Phase 2, Item 2.2)
        if self.global_scale_range and np.random.rand() < 0.5:
            scale_factor = np.random.uniform(*self.global_scale_range)
            signal *= scale_factor
            
        # 4. Baseline Shift (original)
        if self.baseline_shift_std_factor > 0 and np.random.rand() < 0.5:
            signal_mean_abs = torch.mean(torch.abs(signal))
            if signal_mean_abs > 1e-6: # Avoid issues with all-zero signals
                shift_std = self.baseline_shift_std_factor * signal_mean_abs
                shift = torch.randn(1) * shift_std 
                signal += shift.item() # Add scalar shift to all elements
        
        # 5. Randomly scale the PCASL and VSASL components of the signal independently
        if np.random.rand() < 0.5:
            # Number of PLDs is half the raw signal length
            num_plds = signal.shape[0] // 2
            # Apply independent random scaling to PCASL and VSASL parts
            pcasl_scale = np.random.uniform(0.8, 1.2)
            vsasl_scale = np.random.uniform(0.8, 1.2)
            signal[:num_plds] *= pcasl_scale
            signal[num_plds:] *= vsasl_scale

        # 6. Spike Artifact (Tier 3)
        if self.spike_config and np.random.rand() < self.spike_config.get('prob', 0.0):
            noise_std = self.noise_config.get('std_fraction', 0.05) * self.reference_signal_max_for_noise
            if noise_std > 1e-9:
                spike_magnitude = self.spike_config.get('magnitude_factor', 3.0) * noise_std
                spike_idx = np.random.randint(0, len(signal))
                signal[spike_idx] += spike_magnitude * np.random.choice([-1, 1])
        
        # 7. Slow Baseline Drift (Tier 3)
        if self.drift_config and np.random.rand() < self.drift_config.get('prob', 0.0):
            signal_max = torch.max(torch.abs(signal)) if torch.any(torch.abs(signal) > 0) else 1.0
            drift_amplitude = self.drift_config.get('magnitude_factor', 0.05) * signal_max
            if drift_amplitude > 1e-9:
                num_points = len(signal)
                phase = np.random.uniform(0, 2 * np.pi)
                frequency = np.random.uniform(0.1, 0.5)
                t_axis = torch.linspace(0, 2 * np.pi * frequency, num_points)
                drift = drift_amplitude * torch.sin(t_axis + phase)
                signal += drift

        return signal, param

class ASLIterableDataset(IterableDataset):
    """
    An iterable dataset for on-the-fly data generation.
    This avoids creating massive datasets in RAM and ensures the model
    sees unique noise realizations in every epoch.
    """
    def __init__(self, simulator, plds, noise_levels, num_att_bins=14):
        super().__init__()
        self.simulator = simulator
        self.plds = plds
        self.noise_levels = noise_levels
        self.num_att_bins = num_att_bins
        self.base_params = simulator.params
        self.physio_var = simulator.physio_var
        self.att_range = self.physio_var.att_range
        self.cbf_range = self.physio_var.cbf_range
        self.t1_range = self.physio_var.t1_artery_range

    def __iter__(self):
        worker_info = torch.utils.data.get_worker_info()
        # Seed each worker differently for stochastic, non-repeated data
        if worker_info is not None:
            np.random.seed(worker_info.id + int(time.time() * 1000) % (2**32))
        else:
            np.random.seed(int(time.time() * 1000) % (2**32))

        # Continuously generate data
        while True:
            # Re-implement the core logic from `_generate_single_balanced_subject`
            # 1. Stratified ATT sampling
            att_bins = np.linspace(self.att_range[0], self.att_range[1], self.num_att_bins + 1)
            att_bin_idx = np.random.randint(0, self.num_att_bins)
            true_att = np.random.uniform(att_bins[att_bin_idx], att_bins[att_bin_idx+1])

            # 2. Independent sampling of other parameters
            true_cbf = np.random.uniform(*self.cbf_range)
            true_t1_artery = np.random.uniform(*self.t1_range)
            current_snr = np.random.choice(self.noise_levels)
            
            # 3. Apply sequence parameter perturbations for robustness
            perturbed_t_tau = self.base_params.T_tau * (1 + np.random.uniform(*self.physio_var.t_tau_perturb_range))
            perturbed_alpha_pcasl = np.clip(self.base_params.alpha_PCASL * (1 + np.random.uniform(*self.physio_var.alpha_perturb_range)), 0.1, 1.1)
            perturbed_alpha_vsasl = np.clip(self.base_params.alpha_VSASL * (1 + np.random.uniform(*self.physio_var.alpha_perturb_range)), 0.1, 1.0)

            # 4. Generate the noisy signal
            data_dict = self.simulator.generate_synthetic_data(
                self.plds,
                att_values=np.array([true_att]),
                n_noise=1,
                tsnr=current_snr,
                cbf_val=true_cbf,
                t1_artery_val=true_t1_artery,
                t_tau_val=perturbed_t_tau,
                alpha_pcasl_val=perturbed_alpha_pcasl,
                alpha_vsasl_val=perturbed_alpha_vsasl
            )
            
            pcasl_noisy = data_dict['MULTIVERSE'][0, 0, :, 0]
            vsasl_noisy = data_dict['MULTIVERSE'][0, 0, :, 1]
            raw_signal = np.concatenate([pcasl_noisy, vsasl_noisy]).astype(np.float32)
            params = np.array([true_cbf, true_att], dtype=np.float32)

            yield raw_signal, params

class EnhancedASLTrainer:
    def __init__(self,
                 model_config: Dict, # Pass necessary params for model instantiation and loss
                 model_class: callable, # e.g., EnhancedASLNet
                 input_size: int, # Actual input size for the model
                 learning_rate: float = 0.001,
                 weight_decay: float = 1e-5, # Added weight decay
                 batch_size: int = 256,
                 n_ensembles: int = 5,
                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu',
                 n_plds_for_model: Optional[int] = None, # Used for splitting PCASL/VSASL if needed
                 m0_input_feature_model: bool = False,
                ):
        self.device = device
        self.batch_size = batch_size
        self.n_ensembles = n_ensembles
        self.n_plds_for_model = n_plds_for_model # Number of PLDs per modality
        self.m0_input_feature_model = m0_input_feature_model
        self.input_size_model = input_size
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay # Store weight decay
        self.model_config = model_config # Store the full config for model and loss
        self.scalers = [GradScaler() for _ in range(self.n_ensembles)]

        self.models = [
            torch.compile(model_class(**model_config).to(device), mode="max-autotune")
            for _ in range(n_ensembles)
        ]
        self.best_states = [None] * self.n_ensembles
        self.optimizers = [torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=self.weight_decay) for model in self.models]
        self.schedulers = []
        
        loss_params = {
            'w_cbf': model_config.get('loss_weight_cbf', 1.0),
            'w_att': model_config.get('loss_weight_att', 1.0),
            'log_var_reg_lambda': model_config.get('loss_log_var_reg_lambda', 0.0),
            'focal_gamma': model_config.get('focal_gamma', 1.5),
            'pinn_weight': model_config.get('loss_pinn_weight_stage1', model_config.get('loss_pinn_weight', 0.0)),
            'model_params': model_config, 
            'pre_estimator_loss_weight': model_config.get('pre_estimator_loss_weight', 0.5) 
        }
        self.custom_loss_fn = CustomLoss(**loss_params)

        self.train_losses = defaultdict(list)
        self.val_metrics = defaultdict(list)
        self.global_step = 0
        self.norm_stats = None

    def train_ensemble(self,
                   train_loaders: List[DataLoader],
                   val_loaders: List[Optional[DataLoader]],
                   epoch_schedule: List[int],
                   steps_per_epoch_schedule: List[int],
                   early_stopping_patience: int = 20) -> Dict[str, Any]:
        
        histories = defaultdict(lambda: defaultdict(list))
        self.global_step = 0
        global_epoch_counter = 0 

        if not train_loaders:
            logger.error("train_loaders is empty. Aborting training.")
            return {'final_mean_train_loss': float('nan'), 'final_mean_val_loss': float('nan'), 'all_histories': histories}

        for stage_idx, train_loader in enumerate(train_loaders):
            if stage_idx == 0:
                self.custom_loss_fn.pinn_weight = self.model_config.get('loss_pinn_weight_stage1', 1.0)
                logger.info(f"--- STAGE {stage_idx+1}: Foundational Pre-training ---")
                logger.info(f"  Setting PINN weight to {self.custom_loss_fn.pinn_weight}")
            elif stage_idx == 1:
                self.custom_loss_fn.pinn_weight = self.model_config.get('loss_pinn_weight_stage2', 0.1)
                logger.info(f"--- STAGE {stage_idx+1}: Full-Spectrum Fine-tuning ---")
                logger.info(f"  Setting PINN weight to {self.custom_loss_fn.pinn_weight}")

            current_val_loader = val_loaders[stage_idx] if stage_idx < len(val_loaders) else None
            steps_per_epoch = steps_per_epoch_schedule[stage_idx]

            logger.info(f"  Training for {epoch_schedule[stage_idx]} epochs with {steps_per_epoch} train batches per epoch.")
            
            logger.info(f"Resetting early stopping state for Stage {stage_idx+1}.")
            best_val_losses_stage = [float('inf')] * self.n_ensembles
            patience_counters_stage = [0] * self.n_ensembles

            if not hasattr(self, 'overall_best_val_losses'):
                self.overall_best_val_losses = [float('inf')] * self.n_ensembles

            n_epochs_stage = epoch_schedule[stage_idx]
            for epoch in range(n_epochs_stage):
                epoch_train_losses_all_models, epoch_val_metrics_all_models = [], []
                
                # We must create a new iterator for the iterable dataset for each epoch
                train_loader_iter = iter(train_loader)
                
                for model_idx in range(self.n_ensembles):
                    train_loss_epoch = self._train_epoch(self.models[model_idx], train_loader_iter, 
                                                       self.optimizers[model_idx],
                                                       self.scalers[model_idx],
                                                       self.schedulers[model_idx] if self.schedulers else None, 
                                                       global_epoch_counter,
                                                       steps_per_epoch)
                    epoch_train_losses_all_models.append(train_loss_epoch)
                    histories[model_idx][f'train_losses_stage_{stage_idx}'].append(train_loss_epoch)

                    # Re-create iterator for the next model in the ensemble to see the same data batches
                    if model_idx < self.n_ensembles - 1:
                        train_loader_iter = iter(train_loader)

                # After all models have trained for one epoch, run validation
                if current_val_loader:
                    for model_idx in range(self.n_ensembles):
                        val_metrics_dict = self._validate(self.models[model_idx], current_val_loader, global_epoch_counter)
                        epoch_val_metrics_all_models.append(val_metrics_dict)
                        histories[model_idx][f'val_metrics_stage_{stage_idx}'].append(val_metrics_dict)
                        
                        val_loss_for_es = val_metrics_dict.get('val_loss', float('inf'))
                        
                        if val_loss_for_es < best_val_losses_stage[model_idx]:
                            best_val_losses_stage[model_idx] = val_loss_for_es
                            patience_counters_stage[model_idx] = 0
                            
                            if val_loss_for_es < self.overall_best_val_losses[model_idx]:
                                self.overall_best_val_losses[model_idx] = val_loss_for_es
                                self.best_states[model_idx] = self.models[model_idx].state_dict()
                                logger.debug(f"Model {model_idx} new overall best state saved (Val loss: {val_loss_for_es:.4f})")
                        else:
                            patience_counters_stage[model_idx] += 1
                
                if wandb.run:
                    # Logging logic remains fine
                    mean_epoch_train_loss = np.nanmean(epoch_train_losses_all_models) if epoch_train_losses_all_models else float('nan')
                    wandb.log({f'Epoch_Stage{stage_idx}/Mean_Train_Loss': mean_epoch_train_loss, 'epoch_global': global_epoch_counter, 'epoch_stage': epoch})
                    if current_val_loader and len(current_val_loader) > 0 and epoch_val_metrics_all_models:
                        epoch_val_metrics_agg = defaultdict(list)
                        for model_metrics in epoch_val_metrics_all_models:
                            for metric_name, value in model_metrics.items():
                                epoch_val_metrics_agg[metric_name].append(value)
                        
                        for metric_name, values_list in epoch_val_metrics_agg.items():
                            mean_val_metric = np.nanmean(values_list) if values_list else float('nan')
                            wandb.log({f'Epoch_Stage{stage_idx}/Mean_Val_{metric_name.capitalize()}': mean_val_metric, 'epoch_global': global_epoch_counter, 'epoch_stage': epoch})

                if sum(1 for p_count in patience_counters_stage if p_count < early_stopping_patience) == 0 and epoch > 0 : 
                    logger.info(f"All active models early stopped within stage {stage_idx+1}, epoch {epoch+1}. Moving to next stage.")
                    break 

                if (epoch + 1) % 10 == 0:
                    # Console logging logic remains fine
                    mean_train_loss_console = np.nanmean(epoch_train_losses_all_models) if epoch_train_losses_all_models else float('nan')
                    mean_val_loss_console_stage = np.nanmean([m.get('val_loss', np.nan) for m in epoch_val_metrics_all_models]) if epoch_val_metrics_all_models else float('nan')
                    logger.info(f"Stage {stage_idx+1}, Epoch {epoch + 1}/{n_epochs_stage}: Mean Active Train Loss = {mean_train_loss_console:.6f}, Mean Active Val Loss (Stage) = {mean_val_loss_console_stage:.6f}")
                
                global_epoch_counter += 1

        # Final model loading and summary logic is fine
        if not hasattr(self, 'overall_best_val_losses'): self.overall_best_val_losses = [float('inf')] * self.n_ensembles
        for model_idx, state in enumerate(self.best_states):
            if state is not None:
                self.models[model_idx].load_state_dict(state)
                logger.info(f"Loaded best overall state for model {model_idx} (Overall Val Loss: {self.overall_best_val_losses[model_idx]:.4f})")
            else: 
                logger.warning(f"No best overall state found for model {model_idx}. Using final state from last trained stage.")

        final_train_losses_list_overall, final_val_losses_list_overall = [], []
        for i in range(self.n_ensembles):
            model_train_losses = [histories[i][f'train_losses_stage_{s_idx}'][-1] for s_idx in range(len(train_loaders)) if histories[i][f'train_losses_stage_{s_idx}']]
            if model_train_losses: final_train_losses_list_overall.append(np.nanmean(model_train_losses))
            if self.overall_best_val_losses[i] != float('inf'): final_val_losses_list_overall.append(self.overall_best_val_losses[i])

        final_mean_train_loss_overall = np.nanmean(final_train_losses_list_overall) if final_train_losses_list_overall else float('nan')
        final_mean_val_loss_overall = np.nanmean(final_val_losses_list_overall) if final_val_losses_list_overall else float('nan')
        if wandb.run:
            wandb.summary['final_mean_train_loss_overall'], wandb.summary['final_mean_val_loss_overall'] = final_mean_train_loss_overall, final_mean_val_loss_overall
        return {'final_mean_train_loss': final_mean_train_loss_overall, 'final_mean_val_loss': final_mean_val_loss_overall, 'all_histories': histories}

    def _train_epoch(self, model, train_loader_iter, optimizer, scaler, scheduler, current_global_epoch: int, steps_per_epoch: int) -> float:
        model.train()
        total_loss = 0.0
        
        pbar = trange(steps_per_epoch, desc=f"Epoch {current_global_epoch+1} Training", leave=False, ncols=100)
        
        for i in pbar:
            try:
                signals, params_norm = next(train_loader_iter)
            except StopIteration:
                logger.warning("Train loader iterator exhausted prematurely.")
                break

            signals, params_norm = signals.to(self.device), params_norm.to(self.device)
            # --- MODIFIED: Use set_to_none=True for performance ---
            optimizer.zero_grad(set_to_none=True)

            with autocast(dtype=torch.bfloat16):
                outputs = model(signals)
                cbf_mean_norm, att_mean_norm, cbf_log_var, att_log_var, cbf_rough, att_rough = outputs
                loss = self.custom_loss_fn(signals, cbf_mean_norm, att_mean_norm, params_norm[:, 0:1], params_norm[:, 1:2], 
                                           cbf_log_var, att_log_var, cbf_rough, att_rough, current_global_epoch)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            if scheduler:
                scheduler.step()
                if wandb.run: wandb.log({'lr': scheduler.get_last_lr()[0]}, step=self.global_step)
                
            total_loss += loss.item()
            pbar.set_postfix(loss=loss.item())
            self.global_step += 1

        return total_loss / steps_per_epoch if steps_per_epoch > 0 else 0.0

    def _validate(self, model, val_loader, current_global_epoch: int) -> Dict[str, float]:
        model.eval(); total_loss_val = 0.0
        all_cbf_preds_norm, all_att_preds_norm = [], []
        all_cbf_trues_norm, all_att_trues_norm = [], []
        all_cbf_log_vars, all_att_log_vars = [], []

        with torch.no_grad():
            for signals, params_norm in val_loader:
                signals, params_norm = signals.to(self.device), params_norm.to(self.device)
                with autocast(dtype=torch.bfloat16):
                    outputs = model(signals)
                    cbf_mean_norm, att_mean_norm, cbf_log_var, att_log_var, cbf_rough, att_rough = outputs
                    loss = self.custom_loss_fn(signals, cbf_mean_norm, att_mean_norm, params_norm[:, 0:1], params_norm[:, 1:2], 
                                               cbf_log_var, att_log_var, cbf_rough, att_rough, current_global_epoch)
                
                total_loss_val += loss.item()
                all_cbf_preds_norm.append(cbf_mean_norm.cpu()); all_att_preds_norm.append(att_mean_norm.cpu())
                all_cbf_trues_norm.append(params_norm[:, 0:1].cpu()); all_att_trues_norm.append(params_norm[:, 1:2].cpu())
                all_cbf_log_vars.append(cbf_log_var.cpu()); all_att_log_vars.append(att_log_var.cpu())
        
        avg_loss_val = total_loss_val / len(val_loader) if len(val_loader) > 0 else float('inf')
        metrics_dict = {'val_loss': avg_loss_val}

        if all_cbf_preds_norm and self.norm_stats:
            y_mean_cbf = self.norm_stats.get('y_mean_cbf', 0.0)
            y_std_cbf = self.norm_stats.get('y_std_cbf', 1.0)
            y_mean_att = self.norm_stats.get('y_mean_att', 0.0)
            y_std_att = self.norm_stats.get('y_std_att', 1.0)

            cbf_preds_norm_cat = torch.cat(all_cbf_preds_norm).numpy().squeeze()
            att_preds_norm_cat = torch.cat(all_att_preds_norm).numpy().squeeze()
            cbf_trues_norm_cat = torch.cat(all_cbf_trues_norm).numpy().squeeze()
            att_trues_norm_cat = torch.cat(all_att_trues_norm).numpy().squeeze()

            cbf_preds_denorm = cbf_preds_norm_cat * y_std_cbf + y_mean_cbf
            att_preds_denorm = att_preds_norm_cat * y_std_att + y_mean_att
            cbf_trues_denorm = cbf_trues_norm_cat * y_std_cbf + y_mean_cbf
            att_trues_denorm = att_trues_norm_cat * y_std_att + y_mean_att
            
            if len(cbf_preds_denorm) > 0 : 
                metrics_dict['cbf_mae'] = mean_absolute_error(cbf_trues_denorm, cbf_preds_denorm)
                metrics_dict['cbf_rmse'] = np.sqrt(mean_squared_error(cbf_trues_denorm, cbf_preds_denorm))
                metrics_dict['att_mae'] = mean_absolute_error(att_trues_denorm, att_preds_denorm)
                metrics_dict['att_rmse'] = np.sqrt(mean_squared_error(att_trues_denorm, att_preds_denorm))

                cbf_log_vars_cat = torch.cat(all_cbf_log_vars).numpy().squeeze()
                att_log_vars_cat = torch.cat(all_att_log_vars).numpy().squeeze()
                metrics_dict['mean_cbf_log_var'] = np.mean(cbf_log_vars_cat)
                metrics_dict['mean_att_log_var'] = np.mean(att_log_vars_cat)
        return metrics_dict

    def predict(self, signals: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        signals_tensor = torch.FloatTensor(signals).to(self.device)
        if signals_tensor.ndim == 1: signals_tensor = signals_tensor.unsqueeze(0)
        
        all_cbf_means_norm_list, all_att_means_norm_list = [], []
        all_cbf_aleatoric_vars_list, all_att_aleatoric_vars_list = [], []

        for model in self.models:
            model.eval()
            with torch.no_grad():
                cbf_mean_norm, att_mean_norm, cbf_log_var, att_log_var, _, _ = model(signals_tensor)
                all_cbf_means_norm_list.append(cbf_mean_norm.cpu().numpy())
                all_att_means_norm_list.append(att_mean_norm.cpu().numpy())
                all_cbf_aleatoric_vars_list.append(torch.exp(cbf_log_var).cpu().numpy())
                all_att_aleatoric_vars_list.append(torch.exp(att_log_var).cpu().numpy())
        
        if signals_tensor.shape[0] == 1:
            all_cbf_means_norm_np = np.array(all_cbf_means_norm_list).squeeze() 
            all_att_means_norm_np = np.array(all_att_means_norm_list).squeeze() 
            all_cbf_aleatoric_vars_np = np.array(all_cbf_aleatoric_vars_list).squeeze() 
            all_att_aleatoric_vars_np = np.array(all_att_aleatoric_vars_list).squeeze() 
            
            cbf_weights = 1.0 / (all_cbf_aleatoric_vars_np + 1e-9)
            ensemble_cbf_mean_norm = np.average(all_cbf_means_norm_np, weights=cbf_weights)
            
            att_weights = 1.0 / (all_att_aleatoric_vars_np + 1e-9)
            ensemble_att_mean_norm = np.average(all_att_means_norm_np, weights=att_weights)
            
            mean_aleatoric_cbf_var_norm = np.mean(all_cbf_aleatoric_vars_np)
            mean_aleatoric_att_var_norm = np.mean(all_att_aleatoric_vars_np)
            epistemic_cbf_var_norm = np.var(all_cbf_means_norm_np) if self.n_ensembles > 1 else 0.0
            epistemic_att_var_norm = np.var(all_att_means_norm_np) if self.n_ensembles > 1 else 0.0

        else:
            all_cbf_means_norm_np = np.concatenate(all_cbf_means_norm_list, axis=1) 
            all_att_means_norm_np = np.concatenate(all_att_means_norm_list, axis=1) 
            all_cbf_aleatoric_vars_np = np.concatenate(all_cbf_aleatoric_vars_list, axis=1) 
            all_att_aleatoric_vars_np = np.concatenate(all_att_aleatoric_vars_list, axis=1) 

            cbf_weights = 1.0 / (all_cbf_aleatoric_vars_np + 1e-9)
            ensemble_cbf_mean_norm = np.average(all_cbf_means_norm_np, axis=1, weights=cbf_weights)

            att_weights = 1.0 / (all_att_aleatoric_vars_np + 1e-9)
            ensemble_att_mean_norm = np.average(all_att_means_norm_np, axis=1, weights=att_weights)

            mean_aleatoric_cbf_var_norm = np.mean(all_cbf_aleatoric_vars_np, axis=1)
            mean_aleatoric_att_var_norm = np.mean(all_att_aleatoric_vars_np, axis=1)
            epistemic_cbf_var_norm = np.var(all_cbf_means_norm_np, axis=1) if self.n_ensembles > 1 else np.zeros_like(ensemble_cbf_mean_norm)
            epistemic_att_var_norm = np.var(all_att_means_norm_np, axis=1) if self.n_ensembles > 1 else np.zeros_like(ensemble_att_mean_norm)

        y_mean_cbf, y_std_cbf, y_mean_att, y_std_att = 0.0, 1.0, 0.0, 1.0
        if self.norm_stats:
            y_mean_cbf = self.norm_stats.get('y_mean_cbf', 0.0)
            y_std_cbf = self.norm_stats.get('y_std_cbf', 1.0)
            y_mean_att = self.norm_stats.get('y_mean_att', 0.0)
            y_std_att = self.norm_stats.get('y_std_att', 1.0)

        ensemble_cbf_mean_denorm = ensemble_cbf_mean_norm * y_std_cbf + y_mean_cbf
        ensemble_att_mean_denorm = ensemble_att_mean_norm * y_std_att + y_mean_att
        
        total_cbf_var_norm = mean_aleatoric_cbf_var_norm + epistemic_cbf_var_norm
        total_att_var_norm = mean_aleatoric_att_var_norm + epistemic_att_var_norm
        
        total_cbf_var_denorm = total_cbf_var_norm * (y_std_cbf**2)
        total_att_var_denorm = total_att_var_norm * (y_std_att**2)
        
        total_cbf_std_denorm = np.sqrt(np.maximum(total_cbf_var_denorm, 0)) 
        total_att_std_denorm = np.sqrt(np.maximum(total_att_var_denorm, 0))
        
        return ensemble_cbf_mean_denorm, ensemble_att_mean_denorm, total_cbf_std_denorm, total_att_std_denorm