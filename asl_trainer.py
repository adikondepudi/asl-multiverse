import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import numpy as np
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass
from collections import defaultdict
from torch.optim.lr_scheduler import OneCycleLR
import math
import multiprocessing as mp
from pathlib import Path
import wandb # Added for Weights & Biases
from sklearn.metrics import mean_absolute_error, mean_squared_error # For validation metrics


num_workers = mp.cpu_count()

# Assuming EnhancedASLNet and CustomLoss are correctly imported from enhanced_asl_network
from enhanced_asl_network import EnhancedASLNet, CustomLoss
# Assuming RealisticASLSimulator is imported if type hints are to be strictly checked
# from enhanced_simulation import RealisticASLSimulator # For type hinting

# For logging within this file if not passed from main.py
import logging
logger = logging.getLogger(__name__)
# Basic config for logger if it's not configured by main.py when this module is imported
if not logger.hasHandlers():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')


class ASLNet(nn.Module):
    """Neural network for ASL parameter estimation"""

    def __init__(self, input_size: int, hidden_sizes: List[int] = [64, 32, 16]):
        super().__init__()

        # Build network layers
        layers = []
        prev_size = input_size

        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_size),
                nn.Dropout(0.1)
            ])
            prev_size = hidden_size

        # Output layer for CBF and ATT
        layers.append(nn.Linear(prev_size, 2))

        self.network = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x)

class ASLDataset(Dataset):
    """Dataset for ASL signals and parameters"""

    def __init__(self, signals: np.ndarray, params: np.ndarray):
        self.signals = torch.FloatTensor(signals)
        self.params = torch.FloatTensor(params)

    def __len__(self) -> int:
        return len(self.signals)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        return self.signals[idx], self.params[idx]

class ASLTrainer:
    """Training manager for ASL parameter estimation network"""

    def __init__(self,
                 input_size: int,
                 hidden_sizes: List[int] = [64, 32, 16],
                 learning_rate: float = 1e-3,
                 batch_size: int = 32,
                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):

        self.device = device
        self.batch_size = batch_size

        # Initialize network
        self.model = ASLNet(input_size, hidden_sizes).to(device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.criterion = nn.MSELoss()

        # Track training progress
        self.train_losses = []
        self.val_losses = []

    def prepare_data(self,
                    simulator, # ASLSimulator instance
                    n_samples: int = 10000,
                    val_split: float = 0.2,
                    plds_definition: Optional[np.ndarray] = None) -> Tuple[DataLoader, DataLoader]:
        """Prepare training and validation data loaders"""
        if plds_definition is None:
            plds = np.arange(500, 3001, 500)
        else:
            plds = plds_definition

        att_values = np.arange(0, 4001, 100)
        signals_dict = simulator.generate_synthetic_data(
            plds, att_values, n_noise=n_samples, tsnr=5.0,
            cbf_val=simulator.params.CBF
        )

        num_total_instances = n_samples * len(att_values)
        X = np.zeros((num_total_instances, len(plds) * 2))

        pcasl_all = signals_dict['PCASL'].reshape(num_total_instances, len(plds))
        vsasl_all = signals_dict['VSASL'].reshape(num_total_instances, len(plds))

        X[:, :len(plds)] = pcasl_all
        X[:, len(plds):] = vsasl_all

        cbf_true_val = simulator.params.CBF
        cbf_params = np.full(num_total_instances, cbf_true_val)
        att_params = np.tile(np.repeat(att_values, 1), n_samples)
        y = np.column_stack((cbf_params, att_params))

        if num_total_instances == 0:
            raise ValueError("No data generated by simulator.prepare_data.")
        n_val = int(num_total_instances * val_split)
        if n_val == 0 and num_total_instances > 1: n_val = 1
        if n_val >= num_total_instances : n_val = num_total_instances -1 if num_total_instances > 0 else 0

        X_train, X_val = X[:-n_val], X[-n_val:]
        y_train, y_val = y[:-n_val], y[-n_val:]

        if X_train.shape[0] == 0:
            raise ValueError("Training set is empty after split.")

        train_dataset = ASLDataset(X_train, y_train)
        val_dataset = ASLDataset(X_val, y_val)

        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=self.batch_size if X_val.shape[0] > 0 else 1)
        return train_loader, val_loader

    def train_epoch(self, train_loader: DataLoader) -> float:
        self.model.train()
        total_loss = 0.0
        for signals, params in train_loader:
            signals, params = signals.to(self.device), params.to(self.device)
            self.optimizer.zero_grad()
            outputs = self.model(signals)
            loss = self.criterion(outputs, params)
            loss.backward()
            self.optimizer.step()
            total_loss += loss.item()
        return total_loss / len(train_loader) if len(train_loader) > 0 else 0.0

    def validate(self, val_loader: DataLoader) -> float:
        if val_loader is None or len(val_loader) == 0:
            logger.warning("Validation loader is empty or None, skipping validation.")
            return float('inf')
        self.model.eval()
        total_loss = 0.0
        with torch.no_grad():
            for signals, params in val_loader:
                signals, params = signals.to(self.device), params.to(self.device)
                outputs = self.model(signals)
                loss = self.criterion(outputs, params)
                total_loss += loss.item()
        return total_loss / len(val_loader) if len(val_loader) > 0 else float('inf')

    def train(self,
              train_loader: DataLoader,
              val_loader: DataLoader,
              n_epochs: int = 100,
              early_stopping_patience: int = 10) -> Dict[str, List[float]]:
        best_val_loss = float('inf')
        patience_counter = 0
        for epoch in range(n_epochs):
            train_loss = self.train_epoch(train_loader)
            val_loss = self.validate(val_loader)
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                torch.save(self.model.state_dict(), 'best_model_ASLTrainer.pt')
            else:
                patience_counter += 1
            if patience_counter >= early_stopping_patience:
                logger.info(f'Early stopping triggered at epoch {epoch + 1}')
                break
            if (epoch + 1) % 10 == 0:
                logger.info(f'Epoch {epoch + 1}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}')
        model_path = Path('best_model_ASLTrainer.pt')
        if model_path.exists():
            self.model.load_state_dict(torch.load(str(model_path)))
            logger.info(f"Loaded best model from {model_path}")
        else:
            logger.warning(f"Best model file {model_path} not found. Current model state will be used.")
        return {'train_losses': self.train_losses, 'val_losses': self.val_losses}

    def predict(self, signals: np.ndarray) -> np.ndarray:
        self.model.eval()
        with torch.no_grad():
            signals_tensor = torch.FloatTensor(signals).to(self.device)
            if signals_tensor.ndim == 1:
                signals_tensor = signals_tensor.unsqueeze(0)
            predictions = self.model(signals_tensor)
            return predictions.cpu().numpy()

    def evaluate_performance(self,
                           test_signals: np.ndarray,
                           true_params: np.ndarray) -> Dict[str, float]:
        predictions = self.predict(test_signals)
        mae_cbf = np.mean(np.abs(predictions[:,0] - true_params[:,0]))
        mae_att = np.mean(np.abs(predictions[:,1] - true_params[:,1]))
        rmse_cbf = np.sqrt(np.mean((predictions[:,0] - true_params[:,0])**2))
        rmse_att = np.sqrt(np.mean((predictions[:,1] - true_params[:,1])**2))
        rel_error_cbf = np.mean(np.abs(predictions[:,0] - true_params[:,0]) / np.clip(true_params[:,0], 1e-6, None))
        rel_error_att = np.mean(np.abs(predictions[:,1] - true_params[:,1]) / np.clip(true_params[:,1], 1e-6, None))
        return {'MAE_CBF': mae_cbf, 'MAE_ATT': mae_att, 'RMSE_CBF': rmse_cbf, 'RMSE_ATT': rmse_att,
                'RelError_CBF': rel_error_cbf, 'RelError_ATT': rel_error_att}


class EnhancedASLDataset(Dataset):
    def __init__(self,
                 signals: np.ndarray,
                 params: np.ndarray,
                 noise_levels: List[float] = [0.01, 0.02, 0.05],
                 dropout_range: Tuple[float, float] = (0.05, 0.15),
                 global_scale_range: Tuple[float, float] = (0.95, 1.05),
                 baseline_shift_std_factor: float = 0.01
                ):
        self.signals = torch.FloatTensor(signals)
        self.params = torch.FloatTensor(params)
        self.noise_levels = noise_levels
        self.dropout_range = dropout_range
        self.global_scale_range = global_scale_range
        self.baseline_shift_std_factor = baseline_shift_std_factor

    def __len__(self) -> int:
        return len(self.signals)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        signal = self.signals[idx].clone()
        param = self.params[idx].clone()
        if self.noise_levels and np.random.rand() < 0.5:
            noise_level_factor = np.random.choice(self.noise_levels)
            signal_std = torch.std(signal)
            if signal_std > 1e-6:
                effective_noise_std = noise_level_factor * signal_std
                noise = torch.randn_like(signal) * effective_noise_std
                signal += noise
        if self.dropout_range and np.random.rand() < 0.5:
            dropout_prob = np.random.uniform(*self.dropout_range)
            mask = torch.rand_like(signal) > dropout_prob
            signal *= mask
        if self.global_scale_range and np.random.rand() < 0.5:
            scale_factor = np.random.uniform(*self.global_scale_range)
            signal *= scale_factor
        if self.baseline_shift_std_factor > 0 and np.random.rand() < 0.5:
            signal_mean_abs = torch.mean(torch.abs(signal))
            if signal_mean_abs > 1e-6:
                shift_std = self.baseline_shift_std_factor * signal_mean_abs
                shift = torch.randn(1) * shift_std
                signal += shift.item()
        return signal, param

class EnhancedASLTrainer:
    def __init__(self,
                 model_class,
                 input_size: int,
                 hidden_sizes: List[int] = [256, 128, 64],
                 learning_rate: float = 0.001,
                 batch_size: int = 256,
                 n_ensembles: int = 5,
                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu',
                 n_plds_for_model: Optional[int] = None,
                 m0_input_feature_model: bool = False
                ):
        self.device = device
        self.batch_size = batch_size
        self.n_ensembles = n_ensembles
        self.n_plds_for_model = n_plds_for_model
        self.m0_input_feature_model = m0_input_feature_model
        self.hidden_sizes = hidden_sizes
        self.input_size_model = input_size
        self.learning_rate = learning_rate # Store for scheduler

        self.models = [model_class().to(device) for _ in range(n_ensembles)]
        self.best_states = [None] * self.n_ensembles
        self.optimizers = [torch.optim.Adam(model.parameters(), lr=learning_rate) for model in self.models]
        self.schedulers = []
        self.train_losses = defaultdict(list)
        self.val_metrics = defaultdict(list) # Changed from val_losses to store dict of metrics
        self.global_step = 0 # For W&B logging

    def prepare_curriculum_data(self,
                                simulator,
                                n_training_subjects: int = 10000,
                                val_split: float = 0.2,
                                plds: Optional[np.ndarray] = None,
                                curriculum_att_ranges_config: Optional[List[Tuple[float, float, str]]] = None,
                                training_conditions_config: Optional[List[str]] = None,
                                training_noise_levels_config: Optional[List[float]] = None,
                                n_epochs_for_scheduler: int = 200,
                                include_m0_in_data: bool = False
                                ) -> Tuple[List[DataLoader], Optional[DataLoader]]:
        if plds is None: plds = np.arange(500, 3001, 500)
        conditions = training_conditions_config if training_conditions_config is not None else ['healthy', 'stroke', 'tumor', 'elderly']
        noise_levels = training_noise_levels_config if training_noise_levels_config is not None else [3.0, 5.0, 10.0, 15.0]
        logger.info(f"Generating diverse training data with {n_training_subjects} base subjects, conditions: {conditions}, SNRs: {noise_levels}")

        raw_dataset = simulator.generate_diverse_dataset(
            plds=plds, n_subjects=n_training_subjects, conditions=conditions, noise_levels=noise_levels
        )
        X_all_asl, y_all = raw_dataset['signals'], raw_dataset['parameters']

        if include_m0_in_data:
            m0_dummy_values = np.random.normal(1.0, 0.1, size=(X_all_asl.shape[0], 1))
            X_all = np.concatenate((X_all_asl, m0_dummy_values), axis=1)
            logger.info("Included dummy M0 feature in X_all.")
        else:
            X_all = X_all_asl

        logger.info(f"Total generated diverse samples for training/validation: {X_all.shape[0]}")
        if X_all.shape[0] == 0: raise ValueError("No data generated by simulator.generate_diverse_dataset.")

        n_total_samples = X_all.shape[0]
        n_val = int(n_total_samples * val_split)
        if n_val == 0 and n_total_samples > 1: n_val = 1
        if n_val >= n_total_samples : n_val = n_total_samples - 1 if n_total_samples > 0 else 0
        indices = np.random.permutation(n_total_samples)
        train_idx, val_idx = indices[:-n_val], indices[-n_val:]
        X_train, y_train = X_all[train_idx], y_all[train_idx]
        X_val, y_val = X_all[val_idx], y_all[val_idx]

        logger.info(f"Training samples: {X_train.shape[0]}, Validation samples: {X_val.shape[0]}")
        if X_train.shape[0] == 0: raise ValueError("Training set is empty after split.")

        if curriculum_att_ranges_config is None:
            min_att_sim, max_att_sim = simulator.physio_var.att_range
            curriculum_stages_def = [(min_att_sim, 1500.0), (1500.0, 2500.0), (2500.0, max_att_sim)]
        else:
            curriculum_stages_def = [(r[0], r[1]) for r in curriculum_att_ranges_config]

        train_loaders = []
        for i, (att_min_stage, att_max_stage) in enumerate(curriculum_stages_def):
            mask = (y_train[:, 1] >= att_min_stage) & (y_train[:, 1] <= (att_max_stage if i == len(curriculum_stages_def) -1 else att_max_stage -1e-3) ) # Handle last stage inclusively
            stage_X, stage_y = X_train[mask], y_train[mask]
            if len(stage_X) == 0:
                logger.warning(f"Curriculum stage {i+1} (ATT {att_min_stage}-{att_max_stage}ms) has no samples. Skipping.")
                continue
            logger.info(f"Curriculum stage {i+1} (ATT {att_min_stage}-{att_max_stage}ms): {len(stage_X)} samples.")
            att_for_weights = np.clip(stage_y[:, 1], a_min=100.0, a_max=None)
            weights = np.exp(-att_for_weights / 2000.0)
            sampler = None
            if np.sum(weights) > 1e-9 and np.all(np.isfinite(weights)) and len(weights) > 0:
                try: sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)
                except RuntimeError as e: logger.warning(f"Failed to create WeightedRandomSampler for stage {i+1}: {e}. Using uniform sampling.")
            else: logger.warning(f"Invalid weights in curriculum stage {i+1}. Using uniform sampling.")
            dataset = EnhancedASLDataset(stage_X, stage_y)
            loader = DataLoader(dataset, batch_size=self.batch_size, sampler=sampler,
                                num_workers=max(0,num_workers), pin_memory=True, drop_last=(len(stage_X) > self.batch_size))
            train_loaders.append(loader)

        if not train_loaders: logger.error("No training data loaders created.")
        val_loader = None
        if X_val.shape[0] > 0:
            val_dataset = EnhancedASLDataset(X_val, y_val)
            val_loader = DataLoader(val_dataset, batch_size=self.batch_size, num_workers=max(0,num_workers), pin_memory=True, drop_last=False)
        else: logger.warning("Validation set is empty.")

        self.schedulers = []
        if train_loaders:
            total_steps_per_epoch = sum(len(loader) for loader in train_loaders)
            if total_steps_per_epoch > 0 :
                total_steps = total_steps_per_epoch * n_epochs_for_scheduler
                for opt in self.optimizers:
                    current_lr = opt.param_groups[0]['lr'] if opt.param_groups else self.learning_rate
                    self.schedulers.append(OneCycleLR(opt, max_lr=current_lr, total_steps=total_steps))
            else: logger.warning("Total steps per epoch is 0 for scheduler.")
        else: logger.warning("No training loaders for scheduler.")
        return train_loaders, val_loader

    def train_ensemble(self,
                   train_loaders: List[DataLoader],
                   val_loader: Optional[DataLoader],
                   n_epochs: int = 200,
                   early_stopping_patience: int = 20) -> Dict[str, any]:
        best_val_losses = [float('inf')] * self.n_ensembles
        patience_counters = [0] * self.n_ensembles
        histories = defaultdict(lambda: defaultdict(list))
        self.global_step = 0

        if not train_loaders:
            logger.error("train_loaders is empty. Aborting training.")
            return {'final_mean_train_loss': float('nan'), 'final_mean_val_loss': float('nan'), 'all_histories': histories}

        for stage_idx, train_loader in enumerate(train_loaders):
            logger.info(f"\nStarting curriculum stage {stage_idx + 1}/{len(train_loaders)} with {len(train_loader)} batches.")
            if len(train_loader) == 0:
                logger.warning(f"Skipping empty curriculum stage {stage_idx + 1}.")
                continue

            for epoch in range(n_epochs):
                self.global_step += 1 # Increment global step per epoch
                active_models_in_stage = 0
                epoch_train_losses = []
                epoch_val_metrics_agg = defaultdict(list)

                for model_idx in range(self.n_ensembles):
                    if patience_counters[model_idx] >= early_stopping_patience and self.best_states[model_idx] is not None:
                        continue
                    active_models_in_stage +=1
                    model, optimizer = self.models[model_idx], self.optimizers[model_idx]
                    scheduler = self.schedulers[model_idx] if self.schedulers and len(self.schedulers) > model_idx else None
                    
                    train_loss = self._train_epoch(model, train_loader, optimizer, scheduler, epoch, stage_idx, n_epochs)
                    histories[model_idx]['train_losses'].append(train_loss)
                    epoch_train_losses.append(train_loss)
                    
                    # W&B Logging for this model's training loss this epoch
                    wandb.log({f'Model_{model_idx}/Train_Loss_Epoch': train_loss, 
                               f'Model_{model_idx}/Stage': stage_idx,
                               'epoch': self.global_step})


                    if val_loader:
                        val_metrics_dict = self._validate(model, val_loader, epoch, stage_idx, n_epochs)
                        histories[model_idx]['val_metrics'].append(val_metrics_dict)
                        val_loss_for_es = val_metrics_dict.get('val_loss', float('inf')) # Use 'val_loss' for early stopping

                        for k, v in val_metrics_dict.items():
                            epoch_val_metrics_agg[k].append(v)
                            wandb.log({f'Model_{model_idx}/Val_{k.capitalize()}_Epoch': v, 'epoch': self.global_step})

                        if val_loss_for_es < best_val_losses[model_idx]:
                            best_val_losses[model_idx] = val_loss_for_es
                            patience_counters[model_idx] = 0
                            self.best_states[model_idx] = model.state_dict()
                        else:
                            patience_counters[model_idx] += 1
                    else:
                        histories[model_idx]['val_metrics'].append({'val_loss': float('inf')}) # Add dummy if no val

                # Aggregate and log mean metrics for the epoch across active models
                mean_epoch_train_loss = np.nanmean(epoch_train_losses) if epoch_train_losses else float('nan')
                wandb.log({'Epoch/Mean_Train_Loss': mean_epoch_train_loss, 'epoch': self.global_step})
                
                if val_loader and epoch_val_metrics_agg:
                    for metric_name, values_list in epoch_val_metrics_agg.items():
                        mean_val_metric = np.nanmean(values_list) if values_list else float('nan')
                        wandb.log({f'Epoch/Mean_Val_{metric_name.capitalize()}': mean_val_metric, 'epoch': self.global_step})


                if active_models_in_stage == 0 and epoch > 0:
                    logger.info(f"All active models early stopped at stage {stage_idx+1}, epoch {epoch+1}.")
                    break

                if (epoch + 1) % 10 == 0: # Console logging
                    current_train_losses_console = [h['train_losses'][-1] for h_idx, h in histories.items() if h['train_losses'] and (patience_counters[h_idx] < early_stopping_patience or self.best_states[h_idx] is None)]
                    current_val_losses_console = [h['val_metrics'][-1]['val_loss'] for h_idx, h in histories.items() if h['val_metrics'] and h['val_metrics'][-1]['val_loss'] != float('inf') and (patience_counters[h_idx] < early_stopping_patience or self.best_states[h_idx] is None)]
                    mean_train_loss_console = np.nanmean(current_train_losses_console) if current_train_losses_console else float('nan')
                    mean_val_loss_console = np.nanmean(current_val_losses_console) if current_val_losses_console else float('nan')
                    logger.info(f"Stage {stage_idx+1}, Epoch {epoch + 1}: Mean Active Train Loss = {mean_train_loss_console:.6f}, Mean Active Val Loss = {mean_val_loss_console:.6f}")
        
        for model_idx, state in enumerate(self.best_states):
            if state is not None:
                self.models[model_idx].load_state_dict(state)
                logger.info(f"Loaded best state for model {model_idx} (Val Loss: {best_val_losses[model_idx]:.4f})")
            else:
                logger.warning(f"No best state found for model {model_idx}. Using final state.")

        final_train_losses_list = [np.nanmean(histories[i]['train_losses']) for i in range(self.n_ensembles) if histories[i]['train_losses']]
        final_val_losses_list = [best_val_losses[i] for i in range(self.n_ensembles) if best_val_losses[i] != float('inf')]
        
        final_mean_train_loss = np.nanmean(final_train_losses_list) if final_train_losses_list else float('nan')
        final_mean_val_loss = np.nanmean(final_val_losses_list) if final_val_losses_list else float('nan')

        wandb.summary['final_mean_train_loss'] = final_mean_train_loss
        wandb.summary['final_mean_val_loss'] = final_mean_val_loss

        return {'final_mean_train_loss': final_mean_train_loss, 'final_mean_val_loss': final_mean_val_loss, 'all_histories': histories}

    def _train_epoch(self, model, train_loader, optimizer, scheduler, epoch, stage, n_epochs_per_stage) -> float:
        model.train()
        total_loss = 0.0
        global_epoch_for_loss = stage * n_epochs_per_stage + epoch
        batch_num = 0

        for signals, params in train_loader:
            signals, params = signals.to(self.device), params.to(self.device)
            optimizer.zero_grad()
            cbf_mean, att_mean, cbf_log_var, att_log_var = model(signals)
            loss = CustomLoss()(cbf_mean, att_mean, params[:, 0:1], params[:, 1:2], cbf_log_var, att_log_var, global_epoch_for_loss)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            if scheduler: scheduler.step()
            total_loss += loss.item()
            
            # W&B logging for batch loss and LR
            # wandb.log({'Train/Batch_Loss': loss.item(), 
            #            'Train/LR': scheduler.get_last_lr()[0] if scheduler else optimizer.param_groups[0]['lr'],
            #            'batch_step': self.global_step * len(train_loader) + batch_num}) # More granular step
            batch_num += 1
        return total_loss / len(train_loader) if len(train_loader) > 0 else 0.0

    def _validate(self, model, val_loader, epoch, stage, n_epochs_per_stage) -> Dict[str, float]:
        model.eval()
        total_loss_val = 0.0
        global_epoch_for_loss = stage * n_epochs_per_stage + epoch
        
        all_cbf_preds_val, all_att_preds_val = [], []
        all_cbf_trues_val, all_att_trues_val = [], []
        all_cbf_log_vars_val, all_att_log_vars_val = [], []

        with torch.no_grad():
            for signals, params in val_loader:
                signals, params = signals.to(self.device), params.to(self.device)
                cbf_mean, att_mean, cbf_log_var, att_log_var = model(signals)
                loss = CustomLoss()(cbf_mean, att_mean, params[:, 0:1], params[:, 1:2], cbf_log_var, att_log_var, global_epoch_for_loss)
                total_loss_val += loss.item()

                all_cbf_preds_val.append(cbf_mean.cpu())
                all_att_preds_val.append(att_mean.cpu())
                all_cbf_trues_val.append(params[:, 0:1].cpu())
                all_att_trues_val.append(params[:, 1:2].cpu())
                all_cbf_log_vars_val.append(cbf_log_var.cpu())
                all_att_log_vars_val.append(att_log_var.cpu())
        
        avg_loss_val = total_loss_val / len(val_loader) if len(val_loader) > 0 else float('inf')
        metrics_dict_val = {'val_loss': avg_loss_val}

        if all_cbf_preds_val:
            cbf_preds_cat_val = torch.cat(all_cbf_preds_val).numpy().squeeze()
            att_preds_cat_val = torch.cat(all_att_preds_val).numpy().squeeze()
            cbf_trues_cat_val = torch.cat(all_cbf_trues_val).numpy().squeeze()
            att_trues_cat_val = torch.cat(all_att_trues_val).numpy().squeeze()
            
            if len(cbf_preds_cat_val) > 0:
                metrics_dict_val['cbf_mae'] = mean_absolute_error(cbf_trues_cat_val, cbf_preds_cat_val)
                metrics_dict_val['cbf_rmse'] = np.sqrt(mean_squared_error(cbf_trues_cat_val, cbf_preds_cat_val))
                metrics_dict_val['att_mae'] = mean_absolute_error(att_trues_cat_val, att_preds_cat_val)
                metrics_dict_val['att_rmse'] = np.sqrt(mean_squared_error(att_trues_cat_val, att_preds_cat_val))

                cbf_log_vars_cat_val = torch.cat(all_cbf_log_vars_val).numpy().squeeze()
                att_log_vars_cat_val = torch.cat(all_att_log_vars_val).numpy().squeeze()
                metrics_dict_val['mean_cbf_log_var'] = np.mean(cbf_log_vars_cat_val)
                metrics_dict_val['mean_att_log_var'] = np.mean(att_log_vars_cat_val)
            else:
                 metrics_dict_val.update({'cbf_mae': np.nan, 'cbf_rmse': np.nan, 'att_mae': np.nan, 'att_rmse': np.nan, 'mean_cbf_log_var': np.nan, 'mean_att_log_var': np.nan})
        return metrics_dict_val

    def predict(self, signals: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        signals_tensor = torch.FloatTensor(signals).to(self.device)
        if signals_tensor.ndim == 1: signals_tensor = signals_tensor.unsqueeze(0)
        all_cbf_means, all_att_means, all_cbf_aleatoric_vars, all_att_aleatoric_vars = [], [], [], []
        for model in self.models:
            model.eval()
            with torch.no_grad():
                cbf_mean, att_mean, cbf_log_var, att_log_var = model(signals_tensor)
                all_cbf_means.append(cbf_mean.cpu().numpy())
                all_att_means.append(att_mean.cpu().numpy())
                all_cbf_aleatoric_vars.append(torch.exp(cbf_log_var).cpu().numpy())
                all_att_aleatoric_vars.append(torch.exp(att_log_var).cpu().numpy())
        all_cbf_means_np = np.concatenate(all_cbf_means, axis=1)
        all_att_means_np = np.concatenate(all_att_means, axis=1)
        all_cbf_aleatoric_vars_np = np.concatenate(all_cbf_aleatoric_vars, axis=1)
        all_att_aleatoric_vars_np = np.concatenate(all_att_aleatoric_vars, axis=1)
        ensemble_cbf_mean, ensemble_att_mean = np.mean(all_cbf_means_np, axis=1), np.mean(all_att_means_np, axis=1)
        mean_aleatoric_cbf_var, mean_aleatoric_att_var = np.mean(all_cbf_aleatoric_vars_np, axis=1), np.mean(all_att_aleatoric_vars_np, axis=1)
        epistemic_cbf_var, epistemic_att_var = np.var(all_cbf_means_np, axis=1), np.var(all_att_means_np, axis=1)
        total_cbf_var, total_att_var = mean_aleatoric_cbf_var + epistemic_cbf_var, mean_aleatoric_att_var + epistemic_att_var
        total_cbf_std, total_att_std = np.sqrt(np.maximum(total_cbf_var, 0)), np.sqrt(np.maximum(total_att_var, 0))
        return ensemble_cbf_mean, ensemble_att_mean, total_cbf_std, total_att_std
