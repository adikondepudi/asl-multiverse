# FILE: asl_trainer.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, IterableDataset
from torch.amp.autocast_mode import autocast
import numpy as np
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass
from collections import defaultdict
from torch.optim.lr_scheduler import OneCycleLR
import math
import multiprocessing as mp
from pathlib import Path
import wandb 
from sklearn.metrics import mean_absolute_error, mean_squared_error 
from torch.cuda.amp import GradScaler, autocast
from tqdm import trange, tqdm
import time
from itertools import islice
import traceback
import optuna

num_workers = mp.cpu_count()

from enhanced_asl_network import EnhancedASLNet, CustomLoss
from enhanced_simulation import RealisticASLSimulator, _generate_single_balanced_subject
from utils import engineer_signal_features

import logging
logger = logging.getLogger(__name__)
if not logger.hasHandlers():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

class ASLNet(nn.Module):
    """Neural network for ASL parameter estimation"""

    def __init__(self, input_size: int, hidden_sizes: List[int] = [64, 32, 16]):
        super().__init__()
        layers = []
        prev_size = input_size
        for hidden_size in hidden_sizes:
            layers.extend([nn.Linear(prev_size, hidden_size), nn.ReLU(), nn.BatchNorm1d(hidden_size), nn.Dropout(0.1)])
            prev_size = hidden_size
        layers.append(nn.Linear(prev_size, 2))
        self.network = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x)

class ASLDataset(Dataset):
    """Dataset for ASL signals and parameters"""
    def __init__(self, signals: np.ndarray, params: np.ndarray):
        self.signals = torch.FloatTensor(signals)
        self.params = torch.FloatTensor(params)

    def __len__(self) -> int:
        return len(self.signals)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        return self.signals[idx], self.params[idx]

class ASLTrainer:
    """Training manager for ASL parameter estimation network"""
    def __init__(self, input_size: int, hidden_sizes: List[int] = [64, 32, 16], learning_rate: float = 1e-3, batch_size: int = 32, device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):
        self.device = device
        self.batch_size = batch_size
        self.model = ASLNet(input_size, hidden_sizes).to(device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.criterion = nn.MSELoss()
        self.train_losses = []
        self.val_losses = []

    def prepare_data(self, simulator, n_samples: int = 10000, val_split: float = 0.2, plds_definition: Optional[np.ndarray] = None) -> Tuple[DataLoader, DataLoader]:
        if plds_definition is None: plds = np.arange(500, 3001, 500)
        else: plds = plds_definition
        att_values = np.arange(0, 4001, 100)
        signals_dict = simulator.generate_synthetic_data(plds, att_values, n_noise=n_samples, tsnr=5.0, cbf_val=simulator.params.CBF)
        num_total_instances = n_samples * len(att_values)
        X = np.zeros((num_total_instances, len(plds) * 2))
        pcasl_all = signals_dict['PCASL'].reshape(num_total_instances, len(plds))
        vsasl_all = signals_dict['VSASL'].reshape(num_total_instances, len(plds))
        X[:, :len(plds)] = pcasl_all
        X[:, len(plds):] = vsasl_all
        cbf_true_val = simulator.params.CBF
        cbf_params = np.full(num_total_instances, cbf_true_val)
        att_params = np.tile(np.repeat(att_values, 1), n_samples)
        y = np.column_stack((cbf_params, att_params))
        if num_total_instances == 0: raise ValueError("No data generated by simulator.prepare_data.")
        n_val = int(num_total_instances * val_split)
        if n_val == 0 and num_total_instances > 1: n_val = 1
        if n_val >= num_total_instances : n_val = num_total_instances -1 if num_total_instances > 0 else 0
        X_train, X_val = X[:-n_val], X[-n_val:]
        y_train, y_val = y[:-n_val], y[-n_val:]
        if X_train.shape[0] == 0: raise ValueError("Training set is empty after split.")
        train_dataset = ASLDataset(X_train, y_train)
        val_dataset = ASLDataset(X_val, y_val)
        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=self.batch_size if X_val.shape[0] > 0 else 1)
        return train_loader, val_loader

    def train_epoch(self, train_loader: DataLoader) -> float:
        self.model.train()
        total_loss = 0.0
        for signals, params in train_loader:
            signals, params = signals.to(self.device), params.to(self.device)
            self.optimizer.zero_grad()
            outputs = self.model(signals)
            loss = self.criterion(outputs, params)
            loss.backward()
            self.optimizer.step()
            total_loss += loss.item()
        return total_loss / len(train_loader) if len(train_loader) > 0 else 0.0

    def validate(self, val_loader: DataLoader) -> float:
        if val_loader is None or len(val_loader) == 0:
            logger.warning("Validation loader is empty or None, skipping validation.")
            return float('inf')
        self.model.eval()
        total_loss = 0.0
        with torch.no_grad():
            for signals, params in val_loader:
                signals, params = signals.to(self.device), params.to(self.device)
                outputs = self.model(signals)
                loss = self.criterion(outputs, params)
                total_loss += loss.item()
        return total_loss / len(val_loader) if len(val_loader) > 0 else float('inf')

    def train(self, train_loader: DataLoader, val_loader: DataLoader, n_epochs: int = 100, early_stopping_patience: int = 10) -> Dict[str, List[float]]:
        best_val_loss = float('inf')
        patience_counter = 0
        for epoch in range(n_epochs):
            train_loss = self.train_epoch(train_loader)
            val_loss = self.validate(val_loader)
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                torch.save(self.model.state_dict(), 'best_model_ASLTrainer.pt')
            else:
                patience_counter += 1
            if patience_counter >= early_stopping_patience:
                logger.info(f'Early stopping triggered at epoch {epoch + 1}')
                break
            if (epoch + 1) % 10 == 0:
                logger.info(f'Epoch {epoch + 1}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}')
        model_path = Path('best_model_ASLTrainer.pt')
        if model_path.exists():
            self.model.load_state_dict(torch.load(str(model_path)))
            logger.info(f"Loaded best model from {model_path}")
        else:
            logger.warning(f"Best model file {model_path} not found. Current model state will be used.")
        return {'train_losses': self.train_losses, 'val_losses': self.val_losses}

    def predict(self, signals: np.ndarray) -> np.ndarray:
        self.model.eval()
        with torch.no_grad():
            signals_tensor = torch.FloatTensor(signals).to(self.device)
            if signals_tensor.ndim == 1: signals_tensor = signals_tensor.unsqueeze(0)
            predictions = self.model(signals_tensor)
            return predictions.cpu().numpy()

    def evaluate_performance(self, test_signals: np.ndarray, true_params: np.ndarray) -> Dict[str, float]:
        predictions = self.predict(test_signals)
        mae_cbf = np.mean(np.abs(predictions[:,0] - true_params[:,0]))
        mae_att = np.mean(np.abs(predictions[:,1] - true_params[:,1]))
        rmse_cbf = np.sqrt(np.mean((predictions[:,0] - true_params[:,0])**2))
        rmse_att = np.sqrt(np.mean((predictions[:,1] - true_params[:,1])**2))
        rel_error_cbf = np.mean(np.abs(predictions[:,0] - true_params[:,0]) / np.clip(true_params[:,0], 1e-6, None))
        rel_error_att = np.mean(np.abs(predictions[:,1] - true_params[:,1]) / np.clip(true_params[:,1], 1e-6, None))
        return {'MAE_CBF': mae_cbf, 'MAE_ATT': mae_att, 'RMSE_CBF': rmse_cbf, 'RMSE_ATT': rmse_att, 'RelError_CBF': rel_error_cbf, 'RelError_ATT': rel_error_att}

class EnhancedASLDataset(Dataset):
    def __init__(self, signals: np.ndarray, params: np.ndarray, **kwargs):
        self.signals = torch.FloatTensor(signals)
        self.params = torch.FloatTensor(params)
    def __len__(self) -> int: return len(self.signals)
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]: return self.signals[idx], self.params[idx]

class ASLIterableDataset(IterableDataset):
    """
    An iterable dataset for on-the-fly data generation, normalization, and feature engineering.
    This avoids creating massive datasets in RAM and ensures the model sees unique data in every epoch.
    """
    def __init__(self, simulator: RealisticASLSimulator, plds: np.ndarray, 
                 noise_levels: List[float], norm_stats: Dict, num_att_bins: int = 14):
        super().__init__()
        self.simulator = simulator
        self.plds = plds
        self.num_plds = len(plds)
        self.noise_levels = noise_levels
        self.num_att_bins = num_att_bins
        self.base_params = simulator.params
        self.physio_var = simulator.physio_var
        self.att_range = self.physio_var.att_range
        self.cbf_range = self.physio_var.cbf_range
        self.t1_range = self.physio_var.t1_artery_range
        self.norm_stats = norm_stats

    def __iter__(self):
        worker_info = torch.utils.data.get_worker_info()
        if worker_info is not None: np.random.seed(worker_info.id + int(time.time() * 1000) % (2**32))
        else: np.random.seed(int(time.time() * 1000) % (2**32))

        consecutive_failures = 0
        max_consecutive_failures = 100 

        while True:
            try:
                # 1. Stratified ATT sampling
                att_bins = np.linspace(self.att_range[0], self.att_range[1], self.num_att_bins + 1)
                att_bin_idx = np.random.randint(0, self.num_att_bins)
                true_att = np.random.uniform(att_bins[att_bin_idx], att_bins[att_bin_idx+1])
                true_cbf = np.random.uniform(*self.cbf_range)
                true_t1_artery = np.random.uniform(*self.t1_range)
                current_snr = np.random.choice(self.noise_levels)
                
                # 2. Apply sequence parameter perturbations for robustness
                perturbed_t_tau = self.base_params.T_tau * (1 + np.random.uniform(*self.physio_var.t_tau_perturb_range))
                perturbed_alpha_pcasl = np.clip(self.base_params.alpha_PCASL * (1 + np.random.uniform(*self.physio_var.alpha_perturb_range)), 0.1, 1.1)
                perturbed_alpha_vsasl = np.clip(self.base_params.alpha_VSASL * (1 + np.random.uniform(*self.physio_var.alpha_perturb_range)), 0.1, 1.0)

                # 3. Generate the noisy signal
                data_dict = self.simulator.generate_synthetic_data(
                    self.plds, att_values=np.array([true_att]), n_noise=1, tsnr=current_snr,
                    cbf_val=true_cbf, t1_artery_val=true_t1_artery, t_tau_val=perturbed_t_tau,
                    alpha_pcasl_val=perturbed_alpha_pcasl, alpha_vsasl_val=perturbed_alpha_vsasl)
                
                pcasl_noisy = data_dict['MULTIVERSE'][0, 0, :, 0]
                vsasl_noisy = data_dict['MULTIVERSE'][0, 0, :, 1]
                raw_signal = np.concatenate([pcasl_noisy, vsasl_noisy])
                
                # 4. Engineer features from raw signal
                eng_features = engineer_signal_features(raw_signal, self.num_plds)
                
                # 5. Normalize raw signals
                pcasl_norm = (raw_signal[:self.num_plds] - self.norm_stats['pcasl_mean']) / (np.array(self.norm_stats['pcasl_std']) + 1e-6)
                vsasl_norm = (raw_signal[self.num_plds:] - self.norm_stats['vsasl_mean']) / (np.array(self.norm_stats['vsasl_std']) + 1e-6)
            
                # 6. Concatenate to form the final model input
                final_input = np.concatenate([pcasl_norm, vsasl_norm, eng_features.flatten()])

                # 7. Normalize the target parameters
                params = np.array([true_cbf, true_att])
                params_norm = np.array([
                    (params[0] - self.norm_stats['y_mean_cbf']) / self.norm_stats['y_std_cbf'],
                    (params[1] - self.norm_stats['y_mean_att']) / self.norm_stats['y_std_att']
                ])
                
                yield torch.from_numpy(final_input.astype(np.float32)), torch.from_numpy(params_norm.astype(np.float32))

                consecutive_failures = 0

            except Exception as e:
                worker_id = worker_info.id if worker_info else -1
                logger.error(f"DataLoader worker {worker_id} failed to generate a sample: {e}")
                traceback.print_exc()

                consecutive_failures += 1
                if consecutive_failures >= max_consecutive_failures:
                    raise RuntimeError(
                        f"DataLoader worker {worker_id} has failed {max_consecutive_failures} consecutive times. "
                        "This indicates a persistent bug in data generation. Aborting training."
                    ) from e
                
                continue 

class EnhancedASLTrainer:
    def __init__(self,
                 model_config: Dict, model_class: callable, input_size: int,
                 weight_decay: float = 1e-5, batch_size: int = 256, n_ensembles: int = 5,
                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu',
                 n_plds_for_model: Optional[int] = None, m0_input_feature_model: bool = False):
        self.device = torch.device(device) # Ensure device is a torch.device object
        self.batch_size = batch_size

        learning_rate = model_config.get('learning_rate', 0.001)

        self.lr_cbf = model_config.get('learning_rate_cbf', learning_rate)
        self.lr_att = model_config.get('learning_rate_att', learning_rate)
        self.lr_stage2_cbf = model_config.get('learning_rate_stage2_cbf', self.lr_cbf / 10.0)
        self.lr_stage2_att = model_config.get('learning_rate_stage2_att', self.lr_att / 10.0)

        self.learning_rate = learning_rate # Keep this for backward compatibility if needed elsewhere
        self.weight_decay = weight_decay

        self.n_ensembles = n_ensembles
        self.n_plds_for_model = n_plds_for_model
        self.m0_input_feature_model = m0_input_feature_model
        self.input_size_model = input_size

        self.lr_cbf = model_config.get('learning_rate_cbf', learning_rate)
        self.lr_att = model_config.get('learning_rate_att', learning_rate)
        self.lr_stage2_cbf = model_config.get('learning_rate_stage2_cbf', self.lr_cbf / 10.0)
        self.lr_stage2_att = model_config.get('learning_rate_stage2_att', self.lr_att / 10.0)

        self.learning_rate = learning_rate
        self.weight_decay = weight_decay
        self.model_config = model_config
        self.validation_steps_per_epoch = model_config.get('validation_steps_per_epoch', 50)
        self.scalers = [torch.cuda.amp.GradScaler() for _ in range(self.n_ensembles)]

        self.models = [model_class(**model_config).to(self.device) for _ in range(n_ensembles)]
        self.best_states = [None] * self.n_ensembles
        self.optimizers = []
        for model in self.models:
            param_groups = model.get_param_groups() # Use the new method from Change 2
            # Create a list of parameter groups for the optimizer
            optimizer_param_groups = [
                {'params': group['params'], 'lr': self.lr_cbf if group['name'] == 'cbf_stream' else self.lr_att}
                for group in param_groups
            ]
            self.optimizers.append(torch.optim.AdamW(optimizer_param_groups, weight_decay=self.weight_decay))
        self.schedulers = [] 
        
        loss_params = {
            'w_cbf': model_config.get('loss_weight_cbf', 1.0), 'w_att': model_config.get('loss_weight_att', 1.0),
            'log_var_reg_lambda': model_config.get('loss_log_var_reg_lambda', 0.0), 'focal_gamma': model_config.get('focal_gamma', 1.5),
            'pinn_weight': model_config.get('loss_pinn_weight_stage1', model_config.get('loss_pinn_weight', 0.0)),
            'model_params': model_config, 'pre_estimator_loss_weight': model_config.get('pre_estimator_loss_weight', 0.5)}
        self.custom_loss_fn = CustomLoss(**loss_params)

        self.train_losses = defaultdict(list); self.val_metrics = defaultdict(list); self.global_step = 0; self.norm_stats = None

    def train_ensemble(self,
                   train_loaders: List[DataLoader], val_loaders: List[Optional[DataLoader]],
                   epoch_schedule: List[int], steps_per_epoch_schedule: List[Optional[int]],
                   early_stopping_patience: int = 20, optuna_trial: Optional[Any] = None) -> Dict[str, Any]:
        
        histories = defaultdict(lambda: defaultdict(list)); self.global_step = 0; global_epoch_counter = 0 
        if not train_loaders:
            logger.error("train_loaders is empty. Aborting training.")
            return {'final_mean_train_loss': float('nan'), 'final_mean_val_loss': float('nan'), 'all_histories': histories}
        
        self.overall_best_val_losses = [float('inf')] * self.n_ensembles
        
        num_stages = len(epoch_schedule)

        for stage_idx, train_loader in enumerate(train_loaders):
            steps_per_epoch = steps_per_epoch_schedule[stage_idx]
            n_epochs_stage = epoch_schedule[stage_idx]
            
            if steps_per_epoch is None:
                steps_per_epoch = len(train_loader)
                logger.info(f"  Using map-style dataset. One epoch will be {steps_per_epoch} steps (full dataset).")

            total_steps_stage = steps_per_epoch * n_epochs_stage
            
            # All stages except the last one are 'foundational'. The last one is 'fine-tuning'.
            is_fine_tuning_stage = (stage_idx == num_stages - 1) and num_stages > 1

            if not is_fine_tuning_stage:
                # Apply settings for foundational stages (stage 1 config keys)
                stage_title = f"Foundational Training (Stage {stage_idx + 1}/{num_stages})"
                pinn_weight = self.model_config.get('loss_pinn_weight_stage1', 1.0)
                pre_est_weight = self.model_config.get('pre_estimator_loss_weight_stage1', 1.0)
                stage_max_lrs = [self.lr_cbf, self.lr_att]
            else:
                # Apply settings for the final, fine-tuning stage (stage 2 config keys)
                stage_title = f"Fine-tuning (Stage {stage_idx + 1}/{num_stages})"
                pinn_weight = self.model_config.get('loss_pinn_weight_stage2', 0.1)
                pre_est_weight = self.model_config.get('pre_estimator_loss_weight_stage2', 0.0)
                stage_max_lrs = [self.lr_stage2_cbf, self.lr_stage2_att]

            self.custom_loss_fn.pinn_weight = pinn_weight
            self.custom_loss_fn.pre_estimator_loss_weight = pre_est_weight

            logger.info(f"--- {stage_title}: {n_epochs_stage} epochs ---")
            logger.info(f"  PINN Weight: {pinn_weight}, Pre-Estimator Weight: {pre_est_weight}")
            logger.info(f"  Max LRs: CBF={stage_max_lrs[0]}, ATT={stage_max_lrs[1]}")

            # This single scheduler creation now works for all stages
            self.schedulers = [
                torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=stage_max_lrs, total_steps=total_steps_stage)
                for opt in self.optimizers
            ]

            current_val_loader = val_loaders[stage_idx] if stage_idx < len(val_loaders) else None
            logger.info(f"  Training for {n_epochs_stage} epochs with {steps_per_epoch} train batches per epoch.")
            best_val_losses_stage = [float('inf')] * self.n_ensembles
            patience_counters_stage = [0] * self.n_ensembles
            
            for epoch in range(n_epochs_stage):
                epoch_train_losses_all_models = defaultdict(list)
                epoch_val_metrics_all_models = []
                
                # For map-style datasets, the iterator is recreated each epoch if shuffle=True
                train_loader_iter = iter(train_loader)
                for model_idx in range(self.n_ensembles):
                    train_loss_epoch, train_loss_components = self._train_epoch(
                        self.models[model_idx], train_loader_iter, self.optimizers[model_idx], 
                        self.scalers[model_idx], self.schedulers[model_idx], 
                        global_epoch_counter, steps_per_epoch
                    )
                    epoch_train_losses_all_models['total_loss'].append(train_loss_epoch)
                    for key, val in train_loss_components.items():
                        epoch_train_losses_all_models[key].append(val)
                    
                    histories[model_idx][f'train_losses_stage_{stage_idx}'].append(train_loss_epoch)
                    # If using map-style, we need a fresh iterator for each model in the same epoch
                    if model_idx < self.n_ensembles - 1: train_loader_iter = iter(train_loader)

                if current_val_loader is not None and current_val_loader.dataset is not None:
                    for model_idx in range(self.n_ensembles):
                        val_metrics_dict = self._validate(self.models[model_idx], current_val_loader, global_epoch_counter)
                        epoch_val_metrics_all_models.append(val_metrics_dict)
                        histories[model_idx][f'val_metrics_stage_{stage_idx}'].append(val_metrics_dict)
                        val_loss_for_es = val_metrics_dict.get('val_loss', float('inf'))
                        
                        if val_loss_for_es < best_val_losses_stage[model_idx]:
                            best_val_losses_stage[model_idx] = val_loss_for_es
                            patience_counters_stage[model_idx] = 0
                            if val_loss_for_es < self.overall_best_val_losses[model_idx]:
                                self.overall_best_val_losses[model_idx] = val_loss_for_es
                                self.best_states[model_idx] = self.models[model_idx].state_dict()
                                logger.debug(f"Model {model_idx} new overall best state saved (Val loss: {val_loss_for_es:.4f})")
                        else:
                            patience_counters_stage[model_idx] += 1
                
                if wandb.run:
                    log_dict = {'epoch': global_epoch_counter}
                    
                    for key, values in epoch_train_losses_all_models.items():
                        log_dict[f'train/mean_{key}'] = np.nanmean(values)

                    for i, loss_val in enumerate(epoch_train_losses_all_models['total_loss']):
                        log_dict[f'train/loss_model_{i}'] = loss_val

                    if epoch_val_metrics_all_models:
                        aggregated_val_metrics = defaultdict(list)
                        for i, m_dict in enumerate(epoch_val_metrics_all_models):
                            for key, value in m_dict.items():
                                if value is not None and np.isfinite(value):
                                    aggregated_val_metrics[key].append(value)
                                    log_dict[f'val/{key}_model_{i}'] = value
                        
                        for key, values in aggregated_val_metrics.items():
                            log_dict[f'val/mean_{key}'] = np.nanmean(values)
                    
                    wandb.log(log_dict, step=self.global_step)

                if optuna_trial:
                    mean_val_loss_for_pruning = np.nanmean([m.get('val_loss', np.nan) for m in epoch_val_metrics_all_models]) if epoch_val_metrics_all_models else float('inf')
                    optuna_trial.report(mean_val_loss_for_pruning, global_epoch_counter)
                    if optuna_trial.should_prune():
                        raise optuna.exceptions.TrialPruned()

                if sum(1 for p_count in patience_counters_stage if p_count < early_stopping_patience) == 0 and epoch > 0 : 
                    logger.info(f"All active models early stopped within stage {stage_idx+1}, epoch {epoch+1}. Moving to next stage.")
                    break 
                if (epoch + 1) % 10 == 0:
                    mean_train_loss_console = np.nanmean(epoch_train_losses_all_models['total_loss']) if 'total_loss' in epoch_train_losses_all_models else float('nan')
                    mean_val_loss_console_stage = np.nanmean([m.get('val_loss', np.nan) for m in epoch_val_metrics_all_models]) if epoch_val_metrics_all_models else float('nan')
                    logger.info(f"Stage {stage_idx+1}, Epoch {epoch + 1}/{n_epochs_stage}: Mean Active Train Loss = {mean_train_loss_console:.6f}, Mean Active Val Loss (Stage) = {mean_val_loss_console_stage:.6f}")
                global_epoch_counter += 1

        for model_idx, state in enumerate(self.best_states):
            if state is not None: self.models[model_idx].load_state_dict(state); logger.info(f"Loaded best overall state for model {model_idx} (Overall Val Loss: {self.overall_best_val_losses[model_idx]:.4f})")
            else: logger.warning(f"No best overall state found for model {model_idx}. Using final state from last trained stage.")

        final_val_losses_list_overall = [loss for loss in self.overall_best_val_losses if loss != float('inf')]
        final_mean_val_loss_overall = np.nanmean(final_val_losses_list_overall) if final_val_losses_list_overall else float('nan')
        
        if wandb.run: wandb.summary['final_mean_val_loss_overall'] = final_mean_val_loss_overall
        return {'final_mean_val_loss': final_mean_val_loss_overall, 'all_histories': histories}

    def _train_epoch(self, model, train_loader_iter, optimizer, scaler, scheduler, current_global_epoch: int, steps_per_epoch: int) -> Tuple[float, Dict[str, float]]:
        model.train()
        total_loss = 0.0
        total_components = defaultdict(float)
        
        pbar = tqdm(islice(train_loader_iter, steps_per_epoch), total=steps_per_epoch, desc=f"Epoch {current_global_epoch+1} Training", leave=False, ncols=100)
        
        for i, (signals, params_norm) in enumerate(pbar):
            signals, params_norm = signals.to(self.device), params_norm.to(self.device)
            optimizer.zero_grad(set_to_none=True)
            with torch.amp.autocast(device_type=self.device.type, dtype=torch.bfloat16):
                outputs = model(signals)
                cbf_mean_norm, att_mean_norm, cbf_log_var, att_log_var, cbf_rough, att_rough = outputs
                loss, loss_components = self.custom_loss_fn(signals, cbf_mean_norm, att_mean_norm, params_norm[:, 0:1], params_norm[:, 1:2], cbf_log_var, att_log_var, cbf_rough, att_rough, current_global_epoch)
            
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()

            if scheduler: scheduler.step()
            if wandb.run and scheduler: wandb.log({'lr': scheduler.get_last_lr()[0]}, step=self.global_step)
            
            total_loss += loss.item()
            for key, value in loss_components.items():
                total_components[key] += value.item()
                
            pbar.set_postfix(loss=loss.item()); self.global_step += 1
        
        num_steps = i + 1
        mean_total_loss = total_loss / num_steps if num_steps > 0 else 0.0
        mean_components = {key: val / num_steps for key, val in total_components.items()} if num_steps > 0 else {}
        
        return mean_total_loss, mean_components

    def _validate(self, model, val_loader, current_global_epoch: int) -> Dict[str, float]:
        model.eval(); total_loss_val = 0.0
        all_cbf_preds_norm, all_att_preds_norm = [], []; all_cbf_trues_norm, all_att_trues_norm = [], []
        all_cbf_log_vars, all_att_log_vars = [], []

        with torch.no_grad():
            pbar_val = tqdm(islice(val_loader, self.validation_steps_per_epoch), total=self.validation_steps_per_epoch, desc=f"Epoch {current_global_epoch+1} Validation", leave=False, ncols=100)
            for signals, params_norm in pbar_val:
                signals, params_norm = signals.to(self.device), params_norm.to(self.device)
                with torch.amp.autocast(device_type=self.device.type, dtype=torch.bfloat16):
                    outputs = model(signals)
                    cbf_mean_norm, att_mean_norm, cbf_log_var, att_log_var, cbf_rough, att_rough = outputs
                    loss, _ = self.custom_loss_fn(signals, cbf_mean_norm, att_mean_norm, params_norm[:, 0:1], params_norm[:, 1:2], cbf_log_var, att_log_var, cbf_rough, att_rough, current_global_epoch)
                total_loss_val += loss.item()
                all_cbf_preds_norm.append(cbf_mean_norm.cpu()); all_att_preds_norm.append(att_mean_norm.cpu())
                all_cbf_trues_norm.append(params_norm[:, 0:1].cpu()); all_att_trues_norm.append(params_norm[:, 1:2].cpu())
                all_cbf_log_vars.append(cbf_log_var.cpu()); all_att_log_vars.append(att_log_var.cpu())
        
        avg_loss_val = total_loss_val / self.validation_steps_per_epoch if self.validation_steps_per_epoch > 0 else float('inf')
        metrics_dict = {'val_loss': avg_loss_val}

        if all_cbf_preds_norm and self.norm_stats:
            y_mean_cbf, y_std_cbf = self.norm_stats.get('y_mean_cbf', 0.0), self.norm_stats.get('y_std_cbf', 1.0)
            y_mean_att, y_std_att = self.norm_stats.get('y_mean_att', 0.0), self.norm_stats.get('y_std_att', 1.0)
            
            cbf_preds_norm_cat = torch.cat(all_cbf_preds_norm).float().numpy().squeeze(); att_preds_norm_cat = torch.cat(all_att_preds_norm).float().numpy().squeeze()
            cbf_trues_norm_cat = torch.cat(all_cbf_trues_norm).float().numpy().squeeze(); att_trues_norm_cat = torch.cat(all_att_trues_norm).float().numpy().squeeze()
            
            cbf_preds_denorm = cbf_preds_norm_cat * y_std_cbf + y_mean_cbf; att_preds_denorm = att_preds_norm_cat * y_std_att + y_mean_att
            cbf_trues_denorm = cbf_trues_norm_cat * y_std_cbf + y_mean_cbf; att_trues_denorm = att_trues_norm_cat * y_std_att + y_mean_att
            
            if len(cbf_preds_denorm) > 0 : 
                metrics_dict['cbf_mae'] = mean_absolute_error(cbf_trues_denorm, cbf_preds_denorm)
                metrics_dict['cbf_rmse'] = np.sqrt(mean_squared_error(cbf_trues_denorm, cbf_preds_denorm))
                metrics_dict['att_mae'] = mean_absolute_error(att_trues_denorm, att_preds_denorm)
                metrics_dict['att_rmse'] = np.sqrt(mean_squared_error(att_trues_denorm, att_preds_denorm))
                
                cbf_log_vars_cat = torch.cat(all_cbf_log_vars).float().numpy().squeeze(); att_log_vars_cat = torch.cat(all_att_log_vars).float().numpy().squeeze()
                
                metrics_dict['mean_cbf_log_var'] = np.mean(cbf_log_vars_cat); metrics_dict['mean_att_log_var'] = np.mean(att_log_vars_cat)
        return metrics_dict

    def predict(self, signals: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        signals_tensor = torch.FloatTensor(signals).to(self.device)
        if signals_tensor.ndim == 1: signals_tensor = signals_tensor.unsqueeze(0)
        all_cbf_means_norm_list, all_att_means_norm_list = [], []; all_cbf_aleatoric_vars_list, all_att_aleatoric_vars_list = [], []
        for model in self.models:
            model.eval()
            with torch.no_grad():
                cbf_mean_norm, att_mean_norm, cbf_log_var, att_log_var, _, _ = model(signals_tensor)
                all_cbf_means_norm_list.append(cbf_mean_norm.cpu().numpy()); all_att_means_norm_list.append(att_mean_norm.cpu().numpy())
                all_cbf_aleatoric_vars_list.append(torch.exp(cbf_log_var).cpu().numpy()); all_att_aleatoric_vars_list.append(torch.exp(att_log_var).cpu().numpy())
        if signals_tensor.shape[0] == 1:
            all_cbf_means_norm_np = np.array(all_cbf_means_norm_list).squeeze(); all_att_means_norm_np = np.array(all_att_means_norm_list).squeeze() 
            all_cbf_aleatoric_vars_np = np.array(all_cbf_aleatoric_vars_list).squeeze(); all_att_aleatoric_vars_np = np.array(all_att_aleatoric_vars_list).squeeze() 
            cbf_weights = 1.0 / (all_cbf_aleatoric_vars_np + 1e-9); ensemble_cbf_mean_norm = np.average(all_cbf_means_norm_np, weights=cbf_weights)
            att_weights = 1.0 / (all_att_aleatoric_vars_np + 1e-9); ensemble_att_mean_norm = np.average(all_att_means_norm_np, weights=att_weights)
            mean_aleatoric_cbf_var_norm = np.mean(all_cbf_aleatoric_vars_np); mean_aleatoric_att_var_norm = np.mean(all_att_aleatoric_vars_np)
            epistemic_cbf_var_norm = np.var(all_cbf_means_norm_np) if self.n_ensembles > 1 else 0.0
            epistemic_att_var_norm = np.var(all_att_means_norm_np) if self.n_ensembles > 1 else 0.0
        else:
            all_cbf_means_norm_np = np.concatenate(all_cbf_means_norm_list, axis=1); all_att_means_norm_np = np.concatenate(all_att_means_norm_list, axis=1) 
            all_cbf_aleatoric_vars_np = np.concatenate(all_cbf_aleatoric_vars_list, axis=1); all_att_aleatoric_vars_np = np.concatenate(all_att_aleatoric_vars_list, axis=1) 
            cbf_weights = 1.0 / (all_cbf_aleatoric_vars_np + 1e-9); ensemble_cbf_mean_norm = np.average(all_cbf_means_norm_np, axis=1, weights=cbf_weights)
            att_weights = 1.0 / (all_att_aleatoric_vars_np + 1e-9); ensemble_att_mean_norm = np.average(all_att_means_norm_np, axis=1, weights=att_weights)
            mean_aleatoric_cbf_var_norm = np.mean(all_cbf_aleatoric_vars_np, axis=1); mean_aleatoric_att_var_norm = np.mean(all_att_aleatoric_vars_np, axis=1)
            epistemic_cbf_var_norm = np.var(all_cbf_means_norm_np, axis=1) if self.n_ensembles > 1 else np.zeros_like(ensemble_cbf_mean_norm)
            epistemic_att_var_norm = np.var(all_att_means_norm_np, axis=1) if self.n_ensembles > 1 else np.zeros_like(ensemble_att_mean_norm)
        y_mean_cbf, y_std_cbf, y_mean_att, y_std_att = 0.0, 1.0, 0.0, 1.0
        if self.norm_stats:
            y_mean_cbf, y_std_cbf = self.norm_stats.get('y_mean_cbf', 0.0), self.norm_stats.get('y_std_cbf', 1.0)
            y_mean_att, y_std_att = self.norm_stats.get('y_mean_att', 0.0), self.norm_stats.get('y_std_att', 1.0)
        ensemble_cbf_mean_denorm = ensemble_cbf_mean_norm * y_std_cbf + y_mean_cbf; ensemble_att_mean_denorm = ensemble_att_mean_norm * y_std_att + y_mean_att
        total_cbf_var_norm = mean_aleatoric_cbf_var_norm + epistemic_cbf_var_norm; total_att_var_norm = mean_aleatoric_att_var_norm + epistemic_att_var_norm
        total_cbf_var_denorm = total_cbf_var_norm * (y_std_cbf**2); total_att_var_denorm = total_att_var_norm * (y_std_att**2)
        total_cbf_std_denorm = np.sqrt(np.maximum(total_cbf_var_denorm, 0)); total_att_std_denorm = np.sqrt(np.maximum(total_att_var_denorm, 0))
        return ensemble_cbf_mean_denorm, ensemble_att_mean_denorm, total_cbf_std_denorm, total_att_std_denorm

class ASLInMemoryDataset(torch.utils.data.Dataset):
    """
    A high-performance PyTorch Dataset that pre-loads and pre-processes the
    entire offline dataset into RAM. This eliminates I/O bottlenecks during training.
    """
    def __init__(self, data_dir: str, norm_stats: dict):
        self.data_dir = Path(data_dir)
        self.norm_stats = norm_stats
        self.num_plds = len(norm_stats.get('pcasl_mean', []))

        # --- HEAVY LIFTING: Load all data chunks into memory ONCE ---
        logger.info(f"Loading all dataset chunks from {self.data_dir} into RAM...")
        file_paths = sorted(list(self.data_dir.glob('dataset_chunk_*.npz')))
        if not file_paths:
            raise FileNotFoundError(f"No dataset chunks found in {data_dir}. Run generate_offline_dataset.py first.")
        
        all_signals = []
        all_params = []
        for f in tqdm(file_paths, desc="Loading data chunks"):
            with np.load(f) as data:
                all_signals.append(data['signals'])
                all_params.append(data['params'])
        
        self.signals_unnormalized = np.concatenate(all_signals, axis=0)
        self.params_unnormalized = np.concatenate(all_params, axis=0)
        logger.info(f"Loaded {len(self.signals_unnormalized)} samples into memory.")

        # --- ONE-SHOT NORMALIZATION: Process the entire dataset at once ---
        logger.info("Performing one-shot vectorized normalization on the entire dataset...")
        self.signals_normalized = self._normalize_signals(self.signals_unnormalized)
        self.params_normalized = self._normalize_params(self.params_unnormalized)
        logger.info("Normalization complete. Dataset is ready.")

        self.signals_tensor = torch.from_numpy(self.signals_normalized.astype(np.float32))
        self.params_tensor = torch.from_numpy(self.params_normalized.astype(np.float32))

    def _normalize_signals(self, signals_unnorm: np.ndarray) -> np.ndarray:
        pcasl_raw = signals_unnorm[:, :self.num_plds]
        vsasl_raw = signals_unnorm[:, self.num_plds:self.num_plds*2]
        eng_features = signals_unnorm[:, self.num_plds*2:]

        pcasl_mean = np.array(self.norm_stats['pcasl_mean'])
        pcasl_std = np.array(self.norm_stats['pcasl_std']) + 1e-6
        vsasl_mean = np.array(self.norm_stats['vsasl_mean'])
        vsasl_std = np.array(self.norm_stats['vsasl_std']) + 1e-6

        pcasl_norm = (pcasl_raw - pcasl_mean) / pcasl_std
        vsasl_norm = (vsasl_raw - vsasl_mean) / vsasl_std
        
        return np.concatenate([pcasl_norm, vsasl_norm, eng_features], axis=1)

    def _normalize_params(self, params_unnorm: np.ndarray) -> np.ndarray:
        cbf_unnorm = params_unnorm[:, 0]
        att_unnorm = params_unnorm[:, 1]
        
        cbf_norm = (cbf_unnorm - self.norm_stats['y_mean_cbf']) / self.norm_stats['y_std_cbf']
        att_norm = (att_unnorm - self.norm_stats['y_mean_att']) / self.norm_stats['y_std_att']
        
        return np.stack([cbf_norm, att_norm], axis=1)

    def __len__(self):
        return self.signals_tensor.shape[0]

    def __getitem__(self, idx):
        return self.signals_tensor[idx], self.params_tensor[idx]