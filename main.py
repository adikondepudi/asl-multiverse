# asl_trainer.py

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, IterableDataset
import numpy as np
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass
from collections import defaultdict
from torch.optim.lr_scheduler import OneCycleLR
import math
import multiprocessing as mp
from pathlib import Path
import wandb 
from sklearn.metrics import mean_absolute_error, mean_squared_error 
from torch.cuda.amp import GradScaler, autocast
from tqdm import trange
import time
from itertools import islice

num_workers = mp.cpu_count()

from enhanced_asl_network import EnhancedASLNet, CustomLoss
from enhanced_simulation import RealisticASLSimulator, _generate_single_balanced_subject
from utils import engineer_signal_features

import logging
logger = logging.getLogger(__name__)
if not logger.hasHandlers():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

class ASLNet(nn.Module):
    """Neural network for ASL parameter estimation"""

    def __init__(self, input_size: int, hidden_sizes: List[int] = [64, 32, 16]):
        super().__init__()
        layers = []
        prev_size = input_size
        for hidden_size in hidden_sizes:
            layers.extend([nn.Linear(prev_size, hidden_size), nn.ReLU(), nn.BatchNorm1d(hidden_size), nn.Dropout(0.1)])
            prev_size = hidden_size
        layers.append(nn.Linear(prev_size, 2))
        self.network = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x)

class ASLDataset(Dataset):
    """Dataset for ASL signals and parameters"""
    def __init__(self, signals: np.ndarray, params: np.ndarray):
        self.signals = torch.FloatTensor(signals)
        self.params = torch.FloatTensor(params)

    def __len__(self) -> int:
        return len(self.signals)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        return self.signals[idx], self.params[idx]

class ASLTrainer:
    """Training manager for ASL parameter estimation network"""
    def __init__(self, input_size: int, hidden_sizes: List[int] = [64, 32, 16], learning_rate: float = 1e-3, batch_size: int = 32, device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):
        self.device = device
        self.batch_size = batch_size
        self.model = ASLNet(input_size, hidden_sizes).to(device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.criterion = nn.MSELoss()
        self.train_losses = []
        self.val_losses = []

    def prepare_data(self, simulator, n_samples: int = 10000, val_split: float = 0.2, plds_definition: Optional[np.ndarray] = None) -> Tuple[DataLoader, DataLoader]:
        if plds_definition is None: plds = np.arange(500, 3001, 500)
        else: plds = plds_definition
        att_values = np.arange(0, 4001, 100)
        signals_dict = simulator.generate_synthetic_data(plds, att_values, n_noise=n_samples, tsnr=5.0, cbf_val=simulator.params.CBF)
        num_total_instances = n_samples * len(att_values)
        X = np.zeros((num_total_instances, len(plds) * 2))
        pcasl_all = signals_dict['PCASL'].reshape(num_total_instances, len(plds))
        vsasl_all = signals_dict['VSASL'].reshape(num_total_instances, len(plds))
        X[:, :len(plds)] = pcasl_all
        X[:, len(plds):] = vsasl_all
        cbf_true_val = simulator.params.CBF
        cbf_params = np.full(num_total_instances, cbf_true_val)
        att_params = np.tile(np.repeat(att_values, 1), n_samples)
        y = np.column_stack((cbf_params, att_params))
        if num_total_instances == 0: raise ValueError("No data generated by simulator.prepare_data.")
        n_val = int(num_total_instances * val_split)
        if n_val == 0 and num_total_instances > 1: n_val = 1
        if n_val >= num_total_instances : n_val = num_total_instances -1 if num_total_instances > 0 else 0
        X_train, X_val = X[:-n_val], X[-n_val:]
        y_train, y_val = y[:-n_val], y[-n_val:]
        if X_train.shape[0] == 0: raise ValueError("Training set is empty after split.")
        train_dataset = ASLDataset(X_train, y_train)
        val_dataset = ASLDataset(X_val, y_val)
        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=self.batch_size if X_val.shape[0] > 0 else 1)
        return train_loader, val_loader

    def train_epoch(self, train_loader: DataLoader) -> float:
        self.model.train()
        total_loss = 0.0
        for signals, params in train_loader:
            signals, params = signals.to(self.device), params.to(self.device)
            self.optimizer.zero_grad()
            outputs = self.model(signals)
            loss = self.criterion(outputs, params)
            loss.backward()
            self.optimizer.step()
            total_loss += loss.item()
        return total_loss / len(train_loader) if len(train_loader) > 0 else 0.0

    def validate(self, val_loader: DataLoader) -> float:
        if val_loader is None or len(val_loader) == 0:
            logger.warning("Validation loader is empty or None, skipping validation.")
            return float('inf')
        self.model.eval()
        total_loss = 0.0
        with torch.no_grad():
            for signals, params in val_loader:
                signals, params = signals.to(self.device), params.to(self.device)
                outputs = self.model(signals)
                loss = self.criterion(outputs, params)
                total_loss += loss.item()
        return total_loss / len(val_loader) if len(val_loader) > 0 else float('inf')

    def train(self, train_loader: DataLoader, val_loader: DataLoader, n_epochs: int = 100, early_stopping_patience: int = 10) -> Dict[str, List[float]]:
        best_val_loss = float('inf')
        patience_counter = 0
        for epoch in range(n_epochs):
            train_loss = self.train_epoch(train_loader)
            val_loss = self.validate(val_loader)
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                torch.save(self.model.state_dict(), 'best_model_ASLTrainer.pt')
            else:
                patience_counter += 1
            if patience_counter >= early_stopping_patience:
                logger.info(f'Early stopping triggered at epoch {epoch + 1}')
                break
            if (epoch + 1) % 10 == 0:
                logger.info(f'Epoch {epoch + 1}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}')
        model_path = Path('best_model_ASLTrainer.pt')
        if model_path.exists():
            self.model.load_state_dict(torch.load(str(model_path)))
            logger.info(f"Loaded best model from {model_path}")
        else:
            logger.warning(f"Best model file {model_path} not found. Current model state will be used.")
        return {'train_losses': self.train_losses, 'val_losses': self.val_losses}

    def predict(self, signals: np.ndarray) -> np.ndarray:
        self.model.eval()
        with torch.no_grad():
            signals_tensor = torch.FloatTensor(signals).to(self.device)
            if signals_tensor.ndim == 1: signals_tensor = signals_tensor.unsqueeze(0)
            predictions = self.model(signals_tensor)
            return predictions.cpu().numpy()

    def evaluate_performance(self, test_signals: np.ndarray, true_params: np.ndarray) -> Dict[str, float]:
        predictions = self.predict(test_signals)
        mae_cbf = np.mean(np.abs(predictions[:,0] - true_params[:,0]))
        mae_att = np.mean(np.abs(predictions[:,1] - true_params[:,1]))
        rmse_cbf = np.sqrt(np.mean((predictions[:,0] - true_params[:,0])**2))
        rmse_att = np.sqrt(np.mean((predictions[:,1] - true_params[:,1])**2))
        rel_error_cbf = np.mean(np.abs(predictions[:,0] - true_params[:,0]) / np.clip(true_params[:,0], 1e-6, None))
        rel_error_att = np.mean(np.abs(predictions[:,1] - true_params[:,1]) / np.clip(true_params[:,1], 1e-6, None))
        return {'MAE_CBF': mae_cbf, 'MAE_ATT': mae_att, 'RMSE_CBF': rmse_cbf, 'RMSE_ATT': rmse_att, 'RelError_CBF': rel_error_cbf, 'RelError_ATT': rel_error_att}

# This dataset is no longer used for training but is kept as a reference or for augmentation logic.
class EnhancedASLDataset(Dataset):
    def __init__(self, signals: np.ndarray, params: np.ndarray, **kwargs):
        self.signals = torch.FloatTensor(signals)
        self.params = torch.FloatTensor(params)
    def __len__(self) -> int: return len(self.signals)
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]: return self.signals[idx], self.params[idx]

class ASLIterableDataset(IterableDataset):
    """
    An iterable dataset for on-the-fly data generation, normalization, and feature engineering.
    This avoids creating massive datasets in RAM and ensures the model sees unique data in every epoch.
    """
    def __init__(self, simulator: RealisticASLSimulator, plds: np.ndarray, 
                 noise_levels: List[float], norm_stats: Dict, num_att_bins: int = 14):
        super().__init__()
        self.simulator = simulator
        self.plds = plds
        self.num_plds = len(plds)
        self.noise_levels = noise_levels
        self.num_att_bins = num_att_bins
        self.base_params = simulator.params
        self.physio_var = simulator.physio_var
        self.att_range = self.physio_var.att_range
        self.cbf_range = self.physio_var.cbf_range
        self.t1_range = self.physio_var.t1_artery_range
        self.norm_stats = norm_stats

    def __iter__(self):
        worker_info = torch.utils.data.get_worker_info()
        if worker_info is not None: np.random.seed(worker_info.id + int(time.time() * 1000) % (2**32))
        else: np.random.seed(int(time.time() * 1000) % (2**32))

        while True:
            # 1. Stratified ATT sampling
            att_bins = np.linspace(self.att_range[0], self.att_range[1], self.num_att_bins + 1)
            att_bin_idx = np.random.randint(0, self.num_att_bins)
            true_att = np.random.uniform(att_bins[att_bin_idx], att_bins[att_bin_idx+1])
            true_cbf = np.random.uniform(*self.cbf_range)
            true_t1_artery = np.random.uniform(*self.t1_range)
            current_snr = np.random.choice(self.noise_levels)
            
            # 2. Apply sequence parameter perturbations for robustness
            perturbed_t_tau = self.base_params.T_tau * (1 + np.random.uniform(*self.physio_var.t_tau_perturb_range))
            perturbed_alpha_pcasl = np.clip(self.base_params.alpha_PCASL * (1 + np.random.uniform(*self.physio_var.alpha_perturb_range)), 0.1, 1.1)
            perturbed_alpha_vsasl = np.clip(self.base_params.alpha_VSASL * (1 + np.random.uniform(*self.physio_var.alpha_perturb_range)), 0.1, 1.0)

            # 3. Generate the noisy signal
            data_dict = self.simulator.generate_synthetic_data(
                self.plds, att_values=np.array([true_att]), n_noise=1, tsnr=current_snr,
                cbf_val=true_cbf, t1_artery_val=true_t1_artery, t_tau_val=perturbed_t_tau,
                alpha_pcasl_val=perturbed_alpha_pcasl, alpha_vsasl_val=perturbed_alpha_vsasl)
            
            pcasl_noisy = data_dict['MULTIVERSE'][0, 0, :, 0]
            vsasl_noisy = data_dict['MULTIVERSE'][0, 0, :, 1]
            raw_signal = np.concatenate([pcasl_noisy, vsasl_noisy])
            
            # 4. Engineer features from raw signal
            eng_features = engineer_signal_features(raw_signal, self.num_plds)
            
            # 5. Normalize raw signals
            pcasl_norm = (raw_signal[:self.num_plds] - self.norm_stats['pcasl_mean']) / (np.array(self.norm_stats['pcasl_std']) + 1e-6)
            vsasl_norm = (raw_signal[self.num_plds:] - self.norm_stats['vsasl_mean']) / (np.array(self.norm_stats['vsasl_std']) + 1e-6)
        
            # 6. Concatenate to form the final model input
            final_input = np.concatenate([pcasl_norm, vsasl_norm, eng_features.flatten()])

            # 7. Normalize the target parameters
            params = np.array([true_cbf, true_att])
            params_norm = np.array([
                (params[0] - self.norm_stats['y_mean_cbf']) / self.norm_stats['y_std_cbf'],
                (params[1] - self.norm_stats['y_mean_att']) / self.norm_stats['y_std_att']
            ])
            
            yield torch.from_numpy(final_input.astype(np.float32)), torch.from_numpy(params_norm.astype(np.float32))

class EnhancedASLTrainer:
    def __init__(self,
                 model_config: Dict, model_class: callable, input_size: int, learning_rate: float = 0.001,
                 weight_decay: float = 1e-5, batch_size: int = 256, n_ensembles: int = 5,
                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu',
                 n_plds_for_model: Optional[int] = None, m0_input_feature_model: bool = False):
        self.device = device
        self.batch_size = batch_size
        self.n_ensembles = n_ensembles
        self.n_plds_for_model = n_plds_for_model
        self.m0_input_feature_model = m0_input_feature_model
        self.input_size_model = input_size
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay
        self.model_config = model_config
        self.validation_steps_per_epoch = model_config.get('validation_steps_per_epoch', 50)
        self.scalers = [GradScaler() for _ in range(self.n_ensembles)]

        self.models = [torch.compile(model_class(**model_config).to(device), mode="max-autotune") for _ in range(n_ensembles)]
        self.best_states = [None] * self.n_ensembles
        self.optimizers = [torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=self.weight_decay) for model in self.models]
        self.schedulers = []
        
        loss_params = {
            'w_cbf': model_config.get('loss_weight_cbf', 1.0), 'w_att': model_config.get('loss_weight_att', 1.0),
            'log_var_reg_lambda': model_config.get('loss_log_var_reg_lambda', 0.0), 'focal_gamma': model_config.get('focal_gamma', 1.5),
            'pinn_weight': model_config.get('loss_pinn_weight_stage1', model_config.get('loss_pinn_weight', 0.0)),
            'model_params': model_config, 'pre_estimator_loss_weight': model_config.get('pre_estimator_loss_weight_stage1', 0.5)}
        self.custom_loss_fn = CustomLoss(**loss_params)

        self.train_losses = defaultdict(list); self.val_metrics = defaultdict(list); self.global_step = 0; self.norm_stats = None

    def train_ensemble(self,
                   train_loaders: List[DataLoader], val_loaders: List[Optional[DataLoader]],
                   epoch_schedule: List[int], steps_per_epoch_schedule: List[int],
                   early_stopping_patience: int = 20) -> Dict[str, Any]:
        
        histories = defaultdict(lambda: defaultdict(list)); self.global_step = 0; global_epoch_counter = 0 
        if not train_loaders:
            logger.error("train_loaders is empty. Aborting training.")
            return {'final_mean_train_loss': float('nan'), 'final_mean_val_loss': float('nan'), 'all_histories': histories}

        for stage_idx, train_loader in enumerate(train_loaders):
            # --- FIX: Implemented the full curriculum for both PINN and Pre-Estimator loss weights ---
            if stage_idx == 0:
                self.custom_loss_fn.pinn_weight = self.model_config.get('loss_pinn_weight_stage1', 1.0)
                self.custom_loss_fn.pre_estimator_loss_weight = self.model_config.get('pre_estimator_loss_weight_stage1', 1.0)
                logger.info(f"--- STAGE {stage_idx+1}: Foundational Pre-training ---")
                logger.info(f"  Setting PINN weight to {self.custom_loss_fn.pinn_weight}")
                logger.info(f"  Setting Pre-Estimator weight to {self.custom_loss_fn.pre_estimator_loss_weight}")
            elif stage_idx == 1:
                self.custom_loss_fn.pinn_weight = self.model_config.get('loss_pinn_weight_stage2', 0.1)
                self.custom_loss_fn.pre_estimator_loss_weight = self.model_config.get('pre_estimator_loss_weight_stage2', 0.0)
                logger.info(f"--- STAGE {stage_idx+1}: Full-Spectrum Fine-tuning ---")
                logger.info(f"  Setting PINN weight to {self.custom_loss_fn.pinn_weight}")
                logger.info(f"  Setting Pre-Estimator weight to {self.custom_loss_fn.pre_estimator_loss_weight}")

            current_val_loader = val_loaders[stage_idx] if stage_idx < len(val_loaders) else None
            steps_per_epoch = steps_per_epoch_schedule[stage_idx]
            logger.info(f"  Training for {epoch_schedule[stage_idx]} epochs with {steps_per_epoch} train batches per epoch.")
            logger.info(f"Resetting early stopping state for Stage {stage_idx+1}.")
            best_val_losses_stage = [float('inf')] * self.n_ensembles
            patience_counters_stage = [0] * self.n_ensembles

            if not hasattr(self, 'overall_best_val_losses'): self.overall_best_val_losses = [float('inf')] * self.n_ensembles
            n_epochs_stage = epoch_schedule[stage_idx]
            for epoch in range(n_epochs_stage):
                epoch_train_losses_all_models, epoch_val_metrics_all_models = [], []
                train_loader_iter = iter(train_loader)
                for model_idx in range(self.n_ensembles):
                    train_loss_epoch = self._train_epoch(self.models[model_idx], train_loader_iter, self.optimizers[model_idx], self.scalers[model_idx], self.schedulers[model_idx] if self.schedulers else None, global_epoch_counter, steps_per_epoch)
                    epoch_train_losses_all_models.append(train_loss_epoch)
                    histories[model_idx][f'train_losses_stage_{stage_idx}'].append(train_loss_epoch)
                    if model_idx < self.n_ensembles - 1: train_loader_iter = iter(train_loader)

                if current_val_loader:
                    for model_idx in range(self.n_ensembles):
                        val_metrics_dict = self._validate(self.models[model_idx], current_val_loader, global_epoch_counter)
                        epoch_val_metrics_all_models.append(val_metrics_dict)
                        histories[model_idx][f'val_metrics_stage_{stage_idx}'].append(val_metrics_dict)
                        val_loss_for_es = val_metrics_dict.get('val_loss', float('inf'))
                        if val_loss_for_es < best_val_losses_stage[model_idx]:
                            best_val_losses_stage[model_idx] = val_loss_for_es
                            patience_counters_stage[model_idx] = 0
                            if val_loss_for_es < self.overall_best_val_losses[model_idx]:
                                self.overall_best_val_losses[model_idx] = val_loss_for_es
                                self.best_states[model_idx] = self.models[model_idx].state_dict()
                                logger.debug(f"Model {model_idx} new overall best state saved (Val loss: {val_loss_for_es:.4f})")
                        else:
                            patience_counters_stage[model_idx] += 1
                
                if wandb.run:
                    mean_epoch_train_loss = np.nanmean(epoch_train_losses_all_models) if epoch_train_losses_all_models else float('nan')
                    wandb.log({f'Epoch_Stage{stage_idx}/Mean_Train_Loss': mean_epoch_train_loss, 'epoch_global': global_epoch_counter, 'epoch_stage': epoch})
                    if current_val_loader and epoch_val_metrics_all_models:
                        epoch_val_metrics_agg = defaultdict(list)
                        for model_metrics in epoch_val_metrics_all_models:
                            for metric_name, value in model_metrics.items(): epoch_val_metrics_agg[metric_name].append(value)
                        for metric_name, values_list in epoch_val_metrics_agg.items():
                            mean_val_metric = np.nanmean(values_list) if values_list else float('nan')
                            wandb.log({f'Epoch_Stage{stage_idx}/Mean_Val_{metric_name.capitalize()}': mean_val_metric, 'epoch_global': global_epoch_counter, 'epoch_stage': epoch})

                if sum(1 for p_count in patience_counters_stage if p_count < early_stopping_patience) == 0 and epoch > 0 : 
                    logger.info(f"All active models early stopped within stage {stage_idx+1}, epoch {epoch+1}. Moving to next stage.")
                    break 
                if (epoch + 1) % 10 == 0:
                    mean_train_loss_console = np.nanmean(epoch_train_losses_all_models) if epoch_train_losses_all_models else float('nan')
                    mean_val_loss_console_stage = np.nanmean([m.get('val_loss', np.nan) for m in epoch_val_metrics_all_models]) if epoch_val_metrics_all_models else float('nan')
                    logger.info(f"Stage {stage_idx+1}, Epoch {epoch + 1}/{n_epochs_stage}: Mean Active Train Loss = {mean_train_loss_console:.6f}, Mean Active Val Loss (Stage) = {mean_val_loss_console_stage:.6f}")
                global_epoch_counter += 1

        if not hasattr(self, 'overall_best_val_losses'): self.overall_best_val_losses = [float('inf')] * self.n_ensembles
        for model_idx, state in enumerate(self.best_states):
            if state is not None: self.models[model_idx].load_state_dict(state); logger.info(f"Loaded best overall state for model {model_idx} (Overall Val Loss: {self.overall_best_val_losses[model_idx]:.4f})")
            else: logger.warning(f"No best overall state found for model {model_idx}. Using final state from last trained stage.")

        final_train_losses_list_overall, final_val_losses_list_overall = [], []
        for i in range(self.n_ensembles):
            model_train_losses = [histories[i][f'train_losses_stage_{s_idx}'][-1] for s_idx in range(len(train_loaders)) if histories[i][f'train_losses_stage_{s_idx}']]
            if model_train_losses: final_train_losses_list_overall.append(np.nanmean(model_train_losses))
            if self.overall_best_val_losses[i] != float('inf'): final_val_losses_list_overall.append(self.overall_best_val_losses[i])
        final_mean_train_loss_overall = np.nanmean(final_train_losses_list_overall) if final_train_losses_list_overall else float('nan')
        final_mean_val_loss_overall = np.nanmean(final_val_losses_list_overall) if final_val_losses_list_overall else float('nan')
        if wandb.run: wandb.summary['final_mean_train_loss_overall'], wandb.summary['final_mean_val_loss_overall'] = final_mean_train_loss_overall, final_mean_val_loss_overall
        return {'final_mean_train_loss': final_mean_train_loss_overall, 'final_mean_val_loss': final_mean_val_loss_overall, 'all_histories': histories}

    def _train_epoch(self, model, train_loader_iter, optimizer, scaler, scheduler, current_global_epoch: int, steps_per_epoch: int) -> float:
        model.train(); total_loss = 0.0
        pbar = trange(steps_per_epoch, desc=f"Epoch {current_global_epoch+1} Training", leave=False, ncols=100)
        for i in pbar:
            try: signals, params_norm = next(train_loader_iter)
            except StopIteration: logger.warning("Train loader iterator exhausted prematurely."); break
            signals, params_norm = signals.to(self.device), params_norm.to(self.device)
            optimizer.zero_grad(set_to_none=True)
            with autocast(dtype=torch.bfloat16):
                outputs = model(signals)
                cbf_mean_norm, att_mean_norm, cbf_log_var, att_log_var, cbf_rough, att_rough = outputs
                loss = self.custom_loss_fn(signals, cbf_mean_norm, att_mean_norm, params_norm[:, 0:1], params_norm[:, 1:2], cbf_log_var, att_log_var, cbf_rough, att_rough, current_global_epoch)
            scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()
            if scheduler: scheduler.step();
            if wandb.run and scheduler: wandb.log({'lr': scheduler.get_last_lr()[0]}, step=self.global_step)
            total_loss += loss.item(); pbar.set_postfix(loss=loss.item()); self.global_step += 1
        return total_loss / steps_per_epoch if steps_per_epoch > 0 else 0.0

    def _validate(self, model, val_loader, current_global_epoch: int) -> Dict[str, float]:
        model.eval(); total_loss_val = 0.0
        all_cbf_preds_norm, all_att_preds_norm = [], []; all_cbf_trues_norm, all_att_trues_norm = [], []
        all_cbf_log_vars, all_att_log_vars = [], []

        with torch.no_grad():
            pbar_val = trange(self.validation_steps_per_epoch, desc=f"Epoch {current_global_epoch+1} Validation", leave=False, ncols=100)
            for signals, params_norm in islice(val_loader, self.validation_steps_per_epoch):
                signals, params_norm = signals.to(self.device), params_norm.to(self.device)
                with autocast(dtype=torch.bfloat16):
                    outputs = model(signals)
                    cbf_mean_norm, att_mean_norm, cbf_log_var, att_log_var, cbf_rough, att_rough = outputs
                    loss = self.custom_loss_fn(signals, cbf_mean_norm, att_mean_norm, params_norm[:, 0:1], params_norm[:, 1:2], cbf_log_var, att_log_var, cbf_rough, att_rough, current_global_epoch)
                total_loss_val += loss.item()
                all_cbf_preds_norm.append(cbf_mean_norm.cpu()); all_att_preds_norm.append(att_mean_norm.cpu())
                all_cbf_trues_norm.append(params_norm[:, 0:1].cpu()); all_att_trues_norm.append(params_norm[:, 1:2].cpu())
                all_cbf_log_vars.append(cbf_log_var.cpu()); all_att_log_vars.append(att_log_var.cpu())
                pbar_val.update(1)
            pbar_val.close()
        
        avg_loss_val = total_loss_val / self.validation_steps_per_epoch if self.validation_steps_per_epoch > 0 else float('inf')
        metrics_dict = {'val_loss': avg_loss_val}

        if all_cbf_preds_norm and self.norm_stats:
            y_mean_cbf, y_std_cbf = self.norm_stats.get('y_mean_cbf', 0.0), self.norm_stats.get('y_std_cbf', 1.0)
            y_mean_att, y_std_att = self.norm_stats.get('y_mean_att', 0.0), self.norm_stats.get('y_std_att', 1.0)
            cbf_preds_norm_cat = torch.cat(all_cbf_preds_norm).numpy().squeeze(); att_preds_norm_cat = torch.cat(all_att_preds_norm).numpy().squeeze()
            cbf_trues_norm_cat = torch.cat(all_cbf_trues_norm).numpy().squeeze(); att_trues_norm_cat = torch.cat(all_att_trues_norm).numpy().squeeze()
            cbf_preds_denorm = cbf_preds_norm_cat * y_std_cbf + y_mean_cbf; att_preds_denorm = att_preds_norm_cat * y_std_att + y_mean_att
            cbf_trues_denorm = cbf_trues_norm_cat * y_std_cbf + y_mean_cbf; att_trues_denorm = att_trues_norm_cat * y_std_att + y_mean_att
            if len(cbf_preds_denorm) > 0 : 
                metrics_dict['cbf_mae'] = mean_absolute_error(cbf_trues_denorm, cbf_preds_denorm)
                metrics_dict['cbf_rmse'] = np.sqrt(mean_squared_error(cbf_trues_denorm, cbf_preds_denorm))
                metrics_dict['att_mae'] = mean_absolute_error(att_trues_denorm, att_preds_denorm)
                metrics_dict['att_rmse'] = np.sqrt(mean_squared_error(att_trues_denorm, att_preds_denorm))
                cbf_log_vars_cat = torch.cat(all_cbf_log_vars).numpy().squeeze(); att_log_vars_cat = torch.cat(all_att_log_vars).numpy().squeeze()
                metrics_dict['mean_cbf_log_var'] = np.mean(cbf_log_vars_cat); metrics_dict['mean_att_log_var'] = np.mean(att_log_vars_cat)
        return metrics_dict

    def predict(self, signals: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        signals_tensor = torch.FloatTensor(signals).to(self.device)
        if signals_tensor.ndim == 1: signals_tensor = signals_tensor.unsqueeze(0)
        all_cbf_means_norm_list, all_att_means_norm_list = [], []; all_cbf_aleatoric_vars_list, all_att_aleatoric_vars_list = [], []
        for model in self.models:
            model.eval()
            with torch.no_grad():
                cbf_mean_norm, att_mean_norm, cbf_log_var, att_log_var, _, _ = model(signals_tensor)
                all_cbf_means_norm_list.append(cbf_mean_norm.cpu().numpy()); all_att_means_norm_list.append(att_mean_norm.cpu().numpy())
                all_cbf_aleatoric_vars_list.append(torch.exp(cbf_log_var).cpu().numpy()); all_att_aleatoric_vars_list.append(torch.exp(att_log_var).cpu().numpy())
        if signals_tensor.shape[0] == 1:
            all_cbf_means_norm_np = np.array(all_cbf_means_norm_list).squeeze(); all_att_means_norm_np = np.array(all_att_means_norm_list).squeeze() 
            all_cbf_aleatoric_vars_np = np.array(all_cbf_aleatoric_vars_list).squeeze(); all_att_aleatoric_vars_np = np.array(all_att_aleatoric_vars_list).squeeze() 
            cbf_weights = 1.0 / (all_cbf_aleatoric_vars_np + 1e-9); ensemble_cbf_mean_norm = np.average(all_cbf_means_norm_np, weights=cbf_weights)
            att_weights = 1.0 / (all_att_aleatoric_vars_np + 1e-9); ensemble_att_mean_norm = np.average(all_att_means_norm_np, weights=att_weights)
            mean_aleatoric_cbf_var_norm = np.mean(all_cbf_aleatoric_vars_np); mean_aleatoric_att_var_norm = np.mean(all_att_aleatoric_vars_np)
            epistemic_cbf_var_norm = np.var(all_cbf_means_norm_np) if self.n_ensembles > 1 else 0.0
            epistemic_att_var_norm = np.var(all_att_means_norm_np) if self.n_ensembles > 1 else 0.0
        else:
            all_cbf_means_norm_np = np.concatenate(all_cbf_means_norm_list, axis=1); all_att_means_norm_np = np.concatenate(all_att_means_norm_list, axis=1) 
            all_cbf_aleatoric_vars_np = np.concatenate(all_cbf_aleatoric_vars_list, axis=1); all_att_aleatoric_vars_np = np.concatenate(all_att_aleatoric_vars_list, axis=1) 
            cbf_weights = 1.0 / (all_cbf_aleatoric_vars_np + 1e-9); ensemble_cbf_mean_norm = np.average(all_cbf_means_norm_np, axis=1, weights=cbf_weights)
            att_weights = 1.0 / (all_att_aleatoric_vars_np + 1e-9); ensemble_att_mean_norm = np.average(all_att_means_norm_np, axis=1, weights=att_weights)
            mean_aleatoric_cbf_var_norm = np.mean(all_cbf_aleatoric_vars_np, axis=1); mean_aleatoric_att_var_norm = np.mean(all_att_aleatoric_vars_np, axis=1)
            epistemic_cbf_var_norm = np.var(all_cbf_means_norm_np, axis=1) if self.n_ensembles > 1 else np.zeros_like(ensemble_cbf_mean_norm)
            epistemic_att_var_norm = np.var(all_att_means_norm_np, axis=1) if self.n_ensembles > 1 else np.zeros_like(ensemble_att_mean_norm)
        y_mean_cbf, y_std_cbf, y_mean_att, y_std_att = 0.0, 1.0, 0.0, 1.0
        if self.norm_stats:
            y_mean_cbf, y_std_cbf = self.norm_stats.get('y_mean_cbf', 0.0), self.norm_stats.get('y_std_cbf', 1.0)
            y_mean_att, y_std_att = self.norm_stats.get('y_mean_att', 0.0), self.norm_stats.get('y_std_att', 1.0)
        ensemble_cbf_mean_denorm = ensemble_cbf_mean_norm * y_std_cbf + y_mean_cbf; ensemble_att_mean_denorm = ensemble_att_mean_norm * y_std_att + y_mean_att
        total_cbf_var_norm = mean_aleatoric_cbf_var_norm + epistemic_cbf_var_norm; total_att_var_norm = mean_aleatoric_att_var_norm + epistemic_att_var_norm
        total_cbf_var_denorm = total_cbf_var_norm * (y_std_cbf**2); total_att_var_denorm = total_att_var_norm * (y_std_att**2)
        total_cbf_std_denorm = np.sqrt(np.maximum(total_cbf_var_denorm, 0)); total_att_std_denorm = np.sqrt(np.maximum(total_att_var_denorm, 0))
        return ensemble_cbf_mean_denorm, ensemble_att_mean_denorm, total_cbf_std_denorm, total_att_std_denorm