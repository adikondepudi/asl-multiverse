# FILE: final_benchmark.py
# final_benchmark.py
#
# MODIFIED FOR ABLATION STUDY:
# This script performs an inference-only ablation study to probe the behavior
# of a pre-trained MULTIVERSE network, as requested. It compares:
#   1. The full MULTIVERSE network (NN-MULTIVERSE).
#   2. The network fed only PCASL data (NN-PCASL).
#   3. The network fed only VSASL data (NN-VSASL).
#   4. A conventional Least-Squares fit on PCASL data (LS-PCASL).
#   5. A conventional Least-Squares fit on VSASL data (LS-VSASL).
#
# This allows for a deep analysis of single-modality performance and the
# synergistic gains from data fusion without retraining any models.
#
# Original Principles Retained:
#   1. REALISTIC DATA ONLY: All tests are run on data generated by the
#      EnhancedASLSimulator with its full, complex noise and artifact model.
#   2. STRATIFIED ANALYSIS: Performance is broken down by Arterial Transit Time.
#   3. SUPERIOR METRICS: Calculates MAE, MedAE, RMSE, Bias, and CoV.

import torch
import numpy as np
import pandas as pd
from pathlib import Path
import json
import argparse
import sys
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict

# --- Import from your project codebase ---
from enhanced_asl_network import EnhancedASLNet
from enhanced_simulation import RealisticASLSimulator, ASLParameters, PhysiologicalVariation
from pcasl_functions import fit_PCASL_vectInit_pep
from vsasl_functions import fit_VSASL_vectInit_pep
from utils import engineer_signal_features
from predict_on_invivo import apply_normalization_vectorized, denormalize_predictions # Re-use utility functions

# --- Configuration ---
NUM_SAMPLES = 5000  # Number of unique voxels to simulate for the benchmark
NOISE_LEVELS = [3.0, 5.0, 8.0, 10.0, 15.0] # tSNR levels to test
CONDITIONS = ['healthy', 'stroke', 'tumor', 'elderly']

def load_artifacts(model_results_root: Path) -> tuple:
    """Robustly loads the model ensemble, final config, and norm stats."""
    print(f"--> Loading artifacts from: {model_results_root}")
    try:
        with open(model_results_root / 'research_config.json', 'r') as f:
            config = json.load(f)
        with open(model_results_root / 'norm_stats.json', 'r') as f:
            norm_stats = json.load(f)

        models = []
        models_dir = model_results_root / 'trained_models'
        num_plds = len(config['pld_values'])
        base_input_size = num_plds * 2 + 4

        for model_path in models_dir.glob('ensemble_model_*.pt'):
            model = EnhancedASLNet(input_size=base_input_size, **config)
            model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))
            model.eval()
            models.append(model)

        if not models:
            raise FileNotFoundError("No models found in trained_models folder.")
        print(f"--> Successfully loaded {len(models)} models, config, and norm_stats.")
        return models, config, norm_stats
    except Exception as e:
        print(f"[FATAL ERROR] Could not load artifacts: {e}. Exiting.")
        sys.exit(1)

def fit_ls_pcasl_only(pcasl_signal: np.ndarray, plds: np.ndarray, config: Dict):
    """Wrapper for the PCASL-only least-squares fitter."""
    try:
        ls_params = {k: v for k, v in config.items() if k in ['T1_artery', 'T_tau', 'T2_factor', 'alpha_BS1', 'alpha_PCASL']}
        init_guess = [50.0 / 6000.0, 1500.0]
        beta, _, _, _ = fit_PCASL_vectInit_pep(plds, pcasl_signal, init_guess, **ls_params)
        return beta[0] * 6000.0, beta[1], True
    except Exception:
        return np.nan, np.nan, False

def fit_ls_vsasl_only(vsasl_signal: np.ndarray, plds: np.ndarray, config: Dict):
    """Wrapper for the VSASL-only least-squares fitter."""
    try:
        ls_params = {k: v for k, v in config.items() if k in ['T1_artery', 'T2_factor', 'alpha_BS1', 'alpha_VSASL']}
        init_guess = [50.0 / 6000.0, 1500.0]
        beta, _, _, _ = fit_VSASL_vectInit_pep(plds, vsasl_signal, init_guess, **ls_params)
        return beta[0] * 6000.0, beta[1], True
    except Exception:
        return np.nan, np.nan, False

def calculate_metrics(df_group):
    """Calculates a comprehensive set of metrics for a given group of results."""
    metrics = {}
    
    model_keys = ['ls_pcasl', 'ls_vsasl', 'nn_multiverse', 'nn_pcasl', 'nn_vsasl']
    
    for model in model_keys:
        for param in ['cbf', 'att']:
            true_col = f'true_{param}'
            pred_col = f'{model}_{param}_pred'
            
            valid_preds = df_group.dropna(subset=[pred_col])
            if valid_preds.empty:
                for metric_name in ['MAE', 'MedAE', 'RMSE', 'Bias', 'SD']:
                     metrics[f'{model.upper()} {param.upper()} {metric_name}'] = np.nan
                continue

            errors = valid_preds[pred_col] - valid_preds[true_col]
            metrics[f'{model.upper()} {param.upper()} MAE'] = np.mean(np.abs(errors))
            metrics[f'{model.upper()} {param.upper()} MedAE'] = np.median(np.abs(errors))
            metrics[f'{model.upper()} {param.upper()} RMSE'] = np.sqrt(np.mean(errors**2))
            metrics[f'{model.upper()} {param.upper()} Bias'] = np.mean(errors)
            metrics[f'{model.upper()} {param.upper()} SD'] = np.std(valid_preds[pred_col])

    for model in model_keys:
        for param in ['cbf', 'att']:
            mean_val = df_group[f'{model}_{param}_pred'].mean()
            sd_val = metrics.get(f'{model.upper()} {param.upper()} SD', np.nan)
            metrics[f'{model.upper()} {param.upper()} CoV'] = (sd_val / mean_val) if mean_val != 0 else np.nan

    metrics['LS PCASL Fit Success %'] = (df_group['ls_pcasl_fit_success'].sum() / len(df_group)) * 100
    metrics['LS VSASL Fit Success %'] = (df_group['ls_vsasl_fit_success'].sum() / len(df_group)) * 100
    return pd.Series(metrics)

def main(model_dir: Path, output_dir: Path):
    """Main execution function."""
    output_dir.mkdir(parents=True, exist_ok=True)
    
    models, config, norm_stats = load_artifacts(model_dir)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    for model in models:
        model.to(device)

    print("\n--- Generating Maximally Realistic Simulation Dataset for Ablation Study ---")
    asl_params = ASLParameters(**{k: v for k, v in config.items() if k in ASLParameters.__annotations__})
    simulator = RealisticASLSimulator(params=asl_params)
    physio_var = PhysiologicalVariation()
    plds = np.array(config['pld_values'])
    num_plds = len(plds)

    results = []
    for _ in tqdm(range(NUM_SAMPLES), desc="Generating & Evaluating Samples"):
        true_cbf = np.random.uniform(*physio_var.cbf_range)
        true_att = np.random.uniform(*physio_var.att_range)
        true_t1_artery = np.random.uniform(*physio_var.t1_artery_range)
        tsnr = np.random.choice(NOISE_LEVELS)
        condition = np.random.choice(CONDITIONS)

        perturbed_t_tau = simulator.params.T_tau * (1 + np.random.uniform(*physio_var.t_tau_perturb_range))
        perturbed_alpha_pcasl = np.clip(simulator.params.alpha_PCASL * (1 + np.random.uniform(*physio_var.alpha_perturb_range)), 0.1, 1.1)
        perturbed_alpha_vsasl = np.clip(simulator.params.alpha_VSASL * (1 + np.random.uniform(*physio_var.alpha_perturb_range)), 0.1, 1.0)
        
        vsasl_clean = simulator._generate_vsasl_signal(plds, true_att, true_cbf, true_t1_artery, perturbed_alpha_vsasl)
        pcasl_clean = simulator._generate_pcasl_signal(plds, true_att, true_cbf, true_t1_artery, perturbed_t_tau, perturbed_alpha_pcasl)

        pcasl_noisy = simulator.add_realistic_noise(pcasl_clean, snr=tsnr)
        vsasl_noisy = simulator.add_realistic_noise(vsasl_clean, snr=tsnr)

        # --- A) Evaluate with Conventional LS Fitters ---
        ls_pcasl_cbf, ls_pcasl_att, ls_pcasl_success = fit_ls_pcasl_only(pcasl_noisy, plds, config)
        ls_vsasl_cbf, ls_vsasl_att, ls_vsasl_success = fit_ls_vsasl_only(vsasl_noisy, plds, config)

        # --- B) Evaluate with our Neural Network (in 3 ablated ways) ---
        zeros_curve = np.zeros_like(pcasl_noisy)
        
        # B.1: Full MULTIVERSE Input
        signal_multi = np.concatenate([pcasl_noisy, vsasl_noisy])
        feats_multi = engineer_signal_features(signal_multi, num_plds)
        input_multi = np.concatenate([signal_multi, feats_multi])
        
        # B.2: PCASL-Only Input (VSASL ablated)
        signal_pcasl = np.concatenate([pcasl_noisy, zeros_curve])
        feats_pcasl = engineer_signal_features(signal_pcasl, num_plds)
        input_pcasl = np.concatenate([signal_pcasl, feats_pcasl])
        
        # B.3: VSASL-Only Input (PCASL ablated)
        signal_vsasl = np.concatenate([zeros_curve, vsasl_noisy])
        feats_vsasl = engineer_signal_features(signal_vsasl, num_plds)
        input_vsasl = np.concatenate([signal_vsasl, feats_vsasl])

        # Batch all 3 inputs for efficient normalization and prediction
        input_batch = np.stack([input_multi, input_pcasl, input_vsasl])
        norm_batch = apply_normalization_vectorized(input_batch, norm_stats, num_plds)
        input_tensor = torch.FloatTensor(norm_batch).to(device)
        
        with torch.no_grad():
            all_cbf_means, all_att_means = [], []
            for model in models:
                cbf_mean_norm, att_mean_norm, _, _, _, _ = model(input_tensor)
                all_cbf_means.append(cbf_mean_norm.cpu().numpy())
                all_att_means.append(att_mean_norm.cpu().numpy())
        
        ensemble_cbf_norm = np.mean(all_cbf_means, axis=0).squeeze()
        ensemble_att_norm = np.mean(all_att_means, axis=0).squeeze()
        nn_cbf_preds, nn_att_preds = denormalize_predictions(ensemble_cbf_norm, ensemble_att_norm, norm_stats)
        
        results.append({
            'true_cbf': true_cbf, 'true_att': true_att, 'tsnr': tsnr, 'condition': condition,
            'ls_pcasl_cbf_pred': ls_pcasl_cbf, 'ls_pcasl_att_pred': ls_pcasl_att, 'ls_pcasl_fit_success': ls_pcasl_success,
            'ls_vsasl_cbf_pred': ls_vsasl_cbf, 'ls_vsasl_att_pred': ls_vsasl_att, 'ls_vsasl_fit_success': ls_vsasl_success,
            'nn_multiverse_cbf_pred': nn_cbf_preds[0], 'nn_multiverse_att_pred': nn_att_preds[0],
            'nn_pcasl_cbf_pred': nn_cbf_preds[1], 'nn_pcasl_att_pred': nn_att_preds[1],
            'nn_vsasl_cbf_pred': nn_cbf_preds[2], 'nn_vsasl_att_pred': nn_att_preds[2],
        })

    df_results = pd.DataFrame(results)
    df_results.to_csv(output_dir / 'full_ablation_results.csv', index=False)

    print("\n\n" + "="*80); print("ABLATION BENCHMARK RESULTS"); print("="*80 + "\n")

    att_bins = pd.cut(df_results['true_att'], [0, 1500, 2500, 4000], labels=['Short (<1.5s)', 'Medium (1.5-2.5s)', 'Long (>2.5s)'])
    att_summary = df_results.groupby(att_bins, observed=False).apply(calculate_metrics)
    print("\nTABLE: PERFORMANCE STRATIFIED BY ARTERIAL TRANSIT TIME (ATT)")
    print(att_summary.to_string())
    att_summary.to_csv(output_dir / 'summary_ablation_by_att.csv')

    print("\n--- Generating Publication-Ready Plots for Ablation Study ---")
    df_sorted = df_results.sort_values('true_att').dropna(how='all', subset=[c for c in df_results.columns if '_pred' in c])
    window_size = len(df_sorted) // 20

    fig, axes = plt.subplots(3, 2, figsize=(18, 22), sharex=True)
    sns.set_style("whitegrid")
    
    plot_params = {
        'ls_pcasl': {'label': 'LS (PCASL only)', 'color': 'skyblue', 'linestyle': '--', 'linewidth': 2},
        'ls_vsasl': {'label': 'LS (VSASL only)', 'color': 'lightgreen', 'linestyle': '--', 'linewidth': 2},
        'nn_pcasl': {'label': 'NN (PCASL only)', 'color': 'blue', 'linestyle': '-', 'linewidth': 2.5},
        'nn_vsasl': {'label': 'NN (VSASL only)', 'color': 'green', 'linestyle': '-', 'linewidth': 2.5},
        'nn_multiverse': {'label': 'NN (MULTIVERSE)', 'color': 'red', 'linestyle': '-', 'linewidth': 3}
    }
    model_order = ['ls_pcasl', 'ls_vsasl', 'nn_pcasl', 'nn_vsasl', 'nn_multiverse']

    # Row 1: Bias
    for model in model_order:
        bias_cbf = (df_sorted[f'{model}_cbf_pred'] - df_sorted['true_cbf']).rolling(window_size, center=True).mean()
        axes[0, 0].plot(df_sorted['true_att'], bias_cbf, **plot_params[model])
        bias_att = (df_sorted[f'{model}_att_pred'] - df_sorted['true_att']).rolling(window_size, center=True).mean()
        axes[0, 1].plot(df_sorted['true_att'], bias_att, **plot_params[model])
    
    axes[0, 0].set_title('CBF Accuracy (Bias)', fontsize=14, fontweight='bold'); axes[0, 0].set_ylabel('Bias (mL/100g/min)'); axes[0, 0].axhline(0, color='k', linestyle=':', alpha=0.7)
    axes[0, 1].set_title('ATT Accuracy (Bias)', fontsize=14, fontweight='bold'); axes[0, 1].set_ylabel('Bias (ms)'); axes[0, 1].axhline(0, color='k', linestyle=':', alpha=0.7)

    # Row 2: CoV (Precision)
    for model in model_order:
        cov_cbf = (df_sorted[f'{model}_cbf_pred'].rolling(window_size).std() / df_sorted[f'{model}_cbf_pred'].rolling(window_size).mean()) * 100
        axes[1, 0].plot(df_sorted['true_att'], cov_cbf, **plot_params[model])
        cov_att = (df_sorted[f'{model}_att_pred'].rolling(window_size).std() / df_sorted[f'{model}_att_pred'].rolling(window_size).mean()) * 100
        axes[1, 1].plot(df_sorted['true_att'], cov_att, **plot_params[model])

    axes[1, 0].set_title('CBF Precision (Coefficient of Variation)', fontsize=14, fontweight='bold'); axes[1, 0].set_ylabel('CoV (%)'); axes[1, 0].set_ylim(bottom=0)
    axes[1, 1].set_title('ATT Precision (Coefficient of Variation)', fontsize=14, fontweight='bold'); axes[1, 1].set_ylabel('CoV (%)'); axes[1, 1].set_ylim(bottom=0)

    # Row 3: nRMSE
    for model in model_order:
        rmse_cbf = ((df_sorted[f'{model}_cbf_pred'] - df_sorted['true_cbf'])**2).rolling(window_size).mean().apply(np.sqrt)
        nrmse_cbf = (rmse_cbf / df_sorted['true_cbf'].rolling(window_size).mean()) * 100
        axes[2, 0].plot(df_sorted['true_att'], nrmse_cbf, **plot_params[model])
        rmse_att = ((df_sorted[f'{model}_att_pred'] - df_sorted['true_att'])**2).rolling(window_size).mean().apply(np.sqrt)
        nrmse_att = (rmse_att / df_sorted['true_att'].rolling(window_size).mean()) * 100
        axes[2, 1].plot(df_sorted['true_att'], nrmse_att, **plot_params[model])

    axes[2, 0].set_title('CBF Overall Error (nRMSE)', fontsize=14, fontweight='bold'); axes[2, 0].set_ylabel('nRMSE (%)'); axes[2, 0].set_xlabel('True Arterial Transit Time (ms)'); axes[2, 0].set_ylim(bottom=0)
    axes[2, 1].set_title('ATT Overall Error (nRMSE)', fontsize=14, fontweight='bold'); axes[2, 1].set_ylabel('nRMSE (%)'); axes[2, 1].set_xlabel('True Arterial Transit Time (ms)'); axes[2, 1].set_ylim(bottom=0)

    for ax in axes.flat:
        ax.legend(fontsize='small'); ax.set_xlim(500, 4000)

    fig.suptitle('Ablation Study: Performance on Realistic Simulated Data', fontsize=20, fontweight='bold')
    plt.tight_layout(rect=[0, 0.03, 1, 0.96])
    fig_path = output_dir / 'figure_ablation_performance_vs_att.png'
    plt.savefig(fig_path, dpi=300)
    print(f"--> Plots saved to {fig_path}")

    print("\n" + "="*80); print("ABLATION BENCHMARK COMPLETE"); print("="*80)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Run the inference-only ablation benchmark for the ASL Multiverse project.")
    parser.add_argument("model_artifacts_dir", type=str, help="Path to the directory containing the final trained model artifacts (e.g., 'final_training_run_v12').")
    parser.add_argument("output_dir", type=str, nargs='?', default=None, help="Directory to save the benchmark results. If not provided, a 'ablation_benchmark_results' directory will be created.")
    args = parser.parse_args()

    if args.output_dir:
        output_path = Path(args.output_dir)
    else:
        output_path = Path('./ablation_benchmark_results')
        
    main(Path(args.model_artifacts_dir), output_path)